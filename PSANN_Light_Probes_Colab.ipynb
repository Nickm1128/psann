{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSANN Light Probes (Colab) — Do Not Run Locally\n",
    "\n",
    "Upload this notebook and your `datasets.zip` (or a `datasets/` folder) to Colab, then run top-to-bottom.\n",
    "It runs light replications for Jena/Beijing and a small real EAF TEMP run, plus an optional Jacobian PR snapshot.\n",
    "\n",
    "Outputs are written to `colab_results_light/metrics.csv` and optionally `colab_results_light/jacobian_pr.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies (install psann from PyPI)\n",
    "!pip -q install psann torch torchvision torchaudio pandas numpy scikit-learn >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration — adjust as needed\n",
    "TASKS = ['jena', 'beijing', 'eaf']  # choose any subset of: 'jena', 'beijing', 'eaf'\n",
    "SEEDS = [7, 8]                      # a couple of light replications\n",
    "EPOCHS = 10                         # keep small to stay light\n",
    "DEVICE = 'auto'                     # 'auto' | 'cpu' | 'cuda'\n",
    "PR_SNAPSHOTS = False                # set True to record end Jacobian PR on Jena\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e08a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light test runner (bundled version of colab_tests_light/run_light_probes.py)\n",
    "import math, os, random, json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from psann.conv import PSANNConv1dNet\n",
    "from psann.nn import PSANNNet\n",
    "from psann.utils import choose_device, seed_all as psann_seed_all\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd())\n",
    "DATA_ROOT = Path(os.getenv('PSANN_DATA_ROOT', PROJECT_ROOT / 'datasets')).resolve()\n",
    "RESULTS_ROOT = (PROJECT_ROOT / 'colab_results_light').resolve()\n",
    "RESULTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "JENA_ZIP_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip'\n",
    "\n",
    "def ensure_jena_dataset() -> Path:\n",
    "    base = DATA_ROOT / 'Jena Climate 2009-2016'\n",
    "    csv = base / 'jena_climate_2009_2016.csv'\n",
    "    if csv.exists():\n",
    "        return csv\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    tmp_zip = base / 'jena_climate_2009_2016.csv.zip'\n",
    "    import urllib.request, zipfile\n",
    "    try:\n",
    "        print(f'[data] Downloading Jena Climate dataset to {base} ...')\n",
    "        urllib.request.urlretrieve(JENA_ZIP_URL, tmp_zip)\n",
    "        with zipfile.ZipFile(tmp_zip, 'r') as zf:\n",
    "            zf.extractall(base)\n",
    "    except Exception as exc:\n",
    "        print(f'[warn] Failed to download Jena dataset: {exc}')\n",
    "    finally:\n",
    "        if tmp_zip.exists():\n",
    "            tmp_zip.unlink()\n",
    "    if csv.exists():\n",
    "        return csv\n",
    "    matches = list(base.glob('**/jena_climate_2009_2016.csv'))\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    raise FileNotFoundError(f'Could not locate Jena climate CSV under {base}')\n",
    "\n",
    "def seed_all(seed: int) -> None:\n",
    "    psann_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def pick_device(arg: str) -> torch.device:\n",
    "    return choose_device(arg)\n",
    "\n",
    "def _fix_backslash_artifacts(root: Path) -> None:\n",
    "    # Move files like 'datasets\\Human Activity Recognition\\...txt' into proper subfolders under datasets/\n",
    "    for leftover in root.iterdir():\n",
    "        name = leftover.name\n",
    "        if \\\"\\\\\\\" in name and name.lower().startswith('datasets'):\n",
    "            # Normalise to forward slashes and build destination path\n",
    "            rel = Path(*Path(name.replace('\\\\', '/')).parts)\n",
    "            dest = root / rel\n",
    "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                leftover.replace(dest)\n",
    "                print(f'[fix] Moved stray {leftover} -> {dest}')\n",
    "            except Exception as e:\n",
    "                print(f'[warn] Could not move {leftover}: {e}')\n",
    "\n",
    "def maybe_extract_datasets_zip() -> None:\n",
    "    # First, fix any previously extracted stray files with backslashes in their names\n",
    "    _fix_backslash_artifacts(PROJECT_ROOT)\n",
    "    # If datasets/ already looks populated, skip extraction\n",
    "    if DATA_ROOT.exists() and any(DATA_ROOT.rglob('*')):\n",
    "        return\n",
    "    zip_path = PROJECT_ROOT / 'datasets.zip'\n",
    "    if not zip_path.exists():\n",
    "        print(f'[warn] DATA_ROOT {DATA_ROOT} not found and datasets.zip missing. Upload datasets first.')\n",
    "        return\n",
    "    import zipfile\n",
    "    print(f'[info] Extracting {zip_path} to {PROJECT_ROOT}/datasets (robust normalisation)...')\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        for zi in z.infolist():\n",
    "            name = zi.filename\n",
    "            # Normalise path separators and strip leading './'\n",
    "            norm = name.replace('\\\\', '/').lstrip('./')\n",
    "            parts = [p for p in norm.split('/') if p not in ('',)]\n",
    "            if not parts:\n",
    "                continue\n",
    "            if parts[0].lower() != 'datasets':\n",
    "                parts = ['datasets'] + parts\n",
    "            dest = PROJECT_ROOT.joinpath(*parts)\n",
    "            if zi.is_dir():\n",
    "                dest.mkdir(parents=True, exist_ok=True)\n",
    "            else:\n",
    "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with z.open(zi) as src, open(dest, 'wb') as f:\n",
    "                    f.write(src.read())\n",
    "    # Final pass to relocate any leftover stray files created by other tools\n",
    "    _fix_backslash_artifacts(PROJECT_ROOT)\n",
    "\n",
    "class PSANNConvSpine(nn.Module):\n",
    "    def __init__(self, in_ch: int, hidden: int, depth: int, kernel_size: int, horizon: int, aggregator: str='last'):\n",
    "        super().__init__()\n",
    "        self.aggregator = aggregator\n",
    "        self.core = PSANNConv1dNet(\n",
    "            in_channels=in_ch,\n",
    "            out_dim=hidden,\n",
    "            hidden_layers=depth,\n",
    "            conv_channels=hidden,\n",
    "            hidden_channels=hidden,\n",
    "            kernel_size=kernel_size,\n",
    "            segmentation_head=True,\n",
    "        )\n",
    "        self.head = nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.core(x.transpose(1,2))\n",
    "        if self.aggregator == 'last':\n",
    "            pooled = features[:, :, -1]\n",
    "        else:\n",
    "            pooled = features.mean(dim=-1)\n",
    "        return self.head(pooled)\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, depth: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = PSANNNet(\n",
    "            input_dim=in_dim,\n",
    "            output_dim=out_dim,\n",
    "            hidden_layers=depth,\n",
    "            hidden_units=hidden,\n",
    "            hidden_width=hidden,\n",
    "            activation_type='relu',\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, t, f = x.shape\n",
    "        return self.net(x.reshape(b, t * f))\n",
    "\n",
    "@dataclass\n",
    "class TrainSpec:\n",
    "    model: str\n",
    "    hidden: int\n",
    "    depth: int\n",
    "    kernel_size: int = 5\n",
    "    epochs: int = 10\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 256\n",
    "    patience: int = 3\n",
    "\n",
    "def count_params(m: nn.Module) -> int: return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "def train_regressor(model, train_X, train_y, val_X, val_y, spec: TrainSpec, device):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=spec.lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    tX, ty = torch.from_numpy(train_X).float().to(device), torch.from_numpy(train_y).float().to(device)\n",
    "    vX, vy = torch.from_numpy(val_X).float().to(device), torch.from_numpy(val_y).float().to(device)\n",
    "    best_state, best_val, bad = None, math.inf, 0\n",
    "    n = tX.size(0)\n",
    "    for ep in range(spec.epochs):\n",
    "        model.train()\n",
    "        perm = torch.randperm(n, device=device)\n",
    "        for i in range(0, n, spec.batch_size):\n",
    "            idx = perm[i:i+spec.batch_size]\n",
    "            xb, yb = tX.index_select(0, idx), ty.index_select(0, idx)\n",
    "            opt.zero_grad(); loss = loss_fn(model(xb), yb); loss.backward(); opt.step()\n",
    "        model.eval();\n",
    "        with torch.no_grad(): vloss = loss_fn(model(vX), vy).item()\n",
    "        if vloss < best_val - 1e-6: best_val, best_state, bad = vloss, {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}, 0\n",
    "        else: bad += 1\n",
    "        if bad >= spec.patience: break\n",
    "    if best_state is not None: model.load_state_dict(best_state)\n",
    "    return model, {'best_val_mse': best_val}\n",
    "\n",
    "def evaluate_regressor(model, X, y, device):\n",
    "    model.eval(); X_t = torch.from_numpy(X).float().to(device)\n",
    "    with torch.no_grad(): pred = model(X_t).cpu().numpy()\n",
    "    rmse = float(np.sqrt(np.mean((pred - y)**2)))\n",
    "    mae = float(mean_absolute_error(y.reshape(-1), pred.reshape(-1)))\n",
    "    r2  = float(r2_score(y.reshape(-1), pred.reshape(-1)))\n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "def build_windows(frame: pd.DataFrame, feature_cols: List[str], target_col: str, context: int, horizon: int, stride: int = 1, limit: Optional[int] = None):\n",
    "    X, y = frame[feature_cols].values.astype(np.float32), frame[target_col].values.astype(np.float32)\n",
    "    T = len(frame); idxs = [(t-context, t, t, t+horizon) for t in range(context, T-horizon+1, stride)]\n",
    "    if limit is not None: idxs = idxs[:limit]\n",
    "    Xw = np.stack([X[s:e,:] for (s,e,_,__) in idxs], axis=0)\n",
    "    Yw = np.stack([y[s2:e2] for (_,__,s2,e2) in idxs], axis=0)\n",
    "    return Xw, Yw\n",
    "\n",
    "def split_train_val_test(X, y, val_frac=0.15, test_frac=0.15):\n",
    "    n = X.shape[0]; n_test = int(n*test_frac); n_val = int(n*val_frac); n_train = n - n_val - n_test\n",
    "    return X[:n_train], y[:n_train], X[n_train:n_train+n_val], y[n_train:n_train+n_val], X[n_train+n_val:], y[n_train+n_val:]\n",
    "\n",
    "def load_jena_light(context=72, horizon=36, subset_days=120):\n",
    "    # Robustly resolve Jena folder and CSV (handles hyphen/en-dash variants)\n",
    "    def _norm(s: str) -> str:\n",
    "        trans = {\n",
    "            '–': '-',  # en dash\n",
    "            '—': '-',  # em dash\n",
    "            '‑': '-',  # non-breaking hyphen\n",
    "            '−': '-',  # minus sign\n",
    "        }\n",
    "        return ''.join(trans.get(ch, ch) for ch in s).lower()\n",
    "\n",
    "    base = DATA_ROOT / 'Jena Climate 2009-2016'\n",
    "    csv = base / 'jena_climate_2009_2016.csv'\n",
    "    if not csv.exists():\n",
    "        try:\n",
    "            csv = ensure_jena_dataset()\n",
    "        except FileNotFoundError:\n",
    "            csv = None\n",
    "    if csv is None or not Path(csv).exists():\n",
    "        # Search any directory under DATA_ROOT that looks like Jena Climate\n",
    "        candidates = [d for d in DATA_ROOT.iterdir() if d.is_dir() and 'jena' in _norm(d.name)]\n",
    "        found = None\n",
    "        for d in candidates:\n",
    "            # Exact CSV name first\n",
    "            hits = list(d.rglob('jena_climate_2009_2016.csv'))\n",
    "            if hits:\n",
    "                found = hits[0]; break\n",
    "            # Fallback: any CSV containing 'jena' and '2016'\n",
    "            hits = list(d.rglob('*jena*climate*2016*.csv'))\n",
    "            if hits:\n",
    "                found = hits[0]; break\n",
    "        if found is None:\n",
    "            raise FileNotFoundError(f'Could not find Jena climate CSV under {DATA_ROOT}')\n",
    "        csv = found\n",
    "    df = pd.read_csv(csv)\n",
    "    target_col = next((c for c in df.columns if c.strip().lower().startswith('t ') or 'degc' in c.lower()), None)\n",
    "    if target_col is None: raise RuntimeError('Could not find temperature column (e.g., T (degC))')\n",
    "    num_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    if subset_days is not None: num_df = num_df.tail(subset_days * 144)\n",
    "    num_df = (num_df - num_df.mean()) / (num_df.std().replace(0, 1.0))\n",
    "    feature_cols = list(num_df.columns)\n",
    "    Xw, Yw = build_windows(num_df, feature_cols, target_col, context, horizon)\n",
    "    return split_train_val_test(Xw, Yw)\n",
    "\n",
    "def load_beijing_light(station_name='Guanyuan', context=24, horizon=6, subset_days=120):\n",
    "    base = DATA_ROOT / 'Beijing Air Quality'\n",
    "    station_file = None\n",
    "    for p in base.glob('PRSA_Data_*_20130301-20170228.csv'):\n",
    "        if station_name.lower() in p.name.lower(): station_file = p; break\n",
    "    if station_file is None: raise FileNotFoundError(f'Could not find station file containing {station_name}')\n",
    "    df = pd.read_csv(station_file)\n",
    "    target_col = 'PM2.5' if 'PM2.5' in df.columns else df.select_dtypes(include=[np.number]).columns[0]\n",
    "    num_df = df.select_dtypes(include=[np.number]).copy().ffill().bfill().fillna(0.0)\n",
    "    if subset_days is not None: num_df = num_df.tail(subset_days * 24)\n",
    "    num_df = (num_df - num_df.mean()) / (num_df.std().replace(0, 1.0))\n",
    "    feature_cols = list(num_df.columns)\n",
    "    Xw, Yw = build_windows(num_df, feature_cols, target_col, context, horizon)\n",
    "    return split_train_val_test(Xw, Yw)\n",
    "\n",
    "def load_eaf_temp_lite(context=16, horizon=1, heats_limit=5, min_rows=120):\n",
    "    path = DATA_ROOT / 'Industrial Data from the Electric Arc Furnace' / 'eaf_temp.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    if not {'HEATID','DATETIME','TEMP'}.issubset(df.columns):\n",
    "        raise RuntimeError('Missing expected columns in eaf_temp.csv')\n",
    "    df['DATETIME'] = pd.to_datetime(df['DATETIME'], errors='coerce')\n",
    "    df = df.dropna(subset=['DATETIME']).sort_values(['HEATID','DATETIME'])\n",
    "    heats = df.groupby('HEATID').size().reset_index(name='n').query('n >= @min_rows').sort_values('n', ascending=False)\n",
    "    heats = heats.head(heats_limit)['HEATID'].tolist()\n",
    "    parts = []\n",
    "    for hid in heats:\n",
    "        seg = df[df['HEATID']==hid].copy()\n",
    "        num_cols = ['TEMP'] + (['VALO2_PPM'] if 'VALO2_PPM' in seg.columns else [])\n",
    "        seg_num = seg[num_cols]\n",
    "        seg_num = (seg_num - seg_num.mean()) / (seg_num.std().replace(0, 1.0))\n",
    "        Xw, Yw = build_windows(seg_num, feature_cols=num_cols, target_col='TEMP', context=context, horizon=horizon)\n",
    "        parts.append((Xw, Yw))\n",
    "    if not parts: raise RuntimeError('No EAF heats with sufficient rows found for lite run')\n",
    "    X = np.concatenate([p[0] for p in parts], axis=0)\n",
    "    Y = np.concatenate([p[1] for p in parts], axis=0)\n",
    "    return split_train_val_test(X, Y)\n",
    "\n",
    "def jacobian_pr(model: nn.Module, X_sample: np.ndarray, device: torch.device):\n",
    "    model.eval()\n",
    "    x = torch.from_numpy(X_sample).float().to(device)\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x)\n",
    "    grads = torch.autograd.grad(y.sum(), x, create_graph=False, retain_graph=False)[0]\n",
    "    J = grads.detach().cpu().numpy().reshape(x.size(0), -1)\n",
    "    try: s = np.linalg.svd(J, compute_uv=False)\n",
    "    except np.linalg.LinAlgError:\n",
    "        M = J @ J.T; evals, _ = np.linalg.eigh(M); s = np.sqrt(np.clip(evals, 0, None))[::-1]\n",
    "    top_sv = float(s[0]) if s.size>0 else 0.0; sum_sv = float(s.sum()); pr = float((sum_sv**2)/(np.sum(s**2)+1e-8))\n",
    "    return top_sv, sum_sv, pr\n",
    "\n",
    "def run_light_task(task: str, seeds: List[int], device: torch.device, epochs: int, pr_snapshots: bool, metrics_rows: List[dict]):\n",
    "    if task == 'jena':\n",
    "        train_X, train_y, val_X, val_y, test_X, test_y = load_jena_light(72,36,120)\n",
    "        in_ch, horizon = train_X.shape[-1], train_y.shape[-1]\n",
    "        specs = [('psann_conv', TrainSpec('psann_conv', hidden=48, depth=2, kernel_size=5, epochs=epochs)), ('mlp', TrainSpec('mlp', hidden=64, depth=2, epochs=epochs))]\n",
    "        for seed in seeds:\n",
    "            seed_all(seed)\n",
    "            for name, spec in specs:\n",
    "                model = PSANNConvSpine(in_ch, spec.hidden, spec.depth, spec.kernel_size, horizon) if spec.model=='psann_conv' else MLPRegressor(train_X.shape[1]*in_ch, spec.hidden, spec.depth, horizon)\n",
    "                model, info = train_regressor(model, train_X, train_y, val_X, val_y, spec, device)\n",
    "                test_metrics = evaluate_regressor(model, test_X, test_y, device)\n",
    "                metrics_rows.append({'task':'jena_light','model':name,'seed':seed,'params':count_params(model), **info, **test_metrics})\n",
    "                if pr_snapshots and name=='psann_conv':\n",
    "                    idx = np.random.choice(test_X.shape[0], size=min(32, test_X.shape[0]), replace=False)\n",
    "                    top_sv, sum_sv, pr = jacobian_pr(model, test_X[idx], device)\n",
    "                    pr_df = pd.DataFrame([{'task':'jena_light','model':name,'seed':seed,'phase':'end','top_sv':top_sv,'sum_sv':sum_sv,'pr':pr}])\n",
    "                    pr_out = RESULTS_ROOT / 'jacobian_pr.csv'; mode = 'a' if pr_out.exists() else 'w'\n",
    "                    pr_df.to_csv(pr_out, index=False, mode=mode, header=(mode=='w'))\n",
    "    elif task == 'beijing':\n",
    "        train_X, train_y, val_X, val_y, test_X, test_y = load_beijing_light('Guanyuan',24,6,120)\n",
    "        in_ch, horizon = train_X.shape[-1], train_y.shape[-1]\n",
    "        specs = [('psann_conv', TrainSpec('psann_conv', hidden=64, depth=2, kernel_size=5, epochs=epochs)), ('mlp', TrainSpec('mlp', hidden=96, depth=2, epochs=epochs))]\n",
    "        for seed in seeds:\n",
    "            seed_all(seed)\n",
    "            for name, spec in specs:\n",
    "                model = PSANNConvSpine(in_ch, spec.hidden, spec.depth, spec.kernel_size, horizon) if spec.model=='psann_conv' else MLPRegressor(train_X.shape[1]*in_ch, spec.hidden, spec.depth, horizon)\n",
    "                model, info = train_regressor(model, train_X, train_y, val_X, val_y, spec, device)\n",
    "                test_metrics = evaluate_regressor(model, test_X, test_y, device)\n",
    "                metrics_rows.append({'task':'beijing_light','model':name,'seed':seed,'params':count_params(model), **info, **test_metrics})\n",
    "    elif task == 'eaf':\n",
    "        train_X, train_y, val_X, val_y, test_X, test_y = load_eaf_temp_lite(16,1,5,120)\n",
    "        in_ch, horizon = train_X.shape[-1], train_y.shape[-1]\n",
    "        specs = [('psann_conv', TrainSpec('psann_conv', hidden=32, depth=2, kernel_size=3, epochs=max(epochs,8))), ('mlp', TrainSpec('mlp', hidden=48, depth=2, epochs=max(epochs,8)))]\n",
    "        for seed in seeds:\n",
    "            seed_all(seed)\n",
    "            for name, spec in specs:\n",
    "                model = PSANNConvSpine(in_ch, spec.hidden, spec.depth, spec.kernel_size, horizon) if spec.model=='psann_conv' else MLPRegressor(train_X.shape[1]*in_ch, spec.hidden, spec.depth, horizon)\n",
    "                model, info = train_regressor(model, train_X, train_y, val_X, val_y, spec, device)\n",
    "                test_metrics = evaluate_regressor(model, test_X, test_y, device)\n",
    "                metrics_rows.append({'task':'eaf_temp_lite','model':name,'seed':seed,'params':count_params(model), **info, **test_metrics})\n",
    "    else:\n",
    "        raise ValueError(f'Unknown task: {task}')\n",
    "\n",
    "def run_all(tasks, seeds, epochs, device_str, pr_snapshots):\n",
    "    maybe_extract_datasets_zip()\n",
    "    device = pick_device(device_str)\n",
    "    print(f'[env] DATA_ROOT={DATA_ROOT}')\n",
    "    print(f'[env] RESULTS_ROOT={RESULTS_ROOT}')\n",
    "    print(f'[env] device={device}')\n",
    "    metrics_rows: List[dict] = []\n",
    "    for task in tasks:\n",
    "        print(f'[run] task={task}')\n",
    "        run_light_task(task, seeds, device, epochs, pr_snapshots, metrics_rows)\n",
    "    if metrics_rows:\n",
    "        df = pd.DataFrame(metrics_rows)\n",
    "        out = RESULTS_ROOT / 'metrics.csv'\n",
    "        df.to_csv(out, index=False)\n",
    "        display(df.head()); print(f'[done] Wrote metrics to {out}')\n",
    "    else:\n",
    "        print('[warn] No metrics collected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "run_all(TASKS, SEEDS, EPOCHS, DEVICE, PR_SNAPSHOTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
