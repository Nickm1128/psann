{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f3d6a743",
      "metadata": {
        "id": "f3d6a743"
      },
      "source": [
        "# ResPSANN Compute-Parity Experiments (Colab Runner)\n",
        "\n",
        "This notebook orchestrates the experiments described in `plan.txt` using the datasets summarised in `data_descriptions.txt`. Execute it inside Google Colab (GPU runtime recommended).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4740b594",
      "metadata": {
        "id": "4740b594"
      },
      "source": [
        "## Run Checklist\n",
        "- Prefer Google Colab with a GPU runtime (recommended) before running any experiments.\n",
        "- Let the setup cell install the latest published `psann` package via `pip`; no repository clone is required.\n",
        "- Upload or mount the dataset directory so that `DATA_ROOT` points to it (defaults to `<working dir>/datasets`).\n",
        "- Adjust `GLOBAL_CONFIG` and the experiment toggles before launching training to stay within the Colab budget.\n",
        "- Keep the heavy training cells disabled until you are ready to execute them in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2b559b11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b559b11",
        "outputId": "639af6cc-ade9-425d-f1d1-bd59fb6165d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab runtime         : True\n",
            "Project root          : /content\n",
            "Dataset root          : /content/datasets\n",
            "Results directory     : /content/colab_results\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "DEFAULT_PROJECT_ROOT = Path(\"/content\") if COLAB else Path.cwd()\n",
        "PROJECT_ROOT = Path(os.getenv(\"PSANN_PROJECT_ROOT\", DEFAULT_PROJECT_ROOT)).resolve()\n",
        "\n",
        "DATA_ROOT = Path(os.getenv(\"PSANN_DATA_ROOT\", PROJECT_ROOT / \"datasets\")).resolve()\n",
        "RESULTS_ROOT = Path(os.getenv(\"PSANN_RESULTS_ROOT\", PROJECT_ROOT / \"colab_results\")).resolve()\n",
        "FIGURE_ROOT = RESULTS_ROOT / \"figures\"\n",
        "\n",
        "RESULTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "FIGURE_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not DATA_ROOT.exists():\n",
        "    print(f\"[WARN] DATA_ROOT {DATA_ROOT} does not exist yet. Upload datasets or update PSANN_DATA_ROOT.\")\n",
        "\n",
        "print(f\"Colab runtime         : {COLAB}\")\n",
        "print(f\"Project root          : {PROJECT_ROOT}\")\n",
        "print(f\"Dataset root          : {DATA_ROOT}\")\n",
        "print(f\"Results directory     : {RESULTS_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "1c824d6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c824d6c",
        "outputId": "5e7b7a76-8aac-4f25-e6d4-c3c6ae8de937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets already present at /content/datasets\n"
          ]
        }
      ],
      "source": [
        "# --- Robust extraction for your datasets.zip layout ---\n",
        "import zipfile\n",
        "import shutil\n",
        "import re\n",
        "from pathlib import Path, PureWindowsPath\n",
        "\n",
        "# Fallbacks if not already defined in your notebook\n",
        "try:\n",
        "    PROJECT_ROOT\n",
        "except NameError:\n",
        "    PROJECT_ROOT = Path(\"/content\")\n",
        "try:\n",
        "    DATA_ROOT\n",
        "except NameError:\n",
        "    DATA_ROOT = PROJECT_ROOT / \"datasets\"\n",
        "\n",
        "zip_path = PROJECT_ROOT / \"datasets.zip\"\n",
        "\n",
        "# Canonical names your code expects\n",
        "EXPECTED_FOLDERS = [\n",
        "    \"Industrial Data from the Electric Arc Furnace\",\n",
        "    \"Beijing Air Quality\",\n",
        "    \"Human Activity Recognition\",\n",
        "    \"Jena Climate 2009-2016\",  # we'll normalize any en/em/Unicode minus to a hyphen\n",
        "    \"Kaggle Rossmann Store Sales\",\n",
        "]\n",
        "\n",
        "def _normalize_name(s: str) -> str:\n",
        "    # unify hyphen-like chars, collapse whitespace, lowercase\n",
        "    s = (s.replace(\"\\u2013\", \"-\")  # en dash\n",
        "           .replace(\"\\u2014\", \"-\")  # em dash\n",
        "           .replace(\"\\u2212\", \"-\")  # minus\n",
        "           .replace(\"\\xa0\", \" \"))   # non-breaking space\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
        "    return s\n",
        "\n",
        "def _safe_mkdir(path: Path) -> None:\n",
        "    \"\"\"\n",
        "    Create directory `path`, removing any FILE that blocks directory creation\n",
        "    at this path or any ancestor.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        return\n",
        "    except NotADirectoryError:\n",
        "        # Find any ancestor that's a file and remove it.\n",
        "        # Include the path itself first, then walk upward.\n",
        "        for ancestor in [path, *path.parents]:\n",
        "            try:\n",
        "                if ancestor.exists() and ancestor.is_file():\n",
        "                    ancestor.unlink()\n",
        "            except Exception:\n",
        "                # If we can't remove, re-raise later when mkdir fails again\n",
        "                pass\n",
        "        # Try once more after clearing blockers\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _canonicalize_top_level_dirs(root: Path, expected_names: list[str]) -> None:\n",
        "    \"\"\"\n",
        "    If top-level dirs exist with dash/space variants, rename them\n",
        "    to the canonical EXPECTED_FOLDERS names so downstream code works.\n",
        "    \"\"\"\n",
        "    if not root.exists():\n",
        "        return\n",
        "    # Map normalized->actual path for current top-level dirs\n",
        "    current = { _normalize_name(p.name): p for p in root.iterdir() if p.is_dir() }\n",
        "    for exp in expected_names:\n",
        "        canonical = root / exp\n",
        "        if canonical.exists():\n",
        "            continue\n",
        "        norm = _normalize_name(exp)\n",
        "        if norm in current and current[norm].exists():\n",
        "            src = current[norm]\n",
        "            # Avoid rename conflict: if a file with target name exists, remove it\n",
        "            if canonical.exists() and canonical.is_file():\n",
        "                canonical.unlink()\n",
        "            print(f\"[rename] {src.name} -> {exp}\")\n",
        "            src.rename(canonical)\n",
        "\n",
        "def extract_datasets(zip_path: Path, target_root: Path) -> None:\n",
        "    scratch_root = target_root.parent / \"_datasets_unpack_tmp\"\n",
        "    if scratch_root.exists():\n",
        "        shutil.rmtree(scratch_root)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        for entry in zf.infolist():\n",
        "            # Normalize path parts from the ZIP (handles both / and \\ separators)\n",
        "            parts = PureWindowsPath(entry.filename).parts\n",
        "            if not parts or parts[0].lower() != \"datasets\":\n",
        "                continue\n",
        "            rel_parts = parts[1:]\n",
        "            if not rel_parts:\n",
        "                continue\n",
        "\n",
        "            dest = scratch_root.joinpath(*rel_parts)\n",
        "\n",
        "            # Treat as directory if zip marks it so OR the path text ends with a slash/backslash\n",
        "            is_dir_entry = entry.is_dir() or entry.filename.endswith(\"/\") or entry.filename.endswith(\"\\\\\")\n",
        "            if is_dir_entry:\n",
        "                _safe_mkdir(dest)\n",
        "            else:\n",
        "                # Ensure parents exist, removing any file that blocks dir creation\n",
        "                _safe_mkdir(dest.parent)\n",
        "                # Extract the file\n",
        "                with zf.open(entry, \"r\") as src, open(dest, \"wb\") as dst:\n",
        "                    shutil.copyfileobj(src, dst)\n",
        "\n",
        "    # Replace/refresh target_root\n",
        "    if target_root.exists():\n",
        "        shutil.rmtree(target_root)\n",
        "    scratch_root.rename(target_root)\n",
        "\n",
        "    # Canonicalize top-level dir names (e.g., Jena dash variants)\n",
        "    _canonicalize_top_level_dirs(target_root, EXPECTED_FOLDERS)\n",
        "\n",
        "def datasets_ready(root: Path) -> bool:\n",
        "    if not root.exists():\n",
        "        return False\n",
        "    # Accept either exact or normalized matches for robustness\n",
        "    have = { _normalize_name(p.name) for p in root.iterdir() if p.is_dir() }\n",
        "    need = { _normalize_name(n) for n in EXPECTED_FOLDERS }\n",
        "    return need.issubset(have)\n",
        "\n",
        "# --- Clean up stray \"datasets\\...\" artefacts before extraction (from Windows zips) ---\n",
        "for leftover in PROJECT_ROOT.iterdir():\n",
        "    if \"\\\\\" in leftover.name and leftover.name.lower().startswith(\"datasets\"):\n",
        "        if leftover.is_dir():\n",
        "            shutil.rmtree(leftover)\n",
        "        else:\n",
        "            leftover.unlink()\n",
        "\n",
        "# --- Run ---\n",
        "if datasets_ready(DATA_ROOT):\n",
        "    print(f\"Datasets already present at {DATA_ROOT}\")\n",
        "elif zip_path.exists():\n",
        "    print(f\"Extracting {zip_path} (normalising Windows/Unicode paths)…\")\n",
        "    extract_datasets(zip_path, DATA_ROOT)\n",
        "    if datasets_ready(DATA_ROOT):\n",
        "        print(f\"Extraction complete. DATA_ROOT now available at {DATA_ROOT}\")\n",
        "        # Optional: quick sanity peek\n",
        "        for p in sorted(DATA_ROOT.iterdir()):\n",
        "            if p.is_dir():\n",
        "                print(\" -\", p.name)\n",
        "    else:\n",
        "        print(\"[WARN] Extraction finished but expected folders are still missing.\")\n",
        "else:\n",
        "    print(f\"Archive {zip_path} not found. Upload datasets.zip or mount the datasets directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "07209e39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07209e39",
        "outputId": "ccd2c29a-70c2-4eab-a5b1-f5aaeef26597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing psann and supporting packages...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "def install_dependencies():\n",
        "    base_packages = [\n",
        "        \"psann\",\n",
        "        \"pandas>=2.0\",\n",
        "        \"numpy>=1.24\",\n",
        "        \"scikit-learn>=1.3\",\n",
        "        \"torch>=2.1\",\n",
        "        \"torchvision>=0.16\",\n",
        "        \"torchaudio>=2.1\",\n",
        "        \"lightgbm>=4.0\",\n",
        "        \"xgboost>=1.7\",\n",
        "        \"catboost>=1.2\",\n",
        "        \"shap>=0.44\",\n",
        "        \"matplotlib>=3.7\",\n",
        "        \"seaborn>=0.13\",\n",
        "        \"plotly>=5.18\",\n",
        "        \"imbalanced-learn>=0.12\",\n",
        "        \"tqdm>=4.66\",\n",
        "        \"einops>=0.7\",\n",
        "        \"rich>=13.7\",\n",
        "    ]\n",
        "    print(\"Installing psann and supporting packages...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + base_packages)\n",
        "\n",
        "\n",
        "if COLAB:\n",
        "    install_dependencies()\n",
        "else:\n",
        "    print(\"Skipping dependency installation because we are not inside Colab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "8c4441a6",
      "metadata": {
        "id": "8c4441a6"
      },
      "outputs": [],
      "source": [
        "# Core dependencies used across the notebook\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Literal\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "772afbf3",
      "metadata": {
        "id": "772afbf3"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs: int\n",
        "    batch_size: int\n",
        "    learning_rate: float\n",
        "    weight_decay: float = 0.0\n",
        "    max_minutes: Optional[float] = None\n",
        "    early_stopping: bool = True\n",
        "    patience: int = 10\n",
        "    gradient_clip: Optional[float] = None\n",
        "    scheduler: Optional[str] = None\n",
        "    scheduler_params: Optional[Dict[str, Any]] = None\n",
        "    warmup_steps: int = 0\n",
        "    max_batches_per_epoch: Optional[int] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelSpec:\n",
        "    name: str\n",
        "    builder: Callable[[Tuple[int, ...], int, Dict[str, Any]], nn.Module]\n",
        "    train_config: TrainConfig\n",
        "    task_type: Literal[\"regression\", \"classification\", \"multitask\"]\n",
        "    input_kind: Literal[\"tabular\", \"sequence\"]\n",
        "    group: str = \"baseline\"\n",
        "    extra: Dict[str, Any] = field(default_factory=dict)\n",
        "    param_target: Optional[int] = None\n",
        "    notes: str = \"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DatasetBundle:\n",
        "    name: str\n",
        "    task_type: Literal[\"regression\", \"classification\", \"multitask\"]\n",
        "    input_kind: Literal[\"tabular\", \"sequence\"]\n",
        "    feature_names: List[str]\n",
        "    target_names: List[str]\n",
        "    train: Dict[str, np.ndarray]\n",
        "    val: Dict[str, np.ndarray]\n",
        "    test: Dict[str, np.ndarray]\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    def summary(self) -> Dict[str, Any]:\n",
        "        info = {\n",
        "            \"name\": self.name,\n",
        "            \"task_type\": self.task_type,\n",
        "            \"input_kind\": self.input_kind,\n",
        "            \"n_train\": len(self.train[\"X\"]),\n",
        "            \"n_val\": len(self.val[\"X\"]),\n",
        "            \"n_test\": len(self.test[\"X\"]),\n",
        "            \"input_shape\": tuple(self.train[\"X\"].shape[1:]),\n",
        "            \"target_shape\": tuple(self.train[\"y\"].shape[1:]) if self.train[\"y\"].ndim > 1 else (),\n",
        "        }\n",
        "        info.update({f\"meta_{k}\": v for k, v in self.metadata.items() if isinstance(v, (int, float, str))})\n",
        "        return info\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExperimentResult:\n",
        "    dataset: str\n",
        "    task: str\n",
        "    model: str\n",
        "    group: str\n",
        "    split: str\n",
        "    metrics: Dict[str, float]\n",
        "    params: int\n",
        "    train_wall_seconds: float\n",
        "    notes: str = \"\"\n",
        "\n",
        "\n",
        "class ResultLogger:\n",
        "    def __init__(self) -> None:\n",
        "        self._rows: List[ExperimentResult] = []\n",
        "\n",
        "    def append(self, row: ExperimentResult) -> None:\n",
        "        self._rows.append(row)\n",
        "\n",
        "    def to_frame(self) -> pd.DataFrame:\n",
        "        records = []\n",
        "        for row in self._rows:\n",
        "            rec = {\n",
        "                \"dataset\": row.dataset,\n",
        "                \"task\": row.task,\n",
        "                \"model\": row.model,\n",
        "                \"group\": row.group,\n",
        "                \"split\": row.split,\n",
        "                \"params\": row.params,\n",
        "                \"train_wall_seconds\": row.train_wall_seconds,\n",
        "                \"notes\": row.notes,\n",
        "            }\n",
        "            rec.update(row.metrics)\n",
        "            records.append(rec)\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "RESULT_LOGGER = ResultLogger()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "94828a93",
      "metadata": {
        "id": "94828a93"
      },
      "outputs": [],
      "source": [
        "def load_jena_climate(data_root: Path) -> pd.DataFrame:\n",
        "    path = data_root / \"Jena Climate 2009-2016\" / \"jena_climate_2009_2016.csv\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Jena climate CSV not found at {path}\")\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"datetime\"] = pd.to_datetime(df[\"Date Time\"], dayfirst=True)\n",
        "    df = df.drop(columns=[\"Date Time\"])\n",
        "    numeric_cols = [col for col in df.columns if col != \"datetime\"]\n",
        "    df[numeric_cols] = df[numeric_cols].astype(np.float32)\n",
        "    df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_jena_bundle(\n",
        "    df: pd.DataFrame,\n",
        "    target: str = \"T (degC)\",\n",
        "    context_steps: int = 72,\n",
        "    horizon_steps: int = 36,\n",
        "    resample_factor: int = 1,\n",
        ") -> DatasetBundle:\n",
        "    df = df.copy()\n",
        "    if resample_factor > 1:\n",
        "        df = df.iloc[::resample_factor].reset_index(drop=True)\n",
        "    df = add_calendar_features(df, \"datetime\")\n",
        "    feature_cols = [c for c in df.columns if c not in (\"datetime\", target)]\n",
        "    df[feature_cols] = df[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    values = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    target_values = df[target].to_numpy(dtype=np.float32)\n",
        "    timestamps = df[\"datetime\"].to_numpy()\n",
        "\n",
        "    windows = []\n",
        "    targets = []\n",
        "    ts_list = []\n",
        "    for idx in range(context_steps, len(df) - horizon_steps):\n",
        "        window = values[idx - context_steps : idx]\n",
        "        target_value = target_values[idx + horizon_steps]\n",
        "        windows.append(window)\n",
        "        targets.append(target_value)\n",
        "        ts_list.append(timestamps[idx])\n",
        "    X = np.stack(windows)\n",
        "    y = np.asarray(targets, dtype=np.float32)[:, None]\n",
        "    ts = np.asarray(ts_list)\n",
        "\n",
        "    df_windows = pd.DataFrame({\"datetime\": ts})\n",
        "    train_df, val_df, test_df = train_val_test_split_by_time(\n",
        "        df_windows, \"datetime\", \"2015-01-01\", \"2016-01-01\"\n",
        "    )\n",
        "    train_idx = train_df.index.to_numpy()\n",
        "    val_idx = val_df.index.to_numpy()\n",
        "    test_idx = test_df.index.to_numpy()\n",
        "\n",
        "    target_slug = (\n",
        "        target.lower()\n",
        "        .replace(\" \", \"\")\n",
        "        .replace(\"(\", \"\")\n",
        "        .replace(\")\", \"\")\n",
        "        .replace(\"/\", \"\")\n",
        "    )\n",
        "    bundle_name = f\"Jena_{target_slug}_{context_steps}ctx_{horizon_steps}h\"\n",
        "\n",
        "    bundle = DatasetBundle(\n",
        "        name=bundle_name,\n",
        "        task_type=\"regression\",\n",
        "        input_kind=\"sequence\",\n",
        "        feature_names=feature_cols,\n",
        "        target_names=[target],\n",
        "        train={\"X\": X[train_idx], \"y\": y[train_idx]},\n",
        "        val={\"X\": X[val_idx], \"y\": y[val_idx]},\n",
        "        test={\"X\": X[test_idx], \"y\": y[test_idx]},\n",
        "        metadata={\n",
        "            \"context_steps\": context_steps,\n",
        "            \"horizon_steps\": horizon_steps,\n",
        "            \"resample_factor\": resample_factor,\n",
        "        },\n",
        "    )\n",
        "    return bundle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "fdf214b6",
      "metadata": {
        "id": "fdf214b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de1d4c3-fcd7-4601-b793-c5f25cc88e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-2915871693.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  X_train = pd.read_csv(base / \"train\" / \"X_train.txt\", sep='\\s+', header=None)\n",
            "/tmp/ipython-input-2915871693.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  X_test = pd.read_csv(base / \"test\" / \"X_test.txt\", sep='\\s+', header=None)\n"
          ]
        }
      ],
      "source": [
        "def load_har_engineered(data_root: Path):\n",
        "    base = data_root / \"Human Activity Recognition\" / \"UCI HAR Dataset\"\n",
        "    X_train = pd.read_csv(base / \"train\" / \"X_train.txt\", sep='\\s+', header=None)\n",
        "    y_train = pd.read_csv(base / \"train\" / \"y_train.txt\", header=None).squeeze(\"columns\")\n",
        "    subject_train = pd.read_csv(base / \"train\" / \"subject_train.txt\", header=None).squeeze(\"columns\")\n",
        "\n",
        "    X_test = pd.read_csv(base / \"test\" / \"X_test.txt\", sep='\\s+', header=None)\n",
        "    y_test = pd.read_csv(base / \"test\" / \"y_test.txt\", header=None).squeeze(\"columns\")\n",
        "    subject_test = pd.read_csv(base / \"test\" / \"subject_test.txt\", header=None).squeeze(\"columns\")\n",
        "\n",
        "    y_train = y_train.values.astype(int) - 1\n",
        "    y_test = y_test.values.astype(int) - 1\n",
        "\n",
        "    features = (base / \"features.txt\").read_text().strip().splitlines()\n",
        "    feature_names = [line.split()[1] for line in features]\n",
        "\n",
        "    X_train.columns = feature_names\n",
        "    X_test.columns = feature_names\n",
        "\n",
        "    train_df = X_train.copy()\n",
        "    test_df = X_test.copy()\n",
        "    train_df[\"label\"] = y_train\n",
        "    train_df[\"subject\"] = subject_train.values\n",
        "    test_df[\"label\"] = y_test\n",
        "    test_df[\"subject\"] = subject_test.values\n",
        "\n",
        "    return train_df, test_df, feature_names\n",
        "\n",
        "\n",
        "def prepare_har_engineered_bundle(\n",
        "    train_df: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    feature_names: List[str],\n",
        "    val_fraction: float = 0.15,\n",
        ") -> DatasetBundle:\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "    X = train_df[feature_names].to_numpy(dtype=np.float32)\n",
        "    y = train_df[\"label\"].to_numpy(dtype=np.int64)\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_fraction, random_state=GLOBAL_CONFIG[\"seed\"])\n",
        "    train_idx, val_idx = next(splitter.split(X, y))\n",
        "\n",
        "    X_train = X[train_idx]\n",
        "    y_train = y[train_idx][:, None]\n",
        "    X_val = X[val_idx]\n",
        "    y_val = y[val_idx][:, None]\n",
        "\n",
        "    X_test = test_df[feature_names].to_numpy(dtype=np.float32)\n",
        "    y_test = test_df[\"label\"].to_numpy(dtype=np.int64)[:, None]\n",
        "\n",
        "    bundle = DatasetBundle(\n",
        "        name=\"HAR_engineered\",\n",
        "        task_type=\"classification\",\n",
        "        input_kind=\"tabular\",\n",
        "        feature_names=feature_names,\n",
        "        target_names=[\"activity\"],\n",
        "        train={\"X\": X_train, \"y\": y_train},\n",
        "        val={\"X\": X_val, \"y\": y_val},\n",
        "        test={\"X\": X_test, \"y\": y_test},\n",
        "        metadata={\n",
        "            \"n_classes\": 6,\n",
        "            \"label_mapping\": {\n",
        "                0: \"WALKING\",\n",
        "                1: \"WALKING_UPSTAIRS\",\n",
        "                2: \"WALKING_DOWNSTAIRS\",\n",
        "                3: \"SITTING\",\n",
        "                4: \"STANDING\",\n",
        "                5: \"LAYING\",\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "    return bundle\n",
        "\n",
        "\n",
        "def load_har_raw_sequences(data_root: Path):\n",
        "    base = data_root / \"Human Activity Recognition\" / \"UCI HAR Dataset\"\n",
        "    axes = [\n",
        "        \"body_acc_x\",\n",
        "        \"body_acc_y\",\n",
        "        \"body_acc_z\",\n",
        "        \"body_gyro_x\",\n",
        "        \"body_gyro_y\",\n",
        "        \"body_gyro_z\",\n",
        "        \"total_acc_x\",\n",
        "        \"total_acc_y\",\n",
        "        \"total_acc_z\",\n",
        "    ]\n",
        "\n",
        "    def load_split(split: str):\n",
        "        signals = []\n",
        "        for axis in axes:\n",
        "            path = base / split / \"Inertial Signals\" / f\"{axis}_{split}.txt\"\n",
        "            arr = np.loadtxt(path)\n",
        "            signals.append(arr[:, :, None])\n",
        "        X = np.concatenate(signals, axis=2).astype(np.float32)\n",
        "        y = np.loadtxt(base / split / f\"y_{split}.txt\").astype(int) - 1\n",
        "        return X, y\n",
        "\n",
        "    X_train, y_train = load_split(\"train\")\n",
        "    X_test, y_test = load_split(\"test\")\n",
        "    return X_train, y_train, X_test, y_test, axes\n",
        "\n",
        "\n",
        "def prepare_har_raw_bundle(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    val_fraction: float = 0.15,\n",
        ") -> DatasetBundle:\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_fraction, random_state=GLOBAL_CONFIG[\"seed\"])\n",
        "    train_idx, val_idx = next(splitter.split(X_train, y_train))\n",
        "    bundle = DatasetBundle(\n",
        "        name=\"HAR_raw_sequence\",\n",
        "        task_type=\"classification\",\n",
        "        input_kind=\"sequence\",\n",
        "        feature_names=[f\"axis_{i}\" for i in range(X_train.shape[2])],\n",
        "        target_names=[\"activity\"],\n",
        "        train={\"X\": X_train[train_idx], \"y\": y_train[train_idx][:, None]},\n",
        "        val={\"X\": X_train[val_idx], \"y\": y_train[val_idx][:, None]},\n",
        "        test={\"X\": X_test, \"y\": y_test[:, None]},\n",
        "        metadata={\n",
        "            \"sequence_length\": X_train.shape[1],\n",
        "            \"n_channels\": X_train.shape[2],\n",
        "            \"n_classes\": 6,\n",
        "        },\n",
        "    )\n",
        "    return bundle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "b1da5943",
      "metadata": {
        "id": "b1da5943"
      },
      "outputs": [],
      "source": [
        "def load_rossmann_frames(data_root: Path):\n",
        "    base = data_root / \"Kaggle Rossmann Store Sales\" / \"rossmann-store-sales\"\n",
        "    train_path = base / \"train.csv\"\n",
        "    test_path = base / \"test.csv\"\n",
        "    store_path = base / \"store.csv\"\n",
        "    train = pd.read_csv(train_path, parse_dates=[\"Date\"])\n",
        "    test = pd.read_csv(test_path, parse_dates=[\"Date\"])\n",
        "    store = pd.read_csv(store_path)\n",
        "    return train, test, store\n",
        "\n",
        "\n",
        "def is_promo2_active(row: pd.Series) -> int:\n",
        "    if not row.get(\"Promo2\", 0):\n",
        "        return 0\n",
        "    month = row[\"Date\"].month\n",
        "    if isinstance(row[\"PromoInterval\"], str) and row[\"PromoInterval\"]:\n",
        "        month_map = {name: idx for idx, name in enumerate([\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], start=1)}\n",
        "        promo_months = [month_map.get(m.strip(), 0) for m in row[\"PromoInterval\"].split(\",\")]\n",
        "        return int(month in promo_months)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def preprocess_rossmann(train: pd.DataFrame, store: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], str]:\n",
        "    df = train.merge(store, on=\"Store\", how=\"left\")\n",
        "    df = df[df[\"Open\"] != 0].copy()\n",
        "\n",
        "    median_distance = df[\"CompetitionDistance\"].median()\n",
        "    df[\"CompetitionDistance\"] = df[\"CompetitionDistance\"].fillna(median_distance)\n",
        "    df[\"CompetitionOpenSinceYear\"] = df[\"CompetitionOpenSinceYear\"].fillna(df[\"CompetitionOpenSinceYear\"].median())\n",
        "    df[\"CompetitionOpenSinceMonth\"] = df[\"CompetitionOpenSinceMonth\"].fillna(df[\"CompetitionOpenSinceMonth\"].median())\n",
        "    df[\"Promo2SinceWeek\"] = df[\"Promo2SinceWeek\"].fillna(0)\n",
        "    df[\"Promo2SinceYear\"] = df[\"Promo2SinceYear\"].fillna(0)\n",
        "    df[\"PromoInterval\"] = df[\"PromoInterval\"].fillna(\"\")\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df[\"Year\"] = df[\"Date\"].dt.year\n",
        "    df[\"Month\"] = df[\"Date\"].dt.month\n",
        "    df[\"Day\"] = df[\"Date\"].dt.day\n",
        "    df[\"WeekOfYear\"] = df[\"Date\"].dt.isocalendar().week.astype(int)\n",
        "    df[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
        "\n",
        "    df[\"IsPromo2Month\"] = df.apply(is_promo2_active, axis=1)\n",
        "\n",
        "    state_holiday_map = {\"0\": \"None\", \"a\": \"PublicHoliday\", \"b\": \"EasterHoliday\", \"c\": \"Christmas\"}\n",
        "    df[\"StateHoliday\"] = df[\"StateHoliday\"].replace(state_holiday_map)\n",
        "\n",
        "    categorical_cols = [\"StoreType\", \"Assortment\", \"StateHoliday\", \"PromoInterval\"]\n",
        "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    df[\"CustomersLag7\"] = df.groupby(\"Store\")[\"Customers\"].shift(7)\n",
        "    df[\"SalesLag7\"] = df.groupby(\"Store\")[\"Sales\"].shift(7)\n",
        "    df[\"SalesMA14\"] = df.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.rolling(14, min_periods=1).mean())\n",
        "    df[\"PromoMovingAvg\"] = df.groupby(\"Store\")[\"Promo\"].transform(lambda s: s.rolling(30, min_periods=1).mean())\n",
        "\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    feature_cols = [c for c in df.columns if c not in (\"Sales\", \"Date\")]\n",
        "    target_col = \"Sales\"\n",
        "    return df, feature_cols, target_col\n",
        "\n",
        "\n",
        "def prepare_rossmann_bundle(df: pd.DataFrame, feature_cols: List[str], target_col: str) -> DatasetBundle:\n",
        "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "    unique_dates = np.sort(df[\"Date\"].unique())\n",
        "    if unique_dates.size < 3:\n",
        "        raise ValueError(\"Rossmann dataset requires at least three distinct dates to form train/val/test splits.\")\n",
        "\n",
        "    train_cut_idx = max(1, int(0.8 * unique_dates.size))\n",
        "    val_cut_idx = max(train_cut_idx + 1, int(0.9 * unique_dates.size))\n",
        "    if val_cut_idx >= unique_dates.size:\n",
        "        val_cut_idx = unique_dates.size - 1\n",
        "    train_end = unique_dates[train_cut_idx]\n",
        "    val_end = unique_dates[val_cut_idx]\n",
        "\n",
        "    train_mask = df[\"Date\"] < train_end\n",
        "    val_mask = (df[\"Date\"] >= train_end) & (df[\"Date\"] < val_end)\n",
        "    test_mask = df[\"Date\"] >= val_end\n",
        "\n",
        "    train_df = df[train_mask].copy()\n",
        "    val_df = df[val_mask].copy()\n",
        "    test_df = df[test_mask].copy()\n",
        "\n",
        "    if train_df.empty or val_df.empty or test_df.empty:\n",
        "        raise ValueError(\"Rossmann split produced an empty partition; adjust quantiles or check input data.\")\n",
        "\n",
        "    X_train = train_df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    y_train = train_df[target_col].to_numpy(dtype=np.float32)[:, None]\n",
        "    X_val = val_df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    y_val = val_df[target_col].to_numpy(dtype=np.float32)[:, None]\n",
        "    X_test = test_df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    y_test = test_df[target_col].to_numpy(dtype=np.float32)[:, None]\n",
        "\n",
        "    feature_mean = X_train.mean(axis=0, keepdims=True)\n",
        "    feature_std = X_train.std(axis=0, keepdims=True)\n",
        "    feature_std = np.where(feature_std < 1e-6, 1.0, feature_std)\n",
        "\n",
        "    target_mean = y_train.mean(axis=0, keepdims=True)\n",
        "    target_std = y_train.std(axis=0, keepdims=True)\n",
        "    target_std = np.where(target_std < 1e-6, 1.0, target_std)\n",
        "\n",
        "    def _normalize(arr: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "        return ((arr - mean) / std).astype(np.float32)\n",
        "\n",
        "    X_train = _normalize(X_train, feature_mean, feature_std)\n",
        "    X_val = _normalize(X_val, feature_mean, feature_std)\n",
        "    X_test = _normalize(X_test, feature_mean, feature_std)\n",
        "\n",
        "    y_train_norm = _normalize(y_train, target_mean, target_std)\n",
        "    y_val_norm = _normalize(y_val, target_mean, target_std)\n",
        "    y_test_norm = _normalize(y_test, target_mean, target_std)\n",
        "\n",
        "    bundle = DatasetBundle(\n",
        "        name=\"Rossmann_sales\",\n",
        "        task_type=\"regression\",\n",
        "        input_kind=\"tabular\",\n",
        "        feature_names=feature_cols,\n",
        "        target_names=[target_col],\n",
        "        train={\"X\": X_train, \"y\": y_train_norm},\n",
        "        val={\"X\": X_val, \"y\": y_val_norm},\n",
        "        test={\"X\": X_test, \"y\": y_test_norm},\n",
        "        metadata={\n",
        "            \"train_range\": [str(train_df[\"Date\"].min()), str(train_df[\"Date\"].max())],\n",
        "            \"val_range\": [str(val_df[\"Date\"].min()), str(val_df[\"Date\"].max())],\n",
        "            \"test_range\": [str(test_df[\"Date\"].min()), str(test_df[\"Date\"].max())],\n",
        "            \"feature_scaler\": {\n",
        "                \"mean\": feature_mean.flatten().astype(np.float32).tolist(),\n",
        "                \"std\": feature_std.flatten().astype(np.float32).tolist(),\n",
        "            },\n",
        "            \"target_scaler\": {\n",
        "                \"mean\": target_mean.flatten().astype(np.float32).tolist(),\n",
        "                \"std\": target_std.flatten().astype(np.float32).tolist(),\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "    return bundle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "b83bcf91",
      "metadata": {
        "id": "b83bcf91"
      },
      "outputs": [],
      "source": [
        "def load_beijing_stations(data_root: Path) -> Dict[str, pd.DataFrame]:\n",
        "    base = data_root / \"Beijing Air Quality\"\n",
        "    if not base.exists():\n",
        "        raise FileNotFoundError(f\"Beijing Air Quality directory not found at {base}\")\n",
        "    stations: Dict[str, pd.DataFrame] = {}\n",
        "    for csv_path in base.glob(\"PRSA_Data_*.csv\"):\n",
        "        station_name = csv_path.stem.replace(\"PRSA_Data_\", \"\")\n",
        "        print(f\"Loading Beijing station {station_name}...\")\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df[\"datetime\"] = pd.to_datetime(\n",
        "            df[[\"year\", \"month\", \"day\", \"hour\"]].rename(columns=str)\n",
        "        )\n",
        "        df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
        "        if \"No\" in df.columns:\n",
        "            df = df.drop(columns=[\"No\"])\n",
        "        stations[station_name] = df\n",
        "    return stations\n",
        "\n",
        "\n",
        "def preprocess_beijing_station(df: pd.DataFrame, target_col: str = \"PM2.5\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    df = df.copy()\n",
        "    pollutant_cols = [\"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\", \"O3\"]\n",
        "    meteorology_cols = [\"PRES\", \"DEWP\", \"TEMP\", \"RAIN\", \"WSPM\"]\n",
        "    for col in pollutant_cols + meteorology_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    mask = df[pollutant_cols + meteorology_cols].isna()\n",
        "    df[pollutant_cols + meteorology_cols] = df[pollutant_cols + meteorology_cols].interpolate(limit=6, limit_direction=\"both\")\n",
        "    df[pollutant_cols + meteorology_cols] = df[pollutant_cols + meteorology_cols].ffill().bfill()\n",
        "\n",
        "    calendar = pd.DataFrame(\n",
        "        {\n",
        "            \"hour\": df[\"datetime\"].dt.hour,\n",
        "            \"dow\": df[\"datetime\"].dt.dayofweek,\n",
        "            \"month\": df[\"datetime\"].dt.month,\n",
        "        }\n",
        "    )\n",
        "    calendar[\"hour_sin\"] = np.sin(2 * np.pi * calendar[\"hour\"] / 24.0)\n",
        "    calendar[\"hour_cos\"] = np.cos(2 * np.pi * calendar[\"hour\"] / 24.0)\n",
        "    calendar[\"dow_sin\"] = np.sin(2 * np.pi * calendar[\"dow\"] / 7.0)\n",
        "    calendar[\"dow_cos\"] = np.cos(2 * np.pi * calendar[\"dow\"] / 7.0)\n",
        "    calendar[\"month_sin\"] = np.sin(2 * np.pi * calendar[\"month\"] / 12.0)\n",
        "    calendar[\"month_cos\"] = np.cos(2 * np.pi * calendar[\"month\"] / 12.0)\n",
        "\n",
        "    feature_frame = pd.concat(\n",
        "        [df[[\"datetime\", target_col]], df[pollutant_cols + meteorology_cols], calendar],\n",
        "        axis=1,\n",
        "    )\n",
        "    mask_frame = mask.astype(np.float32)\n",
        "    mask_frame.columns = [f\"{col}_mask\" for col in mask_frame.columns]\n",
        "    feature_frame = pd.concat([feature_frame, mask_frame], axis=1)\n",
        "    return feature_frame, mask_frame\n",
        "\n",
        "\n",
        "def build_temporal_windows(frame, target_col, feature_cols, context, horizon, drop_na=True):\n",
        "    \"\"\"\n",
        "    Returns (windows, targets, indices) where:\n",
        "      - windows: list/array of shape (n_windows, context, n_features)\n",
        "      - targets: list/array of target values aligned at idx+horizon\n",
        "      - indices: original indices of the window end (optional)\n",
        "    This robustly handles scalar/array targets and missing values.\n",
        "    \"\"\"\n",
        "    values = frame[feature_cols].to_numpy(dtype=np.float32)\n",
        "    targets = frame[target_col].to_numpy(dtype=np.float32)\n",
        "    windows = []\n",
        "    target_list = []\n",
        "    idxs = []\n",
        "\n",
        "    n = len(values)\n",
        "    for idx in range(context, n - horizon):\n",
        "        window = values[idx - context : idx]\n",
        "        target = targets[idx + horizon]\n",
        "\n",
        "        if drop_na:\n",
        "            # Use pd.isna then np.any so this works if `target` is scalar or array-like\n",
        "            if np.any(pd.isna(window)) or np.any(pd.isna(target)):\n",
        "                continue\n",
        "\n",
        "        windows.append(window)\n",
        "        target_list.append(target)\n",
        "        idxs.append(idx)\n",
        "\n",
        "    X = np.stack(windows).astype(np.float32) if windows else np.empty((0, context, values.shape[1]), dtype=np.float32)\n",
        "    y = np.array(target_list, dtype=np.float32)\n",
        "    return X, y, np.array(idxs)\n",
        "\n",
        "\n",
        "def assemble_beijing_cross_station_bundle(\n",
        "    stations: Dict[str, pd.DataFrame],\n",
        "    train_stations: List[str],\n",
        "    val_station: str,\n",
        "    test_station: str,\n",
        "    target: str = \"PM2.5\",\n",
        "    context: int = 24,\n",
        "    horizon: int = 6,\n",
        ") -> DatasetBundle:\n",
        "    feature_frames: Dict[str, pd.DataFrame] = {}\n",
        "    feature_cols: Optional[List[str]] = None\n",
        "    for name, df in stations.items():\n",
        "        features, _ = preprocess_beijing_station(df, target_col=target)\n",
        "        feature_frames[name] = features\n",
        "        if feature_cols is None:\n",
        "            feature_cols = [col for col in features.columns if col not in (\"datetime\", target)]\n",
        "    assert feature_cols is not None\n",
        "\n",
        "    def collect(names: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        arrays = []\n",
        "        targets = []\n",
        "        for station_name in names:\n",
        "            frame = feature_frames[station_name]\n",
        "            X, y, _ = build_temporal_windows(frame, target, feature_cols, context, horizon)\n",
        "            arrays.append(X)\n",
        "            targets.append(y)\n",
        "        if arrays:\n",
        "            X_all = np.concatenate(arrays, axis=0).astype(np.float32)\n",
        "            y_all = np.concatenate(targets, axis=0)\n",
        "            if y_all.ndim == 1:\n",
        "                y_all = y_all[:, None]\n",
        "            else:\n",
        "                y_all = y_all.reshape(y_all.shape[0], -1)\n",
        "            y_all = y_all.astype(np.float32)\n",
        "        else:\n",
        "            X_all = np.empty((0, context, len(feature_cols)), dtype=np.float32)\n",
        "            y_all = np.empty((0, 1), dtype=np.float32)\n",
        "        return X_all, y_all\n",
        "\n",
        "    X_train, y_train = collect(train_stations)\n",
        "    X_val, y_val = collect([val_station])\n",
        "    X_test, y_test = collect([test_station])\n",
        "\n",
        "    bundle = DatasetBundle(\n",
        "        name=f\"Beijing_PM25_{context}h_ctx_{horizon}h_horizon\",\n",
        "        task_type=\"regression\",\n",
        "        input_kind=\"sequence\",\n",
        "        feature_names=feature_cols,\n",
        "        target_names=[target],\n",
        "        train={\"X\": X_train, \"y\": y_train},\n",
        "        val={\"X\": X_val, \"y\": y_val},\n",
        "        test={\"X\": X_test, \"y\": y_test},\n",
        "        metadata={\n",
        "            \"context_hours\": context,\n",
        "            \"horizon_hours\": horizon,\n",
        "            \"train_stations\": train_stations,\n",
        "            \"val_station\": val_station,\n",
        "            \"test_station\": test_station,\n",
        "        },\n",
        "    )\n",
        "    return bundle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "5367c9e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5367c9e2",
        "outputId": "45deb62c-64a3-406c-ed6e-3d9e6e0d0b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CUDA device name: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "SEED = int(os.getenv(\"PSANN_GLOBAL_SEED\", \"2025\"))\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "GLOBAL_CONFIG: Dict[str, Any] = {\n",
        "    \"seed\": SEED,\n",
        "    \"device\": DEVICE,\n",
        "    \"default_epochs\": 100,\n",
        "    \"default_lr\": 1e-3,\n",
        "    \"default_weight_decay\": 0.0,\n",
        "    \"default_batch_size\": 256,\n",
        "    \"max_time_minutes\": 5.0,\n",
        "    \"num_workers\": 2 if DEVICE.type == \"cuda\" else 0,\n",
        "    \"label_smoothing\": 0.05,\n",
        "    \"results_root\": RESULTS_ROOT,\n",
        "    \"figure_root\": FIGURE_ROOT,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "8025e56a",
      "metadata": {
        "id": "8025e56a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
        "\n",
        "\n",
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "\n",
        "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "\n",
        "def smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred) + 1e-8) / 2.0\n",
        "    return float(np.mean(np.abs(y_true - y_pred) / denom))\n",
        "\n",
        "\n",
        "def r2_score_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return float(1 - ss_res / ss_tot) if ss_tot != 0 else float('nan')\n",
        "\n",
        "\n",
        "def mase(y_true: np.ndarray, y_pred: np.ndarray, seasonal_period: int = 1) -> float:\n",
        "    if len(y_true) <= seasonal_period:\n",
        "        return float('nan')\n",
        "    naive = np.mean(np.abs(np.diff(y_true, n=seasonal_period)))\n",
        "    return float(np.mean(np.abs(y_true - y_pred)) / (naive + 1e-8))\n",
        "\n",
        "\n",
        "def expected_calibration_error(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15) -> float:\n",
        "    confidences = probs.max(axis=1)\n",
        "    predictions = probs.argmax(axis=1)\n",
        "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        mask = (confidences >= bin_edges[i]) & (confidences < bin_edges[i + 1])\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        bin_acc = np.mean(predictions[mask] == y_true[mask])\n",
        "        bin_conf = np.mean(confidences[mask])\n",
        "        ece += np.abs(bin_acc - bin_conf) * np.mean(mask)\n",
        "    return float(ece)\n",
        "\n",
        "\n",
        "def classification_metrics(y_true: np.ndarray, logits: np.ndarray, average: str = 'macro') -> Dict[str, float]:\n",
        "    probs = torch.softmax(torch.from_numpy(logits), dim=-1).numpy()\n",
        "    preds = probs.argmax(axis=1)\n",
        "    metrics = {\n",
        "        'accuracy': float(accuracy_score(y_true, preds)),\n",
        "        'f1_macro': float(f1_score(y_true, preds, average=average)),\n",
        "        'nll': float(log_loss(y_true, probs, labels=list(range(probs.shape[1])))),\n",
        "    }\n",
        "    metrics['ece'] = expected_calibration_error(probs, y_true, n_bins=15)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def _prepare_regression_arrays(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    if y_true.ndim > 2:\n",
        "        y_true = y_true.reshape(y_true.shape[0], -1)\n",
        "    if y_pred.ndim > 2:\n",
        "        y_pred = y_pred.reshape(y_pred.shape[0], -1)\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true[:, None]\n",
        "    if y_pred.ndim == 1:\n",
        "        y_pred = y_pred[:, None]\n",
        "    if y_true.shape != y_pred.shape:\n",
        "        raise ValueError(f'Regression metric shape mismatch: {y_true.shape} vs {y_pred.shape}')\n",
        "    return y_true.astype(np.float64), y_pred.astype(np.float64)\n",
        "\n",
        "\n",
        "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray, seasonal_period: int = 1) -> Dict[str, float]:\n",
        "    y_true, y_pred = _prepare_regression_arrays(y_true, y_pred)\n",
        "    rmse_vals = []\n",
        "    mae_vals = []\n",
        "    smape_vals = []\n",
        "    r2_vals = []\n",
        "    mase_vals = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        yt = y_true[:, i]\n",
        "        yp = y_pred[:, i]\n",
        "        rmse_vals.append(rmse(yt, yp))\n",
        "        mae_vals.append(mae(yt, yp))\n",
        "        smape_vals.append(smape(yt, yp))\n",
        "        r2_vals.append(r2_score_np(yt, yp))\n",
        "        mase_vals.append(mase(yt, yp, seasonal_period=seasonal_period))\n",
        "    return {\n",
        "        'rmse': float(np.mean(rmse_vals)),\n",
        "        'mae': float(np.mean(mae_vals)),\n",
        "        'smape': float(np.mean(smape_vals)),\n",
        "        'r2': float(np.mean(r2_vals)),\n",
        "        'mase': float(np.mean(mase_vals)),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "d7ff9d3d",
      "metadata": {
        "id": "d7ff9d3d"
      },
      "outputs": [],
      "source": [
        "def build_dataloader(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    batch_size: int,\n",
        "    shuffle: bool,\n",
        "    task_type: Literal[\"regression\", \"classification\", \"multitask\"] = \"regression\",\n",
        "    drop_last: bool = False,\n",
        ") -> DataLoader:\n",
        "    X_tensor = torch.from_numpy(X).float()\n",
        "    if task_type == \"classification\":\n",
        "        y_tensor = torch.from_numpy(y.squeeze()).long()\n",
        "    else:\n",
        "        y_tensor = torch.from_numpy(y.astype(np.float32))\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=GLOBAL_CONFIG[\"num_workers\"],\n",
        "        pin_memory=(DEVICE.type == \"cuda\"),\n",
        "    )\n",
        "    return loader\n",
        "\n",
        "\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start = time.perf_counter()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.end = time.perf_counter()\n",
        "\n",
        "    @property\n",
        "    def elapsed(self) -> float:\n",
        "        return getattr(self, \"end\", time.perf_counter()) - getattr(self, \"start\", time.perf_counter())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "c32b3093",
      "metadata": {
        "id": "c32b3093"
      },
      "outputs": [],
      "source": [
        "def coerce_decimal(series: pd.Series) -> pd.Series:\n",
        "    if pd.api.types.is_numeric_dtype(series):\n",
        "        return series\n",
        "    as_str = series.astype(str).str.replace(\" \", \"\")\n",
        "    as_str = as_str.replace({\"nan\": np.nan, \"None\": np.nan})\n",
        "    as_str = as_str.str.replace(\",\", \".\", regex=False)\n",
        "    return pd.to_numeric(as_str, errors=\"coerce\")\n",
        "\n",
        "\n",
        "def coerce_datetime(series: pd.Series) -> pd.Series:\n",
        "    as_str = series.astype(str).str.strip()\n",
        "    as_str = as_str.replace({\"nan\": np.nan, \"NaT\": np.nan})\n",
        "    as_str = as_str.str.replace(\",\", \".\", n=1, regex=False)\n",
        "    return pd.to_datetime(as_str, errors=\"coerce\")\n",
        "\n",
        "\n",
        "def ensure_float(df: pd.DataFrame, columns: Iterable[str]) -> pd.DataFrame:\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = coerce_decimal(df[col])\n",
        "    return df\n",
        "\n",
        "\n",
        "def ensure_datetime(df: pd.DataFrame, columns: Iterable[str]) -> pd.DataFrame:\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = coerce_datetime(df[col])\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_calendar_features(frame: pd.DataFrame, timestamp_col: str) -> pd.DataFrame:\n",
        "    ts = pd.to_datetime(frame[timestamp_col])\n",
        "    frame[f\"{timestamp_col}_year\"] = ts.dt.year\n",
        "    frame[f\"{timestamp_col}_month\"] = ts.dt.month\n",
        "    frame[f\"{timestamp_col}_day\"] = ts.dt.day\n",
        "    frame[f\"{timestamp_col}_hour\"] = ts.dt.hour\n",
        "    frame[f\"{timestamp_col}_dow\"] = ts.dt.dayofweek\n",
        "    frame[f\"{timestamp_col}_week\"] = ts.dt.isocalendar().week.astype(int)\n",
        "    frame[f\"{timestamp_col}_dayofyear\"] = ts.dt.dayofyear\n",
        "    frame[f\"{timestamp_col}_sin_hour\"] = np.sin(2 * np.pi * frame[f\"{timestamp_col}_hour\"] / 24.0)\n",
        "    frame[f\"{timestamp_col}_cos_hour\"] = np.cos(2 * np.pi * frame[f\"{timestamp_col}_hour\"] / 24.0)\n",
        "    frame[f\"{timestamp_col}_sin_dayofyear\"] = np.sin(2 * np.pi * frame[f\"{timestamp_col}_dayofyear\"] / 365.25)\n",
        "    frame[f\"{timestamp_col}_cos_dayofyear\"] = np.cos(2 * np.pi * frame[f\"{timestamp_col}_dayofyear\"] / 365.25)\n",
        "    return frame\n",
        "\n",
        "\n",
        "def train_val_test_split_by_time(df: pd.DataFrame, time_col: str, train_end: str, val_end: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    ts = pd.to_datetime(df[time_col])\n",
        "    train_mask = ts < pd.to_datetime(train_end)\n",
        "    val_mask = (ts >= pd.to_datetime(train_end)) & (ts < pd.to_datetime(val_end))\n",
        "    test_mask = ts >= pd.to_datetime(val_end)\n",
        "    return df[train_mask].copy(), df[val_mask].copy(), df[test_mask].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "3e7bd4b5",
      "metadata": {
        "id": "3e7bd4b5"
      },
      "outputs": [],
      "source": [
        "EAF_TABLES = [\n",
        "    \"eaf_temp\",\n",
        "    \"eaf_gaslance_mat\",\n",
        "    \"inj_mat\",\n",
        "    \"eaf_transformer\",\n",
        "    \"eaf_added_materials\",\n",
        "    \"basket_charged\",\n",
        "    \"lf_added_materials\",\n",
        "    \"lf_initial_chemical_measurements\",\n",
        "    \"eaf_final_chemical_measurements\",\n",
        "    \"ladle_tapping\",\n",
        "]\n",
        "\n",
        "\n",
        "def parse_duration_minutes(value: Any) -> Optional[float]:\n",
        "    if pd.isna(value):\n",
        "        return np.nan\n",
        "    s = str(value).strip()\n",
        "    if not s:\n",
        "        return np.nan\n",
        "    s = s.replace(\" \", \"\")\n",
        "    if \":\" not in s:\n",
        "        return coerce_decimal(pd.Series([s])).iloc[0]\n",
        "    parts = s.split(\":\")\n",
        "    try:\n",
        "        hours = float(parts[0])\n",
        "        minutes = float(parts[1])\n",
        "        return hours * 60.0 + minutes\n",
        "    except Exception:\n",
        "        return np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "7a55a25b",
      "metadata": {
        "id": "7a55a25b"
      },
      "outputs": [],
      "source": [
        "def load_eaf_tables(data_root: Path) -> Dict[str, pd.DataFrame]:\n",
        "    candidate_dirs = [\n",
        "        data_root / \"Industrial Data from the Electric Arc Furnace\",\n",
        "        data_root / \"Industrial_Data_from_the_Electric_Arc_Furnace\",\n",
        "    ]\n",
        "    base = next((path for path in candidate_dirs if path.exists()), None)\n",
        "\n",
        "    table_paths: Dict[str, Path] = {}\n",
        "    if base is not None:\n",
        "        table_paths = {name: base / f\"{name}.csv\" for name in EAF_TABLES}\n",
        "    else:\n",
        "        print(f\"[WARN] Expected EAF directory missing under {data_root}. Falling back to glob search.\")\n",
        "        for name in EAF_TABLES:\n",
        "            path = next(\n",
        "                (\n",
        "                    candidate\n",
        "                    for candidate in [\n",
        "                        data_root / f\"{name}.csv\",\n",
        "                        data_root / f\"{name.upper()}.csv\",\n",
        "                    ]\n",
        "                    if candidate.exists()\n",
        "                ),\n",
        "                None,\n",
        "            )\n",
        "            if path is None:\n",
        "                matches = list(data_root.rglob(f\"{name}.csv\"))\n",
        "                if matches:\n",
        "                    path = matches[0]\n",
        "            table_paths[name] = path\n",
        "\n",
        "    missing = [name for name, path in table_paths.items() if path is None or not path.exists()]\n",
        "    if missing:\n",
        "        raise FileNotFoundError(\n",
        "            \"Unable to locate EAF tables: \" + \", \".join(missing) + f\". Ensure the CSV files are present under {data_root}.\"\n",
        "        )\n",
        "\n",
        "    tables: Dict[str, pd.DataFrame] = {}\n",
        "    for name, path in table_paths.items():\n",
        "        if path is None:\n",
        "            continue\n",
        "        print(f\"Loading {name} from {path}...\")\n",
        "        if name in {\"eaf_gaslance_mat\", \"inj_mat\"}:\n",
        "            df = pd.read_csv(path, dtype=str)\n",
        "            df = ensure_datetime(df, [\"REVTIME\"])\n",
        "            if \"DATETIME\" not in df.columns and \"REVTIME\" in df.columns:\n",
        "                df[\"DATETIME\"] = df[\"REVTIME\"]\n",
        "            numeric_cols = [c for c in df.columns if c not in (\"REVTIME\", \"HEATID\", \"DATETIME\")]\n",
        "            df = ensure_float(df, numeric_cols)\n",
        "        elif name == \"eaf_temp\":\n",
        "            df = pd.read_csv(path)\n",
        "            df = ensure_datetime(df, [\"DATETIME\"])\n",
        "            numeric_cols = [c for c in df.columns if c not in (\"HEATID\", \"DATETIME\")]\n",
        "            df = ensure_float(df, numeric_cols)\n",
        "        elif name == \"eaf_transformer\":\n",
        "            df = pd.read_csv(path, dtype=str)\n",
        "            df = ensure_datetime(df, [\"STARTTIME\"])\n",
        "            if \"DATETIME\" not in df.columns and \"STARTTIME\" in df.columns:\n",
        "                df[\"DATETIME\"] = df[\"STARTTIME\"]\n",
        "            df[\"DURATION_MIN\"] = df[\"DURATION\"].astype(str).str.replace(\" \", \"\")\n",
        "            df[\"DURATION_MIN\"] = df[\"DURATION_MIN\"].apply(parse_duration_minutes)\n",
        "            df = ensure_float(df, [\"DURATION_MIN\", \"MW\"])\n",
        "        else:\n",
        "            df = pd.read_csv(path, dtype=str)\n",
        "            datetime_cols = [c for c in df.columns if \"DATE\" in c.upper() or \"TIME\" in c.upper()]\n",
        "            if datetime_cols:\n",
        "                df = ensure_datetime(df, datetime_cols)\n",
        "                if \"DATETIME\" not in df.columns:\n",
        "                    df[\"DATETIME\"] = df[datetime_cols[0]]\n",
        "            numeric_cols = [\n",
        "                c\n",
        "                for c in df.columns\n",
        "                if c not in datetime_cols and c not in (\"HEATID\", \"RECID\", \"POSITIONROW\", \"DATETIME\")\n",
        "            ]\n",
        "            df = ensure_float(df, numeric_cols)\n",
        "        tables[name] = df\n",
        "    return tables\n",
        "\n",
        "\n",
        "def compute_heatwise_aggregates(df: pd.DataFrame, heat_col: str, aggregations: Dict[str, List[str]]) -> pd.DataFrame:\n",
        "    grouped = df.groupby(heat_col).agg(aggregations)\n",
        "    grouped.columns = [f\"{col}_{agg}\" for col, agg in grouped.columns]\n",
        "    grouped = grouped.reset_index()\n",
        "    return grouped\n",
        "\n",
        "# Fixed vectorized merge_asof_multikey (searchsorted-based, dtype-safe assignments)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "\n",
        "def merge_asof_multikey(\n",
        "    left: pd.DataFrame,\n",
        "    right: pd.DataFrame,\n",
        "    *,\n",
        "    on: str,\n",
        "    by: str,\n",
        "    suffix: str = \"rhs\",\n",
        "    tolerance: Optional[pd.Timedelta] = None,\n",
        "    direction: str = \"backward\",\n",
        "    verbose: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Vectorized as-of merge on (by, on) using numpy.searchsorted on structured keys.\n",
        "    Supports direction='backward' (most common for telemetry alignment).\n",
        "    Returns left with right's non-key columns suffixed by _{suffix}.\n",
        "    \"\"\"\n",
        "    if right is None or len(right) == 0:\n",
        "        return left.copy()\n",
        "    if direction != \"backward\":\n",
        "        raise NotImplementedError(\"Only 'backward' direction supported in this implementation\")\n",
        "\n",
        "    # Basic checks\n",
        "    if on not in left.columns or on not in right.columns or by not in left.columns or by not in right.columns:\n",
        "        raise KeyError(f\"Both frames must contain columns '{by}' and '{on}'\")\n",
        "\n",
        "    # Work on copies\n",
        "    L = left.copy()\n",
        "    R = right.copy()\n",
        "\n",
        "    # Mask and filter rows missing keys (we'll reattach them at the end)\n",
        "    mask_L_valid = L[on].notna() & L[by].notna()\n",
        "    mask_R_valid = R[on].notna() & R[by].notna()\n",
        "    L_valid = L.loc[mask_L_valid].copy()\n",
        "    R_valid = R.loc[mask_R_valid].copy()\n",
        "\n",
        "    # Coerce datetimes\n",
        "    L_valid[on] = pd.to_datetime(L_valid[on], errors=\"coerce\")\n",
        "    R_valid[on] = pd.to_datetime(R_valid[on], errors=\"coerce\")\n",
        "    L_valid = L_valid[L_valid[on].notna()]\n",
        "    R_valid = R_valid[R_valid[on].notna()]\n",
        "\n",
        "    # If no valid rows remain on left, return original left with NaNs for merge cols\n",
        "    if L_valid.empty:\n",
        "        merged = L.copy()\n",
        "        merge_cols = [c for c in R.columns if c not in (by, on)]\n",
        "        for c in merge_cols:\n",
        "            merged[f\"{c}_{suffix}\"] = np.nan\n",
        "        return merged\n",
        "\n",
        "    # Dedupe right on (by, on) keeping last measurement (reduces search space)\n",
        "    R_valid = R_valid.sort_values([by, on], kind=\"mergesort\").drop_duplicates(subset=[by, on], keep=\"last\")\n",
        "\n",
        "    # Factorize right groups to compact integer ids\n",
        "    right_labels = pd.unique(R_valid[by].astype(object))  # preserve order\n",
        "    group_to_id = {val: i for i, val in enumerate(right_labels)}\n",
        "    # Map right group ids\n",
        "    right_group_ids = np.array([group_to_id[v] for v in R_valid[by].astype(object)], dtype=np.int32)\n",
        "\n",
        "    # Map left group ids; groups not in right get -1\n",
        "    left_group_values = L_valid[by].astype(object).values\n",
        "    left_group_ids = np.array([group_to_id.get(v, -1) for v in left_group_values], dtype=np.int32)\n",
        "\n",
        "    # Convert times to int64 ns\n",
        "    left_times_ns = L_valid[on].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
        "    right_times_ns = R_valid[on].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
        "\n",
        "    # Build structured keys and sort by (group,time)\n",
        "    key_dtype = np.dtype([(\"g\", np.int32), (\"t\", np.int64)])\n",
        "    right_keys = np.empty(len(right_group_ids), dtype=key_dtype)\n",
        "    right_keys[\"g\"] = right_group_ids\n",
        "    right_keys[\"t\"] = right_times_ns\n",
        "    order = np.argsort(right_keys, order=(\"g\", \"t\"))\n",
        "    right_keys_sorted = right_keys[order]\n",
        "    R_sorted = R_valid.reset_index(drop=True).iloc[order].reset_index(drop=True)\n",
        "    right_times_sorted = right_keys_sorted[\"t\"]\n",
        "\n",
        "    # Left keys structured array (same dtype)\n",
        "    left_keys = np.empty(len(left_group_ids), dtype=key_dtype)\n",
        "    left_keys[\"g\"] = left_group_ids\n",
        "    left_keys[\"t\"] = left_times_ns\n",
        "\n",
        "    # Searchsorted to find previous (backward) right index for each left key\n",
        "    idxs = np.searchsorted(right_keys_sorted, left_keys, side=\"right\") - 1\n",
        "\n",
        "    # Initialize keep_mask (default False)\n",
        "    keep_mask = np.zeros(len(idxs), dtype=bool)\n",
        "\n",
        "    # valid where idxs >= 0\n",
        "    valid_mask = idxs >= 0\n",
        "    if valid_mask.any():\n",
        "        # Check matched group's id equals left group id (otherwise it's from a different group)\n",
        "        matched_group_ids = right_keys_sorted[\"g\"][idxs[valid_mask]]\n",
        "        left_group_ids_valid = left_keys[\"g\"][valid_mask]\n",
        "        same_group = matched_group_ids == left_group_ids_valid\n",
        "        # Set keep_mask True only where same_group is True\n",
        "        keep_mask[np.flatnonzero(valid_mask)[same_group]] = True\n",
        "\n",
        "    # Apply tolerance if provided (left_time - matched_right_time must be <= tol and >=0)\n",
        "    if tolerance is not None and keep_mask.any():\n",
        "        tol_ns = int(pd.to_timedelta(tolerance).to_timedelta64().astype(\"timedelta64[ns]\") / np.timedelta64(1, \"ns\"))\n",
        "        kept_positions = np.flatnonzero(keep_mask)\n",
        "        matched_right_times = right_times_sorted[idxs[kept_positions]]\n",
        "        left_times_for_kept = left_keys[\"t\"][kept_positions]\n",
        "        diffs = left_times_for_kept - matched_right_times\n",
        "        tol_ok = (diffs >= 0) & (diffs <= tol_ns)\n",
        "        # Zero out positions violating tolerance\n",
        "        if not np.all(tol_ok):\n",
        "            keep_mask[kept_positions[~tol_ok]] = False\n",
        "\n",
        "    # Prepare result skeleton using proper dtypes (avoid assigning arrays of incompatible dtype)\n",
        "    merge_cols = [c for c in R_sorted.columns if c not in (by, on)]\n",
        "    result = L_valid.copy()\n",
        "    for c in merge_cols:\n",
        "        src_dtype = R_sorted[c].dtype\n",
        "        try:\n",
        "            result[f\"{c}_{suffix}\"] = pd.Series(index=result.index, dtype=src_dtype)\n",
        "        except Exception:\n",
        "            result[f\"{c}_{suffix}\"] = pd.Series(index=result.index, dtype=\"object\")\n",
        "\n",
        "    # Fill merged columns for kept matches\n",
        "    kept_positions = np.flatnonzero(keep_mask)\n",
        "    if kept_positions.size:\n",
        "        matched_idxs = idxs[kept_positions]  # indices into R_sorted\n",
        "        for col in merge_cols:\n",
        "            vals = R_sorted.iloc[matched_idxs][col].values\n",
        "            s = pd.Series(vals, index=result.index[kept_positions])\n",
        "            # If target dtype is datetime, ensure series is datetime\n",
        "            if np.issubdtype(result[f\"{col}_{suffix}\"].dtype, np.datetime64):\n",
        "                s = pd.to_datetime(s)\n",
        "            result.loc[result.index[kept_positions], f\"{col}_{suffix}\"] = s\n",
        "\n",
        "    # Rows that were invalid (no match) remain NaN in merged cols\n",
        "\n",
        "    # Reattach left rows that were dropped due to missing keys\n",
        "    if mask_L_valid.sum() != len(L):\n",
        "        dropped = L.loc[~mask_L_valid].copy()\n",
        "        for c in merge_cols:\n",
        "            dropped[f\"{c}_{suffix}\"] = np.nan\n",
        "        combined = pd.concat([result, dropped]).loc[L.index]\n",
        "    else:\n",
        "        combined = result\n",
        "\n",
        "    # Reindex to original left.index to preserve order\n",
        "    combined = combined.reindex(left.index)\n",
        "    return combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "12cfea05",
      "metadata": {
        "id": "12cfea05"
      },
      "outputs": [],
      "source": [
        "def prepare_eaf_temp_and_o2_bundles(\n",
        "    tables: Dict[str, pd.DataFrame],\n",
        "    history_lags: List[int] = (1, 2, 3, 6),\n",
        "    horizon: int = 1,\n",
        ") -> Tuple[DatasetBundle, DatasetBundle]:\n",
        "    temp = tables[\"eaf_temp\"].copy()\n",
        "    temp[\"DATETIME\"] = pd.to_datetime(temp[\"DATETIME\"])\n",
        "    temp = temp.sort_values([\"HEATID\", \"DATETIME\"]).reset_index(drop=True)\n",
        "    temp = temp.drop_duplicates(subset=[\"HEATID\", \"DATETIME\"], keep=\"last\")\n",
        "\n",
        "    for lag in history_lags:\n",
        "        temp[f\"TEMP_lag_{lag}\"] = temp.groupby(\"HEATID\")[\"TEMP\"].shift(lag)\n",
        "        temp[f\"VALO2_lag_{lag}\"] = temp.groupby(\"HEATID\")[\"VALO2_PPM\"].shift(lag)\n",
        "\n",
        "    temp[\"TEMP_target\"] = temp.groupby(\"HEATID\")[\"TEMP\"].shift(-horizon)\n",
        "    temp[\"VALO2_target\"] = temp.groupby(\"HEATID\")[\"VALO2_PPM\"].shift(-horizon)\n",
        "\n",
        "    temp[\"HEAT_START\"] = temp.groupby(\"HEATID\")[\"DATETIME\"].transform(\"min\")\n",
        "    temp[\"minutes_from_heat_start\"] = (temp[\"DATETIME\"] - temp[\"HEAT_START\"]).dt.total_seconds() / 60.0\n",
        "    temp[\"sample_index\"] = temp.groupby(\"HEATID\").cumcount()\n",
        "    temp[\"minutes_between_samples\"] = temp.groupby(\"HEATID\")[\"DATETIME\"].diff().dt.total_seconds().fillna(0.0) / 60.0\n",
        "\n",
        "    gas = tables.get(\"eaf_gaslance_mat\")\n",
        "    if gas is not None and not gas.empty:\n",
        "        gas = gas.sort_values([\"HEATID\", \"REVTIME\"])\n",
        "        for col in [\"O2_AMOUNT\", \"GAS_AMOUNT\", \"O2_FLOW\", \"GAS_FLOW\"]:\n",
        "            if col in gas.columns:\n",
        "                gas[f\"{col}_cum\"] = gas.groupby(\"HEATID\")[col].cumsum()\n",
        "        temp = merge_asof_multikey(\n",
        "            temp,\n",
        "            gas,\n",
        "            on=\"DATETIME\",\n",
        "            by=\"HEATID\",\n",
        "            suffix=\"gas\",\n",
        "            tolerance=pd.Timedelta(minutes=30),\n",
        "        )\n",
        "\n",
        "    inj = tables.get(\"inj_mat\")\n",
        "    if inj is not None and not inj.empty:\n",
        "        inj = inj.sort_values([\"HEATID\", \"REVTIME\"])\n",
        "        for col in [\"INJ_AMOUNT_CARBON\", \"INJ_FLOW_CARBON\"]:\n",
        "            if col in inj.columns:\n",
        "                inj[f\"{col}_cum\"] = inj.groupby(\"HEATID\")[col].cumsum()\n",
        "        temp = merge_asof_multikey(\n",
        "            temp,\n",
        "            inj,\n",
        "            on=\"DATETIME\",\n",
        "            by=\"HEATID\",\n",
        "            suffix=\"inj\",\n",
        "            tolerance=pd.Timedelta(minutes=30),\n",
        "        )\n",
        "\n",
        "    transformer = tables.get(\"eaf_transformer\")\n",
        "    if transformer is not None and not transformer.empty:\n",
        "        transformer = transformer.sort_values([\"HEATID\", \"STARTTIME\"])\n",
        "        temp = merge_asof_multikey(\n",
        "            temp,\n",
        "            transformer,\n",
        "            on=\"DATETIME\",\n",
        "            by=\"HEATID\",\n",
        "            suffix=\"xfmr\",\n",
        "            tolerance=pd.Timedelta(hours=2),\n",
        "        )\n",
        "\n",
        "    temp = add_calendar_features(temp, \"DATETIME\")\n",
        "    feature_cols = [\n",
        "        col\n",
        "        for col in temp.columns\n",
        "        if col\n",
        "        not in {\n",
        "            \"TEMP\",\n",
        "            \"VALO2_PPM\",\n",
        "            \"TEMP_target\",\n",
        "            \"VALO2_target\",\n",
        "            \"HEATID\",\n",
        "            \"HEAT_START\",\n",
        "            \"DATETIME\",\n",
        "        }\n",
        "        and not col.endswith(\"_xfmr\")\n",
        "    ]\n",
        "    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(temp[c])]\n",
        "\n",
        "    temp[feature_cols] = temp[feature_cols].astype(np.float32)\n",
        "\n",
        "    temp = temp.dropna(subset=feature_cols + [\"TEMP_target\", \"VALO2_target\"]).reset_index(drop=True)\n",
        "\n",
        "    temp[\"year\"] = temp[\"DATETIME\"].dt.year\n",
        "    heat_year = temp.groupby(\"HEATID\")[\"year\"].max().reset_index().rename(columns={\"year\": \"heat_year\"})\n",
        "    temp = temp.merge(heat_year, on=\"HEATID\", how=\"left\")\n",
        "\n",
        "    train_mask = temp[\"heat_year\"] <= 2016\n",
        "    val_mask = temp[\"heat_year\"] == 2017\n",
        "    test_mask = temp[\"heat_year\"] >= 2018\n",
        "\n",
        "\n",
        "    feature_mean = temp.loc[train_mask, feature_cols].mean()\n",
        "    feature_std = temp.loc[train_mask, feature_cols].std().replace(0.0, 1.0)\n",
        "    temp[feature_cols] = (temp[feature_cols] - feature_mean) / feature_std\n",
        "\n",
        "    scaler_meta = {\n",
        "        \"feature_mean\": {k: float(v) for k, v in feature_mean.items()},\n",
        "        \"feature_std\": {k: float(v) for k, v in feature_std.items()},\n",
        "    }\n",
        "\n",
        "    def build_split(mask: pd.Series) -> Dict[str, np.ndarray]:\n",
        "        X = temp.loc[mask, feature_cols].to_numpy(dtype=np.float32)\n",
        "        y_temp = temp.loc[mask, \"TEMP_target\"].to_numpy(dtype=np.float32)[:, None]\n",
        "        y_o2 = temp.loc[mask, \"VALO2_target\"].to_numpy(dtype=np.float32)[:, None]\n",
        "        return {\"X\": X, \"y_temp\": y_temp, \"y_o2\": y_o2}\n",
        "\n",
        "    train_split = build_split(train_mask)\n",
        "    val_split = build_split(val_mask)\n",
        "    test_split = build_split(test_mask)\n",
        "\n",
        "    temp_target_mean = train_split[\"y_temp\"].mean(axis=0, keepdims=True)\n",
        "    temp_target_std = train_split[\"y_temp\"].std(axis=0, keepdims=True)\n",
        "    temp_target_std = np.where(temp_target_std < 1e-6, 1.0, temp_target_std)\n",
        "\n",
        "    o2_target_mean = train_split[\"y_o2\"].mean(axis=0, keepdims=True)\n",
        "    o2_target_std = train_split[\"y_o2\"].std(axis=0, keepdims=True)\n",
        "    o2_target_std = np.where(o2_target_std < 1e-6, 1.0, o2_target_std)\n",
        "\n",
        "    def _normalize_target(split: Dict[str, np.ndarray], key: str, mean: np.ndarray, std: np.ndarray) -> None:\n",
        "        split[key] = ((split[key] - mean) / std).astype(np.float32)\n",
        "\n",
        "    for split_dict in (train_split, val_split, test_split):\n",
        "        _normalize_target(split_dict, \"y_temp\", temp_target_mean, temp_target_std)\n",
        "        _normalize_target(split_dict, \"y_o2\", o2_target_mean, o2_target_std)\n",
        "\n",
        "    temp_bundle = DatasetBundle(\n",
        "        name=\"EAF_TEMP_forecast\",\n",
        "        task_type=\"regression\",\n",
        "        input_kind=\"tabular\",\n",
        "        feature_names=feature_cols,\n",
        "        target_names=[\"TEMP_target\"],\n",
        "        train={\"X\": train_split[\"X\"], \"y\": train_split[\"y_temp\"]},\n",
        "        val={\"X\": val_split[\"X\"], \"y\": val_split[\"y_temp\"]},\n",
        "        test={\"X\": test_split[\"X\"], \"y\": test_split[\"y_temp\"]},\n",
        "        metadata={\n",
        "            \"horizon_steps\": horizon,\n",
        "            \"history_lags\": list(history_lags),\n",
        "            \"feature_source\": \"temp + gas + injection + calendar\",\n",
        "            **scaler_meta,\n",
        "            \"target_scaler\": {\n",
        "                \"mean\": float(temp_target_mean.squeeze()),\n",
        "                \"std\": float(temp_target_std.squeeze()),\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "\n",
        "    o2_bundle = DatasetBundle(\n",
        "        name=\"EAF_VALO2_forecast\",\n",
        "        task_type=\"regression\",\n",
        "        input_kind=\"tabular\",\n",
        "        feature_names=feature_cols,\n",
        "        target_names=[\"VALO2_target\"],\n",
        "        train={\"X\": train_split[\"X\"], \"y\": train_split[\"y_o2\"]},\n",
        "        val={\"X\": val_split[\"X\"], \"y\": val_split[\"y_o2\"]},\n",
        "        test={\"X\": test_split[\"X\"], \"y\": test_split[\"y_o2\"]},\n",
        "        metadata={\n",
        "            \"horizon_steps\": horizon,\n",
        "            \"history_lags\": list(history_lags),\n",
        "            \"feature_source\": \"temp + gas + injection + calendar\",\n",
        "            **scaler_meta,\n",
        "            \"target_scaler\": {\n",
        "                \"mean\": float(o2_target_mean.squeeze()),\n",
        "                \"std\": float(o2_target_std.squeeze()),\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "\n",
        "    return temp_bundle, o2_bundle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "9a10e15f",
      "metadata": {
        "id": "9a10e15f"
      },
      "outputs": [],
      "source": [
        "def prepare_eaf_chemistry_bundle(tables: Dict[str, pd.DataFrame]) -> DatasetBundle:\n",
        "    chem = tables[\"eaf_final_chemical_measurements\"].copy()\n",
        "    chem = ensure_datetime(chem, [\"DATETIME\"])\n",
        "    chem = chem.sort_values([\"HEATID\", \"DATETIME\"])\n",
        "    chem = chem.drop_duplicates(subset=[\"HEATID\"], keep=\"last\")\n",
        "\n",
        "    target_cols = [c for c in chem.columns if c not in (\"HEATID\", \"POSITIONROW\", \"DATETIME\")]\n",
        "    chem = ensure_float(chem, target_cols)\n",
        "\n",
        "    temp = tables[\"eaf_temp\"].copy()\n",
        "    temp = ensure_datetime(temp, [\"DATETIME\"])\n",
        "    temp = temp.sort_values([\"HEATID\", \"DATETIME\"])\n",
        "    temp[\"sample_index\"] = temp.groupby(\"HEATID\").cumcount()\n",
        "    temp = add_calendar_features(temp, \"DATETIME\")\n",
        "    temp_aggs = compute_heatwise_aggregates(\n",
        "        temp,\n",
        "        \"HEATID\",\n",
        "        {\n",
        "            \"TEMP\": [\"mean\", \"max\", \"min\", \"last\"],\n",
        "            \"VALO2_PPM\": [\"mean\", \"max\", \"last\"],\n",
        "            \"DATETIME_month\": [\"last\"],\n",
        "            \"DATETIME_hour\": [\"mean\"],\n",
        "            \"sample_index\": [\"max\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def safe_aggregates(frame: Optional[pd.DataFrame], aggregations: Dict[str, List[str]]) -> pd.DataFrame:\n",
        "        if frame is None or frame.empty:\n",
        "            columns = [\"HEATID\"] + [f\"{feature}_{agg}\" for feature, aggs in aggregations.items() for agg in aggs]\n",
        "            return pd.DataFrame(columns=columns)\n",
        "        frame = frame.copy()\n",
        "        datetime_cols = [c for c in frame.columns if \"TIME\" in c.upper() or \"DATE\" in c.upper()]\n",
        "        if datetime_cols:\n",
        "            frame = ensure_datetime(frame, datetime_cols)\n",
        "        numeric_cols = [c for c in frame.columns if c not in (\"HEATID\", \"REVTIME\", \"STARTTIME\")]\n",
        "        frame = ensure_float(frame, numeric_cols)\n",
        "        return compute_heatwise_aggregates(frame, \"HEATID\", aggregations)\n",
        "\n",
        "    gas_aggs = safe_aggregates(\n",
        "        tables.get(\"eaf_gaslance_mat\"),\n",
        "        {\n",
        "            \"O2_AMOUNT\": [\"max\"],\n",
        "            \"GAS_AMOUNT\": [\"max\"],\n",
        "            \"O2_FLOW\": [\"mean\", \"max\"],\n",
        "            \"GAS_FLOW\": [\"mean\", \"max\"],\n",
        "        },\n",
        "    )\n",
        "    inj_aggs = safe_aggregates(\n",
        "        tables.get(\"inj_mat\"),\n",
        "        {\n",
        "            \"INJ_AMOUNT_CARBON\": [\"max\"],\n",
        "            \"INJ_FLOW_CARBON\": [\"mean\", \"max\"],\n",
        "        },\n",
        "    )\n",
        "    transformer_aggs = safe_aggregates(\n",
        "        tables.get(\"eaf_transformer\"),\n",
        "        {\n",
        "            \"MW\": [\"mean\", \"max\"],\n",
        "            \"DURATION_MIN\": [\"sum\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "    features = chem[[\"HEATID\", \"DATETIME\"]].merge(temp_aggs, on=\"HEATID\", how=\"left\")\n",
        "    features = features.merge(gas_aggs, on=\"HEATID\", how=\"left\")\n",
        "    features = features.merge(inj_aggs, on=\"HEATID\", how=\"left\")\n",
        "    features = features.merge(transformer_aggs, on=\"HEATID\", how=\"left\")\n",
        "\n",
        "    numeric_feature_cols = [c for c in features.columns if c not in (\"HEATID\", \"DATETIME\")]\n",
        "    features = ensure_float(features, numeric_feature_cols)\n",
        "    features = add_calendar_features(features, \"DATETIME\")\n",
        "    feature_cols = [c for c in features.columns if c not in (\"HEATID\", \"DATETIME\")]\n",
        "\n",
        "    merged = features.merge(chem[[\"HEATID\"] + target_cols], on=\"HEATID\", how=\"inner\")\n",
        "    merged = merged.dropna(subset=feature_cols + target_cols).reset_index(drop=True)\n",
        "\n",
        "    merged[\"year\"] = pd.to_datetime(merged[\"DATETIME\"]).dt.year\n",
        "    train_mask = merged[\"year\"] <= 2016\n",
        "    val_mask = merged[\"year\"] == 2017\n",
        "    test_mask = merged[\"year\"] >= 2018\n",
        "\n",
        "    X_train = merged.loc[train_mask, feature_cols].to_numpy(dtype=np.float32)\n",
        "    y_train = merged.loc[train_mask, target_cols].to_numpy(dtype=np.float32)\n",
        "    X_val = merged.loc[val_mask, feature_cols].to_numpy(dtype=np.float32)\n",
        "    y_val = merged.loc[val_mask, target_cols].to_numpy(dtype=np.float32)\n",
        "    X_test = merged.loc[test_mask, feature_cols].to_numpy(dtype=np.float32)\n",
        "    y_test = merged.loc[test_mask, target_cols].to_numpy(dtype=np.float32)\n",
        "\n",
        "    if X_train.size == 0 or X_val.size == 0 or X_test.size == 0:\n",
        "        raise ValueError(\"EAF chemistry splits produced empty partitions; check year filters.\")\n",
        "\n",
        "    feature_mean = X_train.mean(axis=0, keepdims=True)\n",
        "    feature_std = X_train.std(axis=0, keepdims=True)\n",
        "    feature_std = np.where(feature_std < 1e-6, 1.0, feature_std)\n",
        "\n",
        "    target_mean = y_train.mean(axis=0, keepdims=True)\n",
        "    target_std = y_train.std(axis=0, keepdims=True)\n",
        "    target_std = np.where(target_std < 1e-6, 1.0, target_std)\n",
        "\n",
        "    def _normalize(arr: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "        return ((arr - mean) / std).astype(np.float32)\n",
        "\n",
        "    X_train = _normalize(X_train, feature_mean, feature_std)\n",
        "    X_val = _normalize(X_val, feature_mean, feature_std)\n",
        "    X_test = _normalize(X_test, feature_mean, feature_std)\n",
        "\n",
        "    y_train_norm = _normalize(y_train, target_mean, target_std)\n",
        "    y_val_norm = _normalize(y_val, target_mean, target_std)\n",
        "    y_test_norm = _normalize(y_test, target_mean, target_std)\n",
        "\n",
        "    bundle = DatasetBundle(\n",
        "        name=\"EAF_chemistry\",\n",
        "        task_type=\"regression\",\n",
        "        input_kind=\"tabular\",\n",
        "        feature_names=feature_cols,\n",
        "        target_names=target_cols,\n",
        "        train={\"X\": X_train, \"y\": y_train_norm},\n",
        "        val={\"X\": X_val, \"y\": y_val_norm},\n",
        "        test={\"X\": X_test, \"y\": y_test_norm},\n",
        "        metadata={\n",
        "            \"target_dim\": len(target_cols),\n",
        "            \"note\": \"heat-level aggregates for final composition\",\n",
        "            \"feature_scaler\": {\n",
        "                \"mean\": feature_mean.flatten().astype(np.float32).tolist(),\n",
        "                \"std\": feature_std.flatten().astype(np.float32).tolist(),\n",
        "            },\n",
        "            \"target_scaler\": {\n",
        "                \"mean\": target_mean.flatten().astype(np.float32).tolist(),\n",
        "                \"std\": target_std.flatten().astype(np.float32).tolist(),\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "    return bundle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "98e9d6d0",
      "metadata": {
        "id": "98e9d6d0"
      },
      "outputs": [],
      "source": [
        "from psann.nn import ResidualPSANNNet\n",
        "\n",
        "\n",
        "class IdentitySpine(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim == 3:\n",
        "            return x.reshape(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TemporalConvSpine(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels: int,\n",
        "        hidden_channels: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 2,\n",
        "        depth: int = 2,\n",
        "        activation: Callable[[], nn.Module] = nn.GELU,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        layers: List[nn.Module] = []\n",
        "        channels = input_channels\n",
        "        for _ in range(depth):\n",
        "            layers.append(\n",
        "                nn.Conv1d(\n",
        "                    channels,\n",
        "                    hidden_channels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=stride,\n",
        "                    padding=kernel_size // 2,\n",
        "                )\n",
        "            )\n",
        "            layers.append(nn.BatchNorm1d(hidden_channels))\n",
        "            layers.append(activation())\n",
        "            channels = hidden_channels\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = x.transpose(1, 2)\n",
        "        z = self.net(z)\n",
        "        z = self.pool(z).squeeze(-1)\n",
        "        return z\n",
        "\n",
        "\n",
        "class TemporalAttentionSpine(nn.Module):\n",
        "    def __init__(self, input_dim: int, num_heads: int = 1, ff_factor: int = 2, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, batch_first=True, dropout=dropout)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Linear(input_dim, ff_factor * input_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(ff_factor * input_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = self.norm(x)\n",
        "        attn_out, _ = self.attn(z, z, z)\n",
        "        z = z + attn_out\n",
        "        z = z + self.ff(z)\n",
        "        return z.mean(dim=1)\n",
        "\n",
        "\n",
        "class FlattenSpine(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim == 3:\n",
        "            return x.reshape(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SequencePSANNModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Tuple[int, ...],\n",
        "        output_dim: int,\n",
        "        *,\n",
        "        hidden_layers: int,\n",
        "        hidden_units: int,\n",
        "        spine_type: str = \"flatten\",\n",
        "        spine_params: Optional[Dict[str, Any]] = None,\n",
        "        activation_type: str = \"psann\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        spine_params = spine_params or {}\n",
        "        time_steps, channels = input_shape\n",
        "        if spine_type == \"conv\":\n",
        "            self.spine = TemporalConvSpine(\n",
        "                channels,\n",
        "                spine_params.get(\"channels\", hidden_units),\n",
        "                kernel_size=spine_params.get(\"kernel_size\", 5),\n",
        "                stride=spine_params.get(\"stride\", 2),\n",
        "                depth=spine_params.get(\"depth\", 2),\n",
        "            )\n",
        "            psann_input_dim = spine_params.get(\"channels\", hidden_units)\n",
        "        elif spine_type == \"attention\":\n",
        "            self.spine = TemporalAttentionSpine(\n",
        "                input_dim=channels,\n",
        "                num_heads=spine_params.get(\"num_heads\", 1),\n",
        "                ff_factor=spine_params.get(\"ff_factor\", 2),\n",
        "                dropout=spine_params.get(\"dropout\", 0.1),\n",
        "            )\n",
        "            psann_input_dim = channels\n",
        "        elif spine_type == \"flatten\":\n",
        "            self.spine = FlattenSpine()\n",
        "            psann_input_dim = time_steps * channels\n",
        "        else:\n",
        "            self.spine = IdentitySpine()\n",
        "            psann_input_dim = time_steps * channels\n",
        "        self.core = ResidualPSANNNet(\n",
        "            psann_input_dim,\n",
        "            output_dim,\n",
        "            hidden_layers=hidden_layers,\n",
        "            hidden_units=hidden_units,\n",
        "            hidden_width=hidden_units,\n",
        "            activation_type=activation_type,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim == 3:\n",
        "            z = self.spine(x)\n",
        "        else:\n",
        "            z = x\n",
        "        return self.core(z)\n",
        "\n",
        "\n",
        "class TabularPSANNModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        *,\n",
        "        hidden_layers: int,\n",
        "        hidden_units: int,\n",
        "        activation_type: str = \"psann\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.core = ResidualPSANNNet(\n",
        "            input_dim,\n",
        "            output_dim,\n",
        "            hidden_layers=hidden_layers,\n",
        "            hidden_units=hidden_units,\n",
        "            hidden_width=hidden_units,\n",
        "            activation_type=activation_type,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim > 2:\n",
        "            x = x.reshape(x.size(0), -1)\n",
        "        return self.core(x)\n",
        "\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int, hidden_layers: int = 3, hidden_units: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        layers: List[nn.Module] = []\n",
        "        in_dim = input_dim\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(nn.Linear(in_dim, hidden_units))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            in_dim = hidden_units\n",
        "        layers.append(nn.Linear(in_dim, output_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim > 2:\n",
        "            x = x.reshape(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class LSTMHead(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_units: int, num_layers: int, output_dim: int, bidirectional: bool = False, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_units,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        out_dim = hidden_units * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Linear(out_dim, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        z = h_n[-1]\n",
        "        return self.head(z)\n",
        "\n",
        "\n",
        "class TinyTCNBlock(nn.Module):\n",
        "    def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(channels, channels, kernel_size, padding=\"same\", dilation=dilation),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(channels, channels, kernel_size, padding=\"same\", dilation=dilation),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.conv(x)\n",
        "\n",
        "\n",
        "class TinyTCN(nn.Module):\n",
        "    def __init__(self, input_channels: int, output_dim: int, hidden_channels: int = 128, layers: int = 3, kernel_size: int = 3, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.pre = nn.Conv1d(input_channels, hidden_channels, kernel_size=1)\n",
        "        blocks = []\n",
        "        for i in range(layers):\n",
        "            blocks.append(TinyTCNBlock(hidden_channels, kernel_size, dilation=2 ** i, dropout=dropout))\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(hidden_channels, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = x.transpose(1, 2)\n",
        "        z = self.pre(z)\n",
        "        z = self.blocks(z)\n",
        "        z = self.head(z)\n",
        "        return z\n",
        "\n",
        "\n",
        "def build_psann_tabular(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
        "    hidden_layers = extra.get(\"hidden_layers\", 8)\n",
        "    hidden_units = extra.get(\"hidden_units\", 256)\n",
        "    activation_type = extra.get(\"activation_type\", \"psann\")\n",
        "    return TabularPSANNModel(\n",
        "        input_dim=int(np.prod(input_shape)),\n",
        "        output_dim=output_dim,\n",
        "        hidden_layers=hidden_layers,\n",
        "        hidden_units=hidden_units,\n",
        "        activation_type=activation_type,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_psann_sequence(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
        "    hidden_layers = extra.get(\"hidden_layers\", 8)\n",
        "    hidden_units = extra.get(\"hidden_units\", 256)\n",
        "    spine_type = extra.get(\"spine_type\", \"flatten\")\n",
        "    spine_params = extra.get(\"spine_params\", {})\n",
        "    activation_type = extra.get(\"activation_type\", \"psann\")\n",
        "    return SequencePSANNModel(\n",
        "        input_shape,\n",
        "        output_dim,\n",
        "        hidden_layers=hidden_layers,\n",
        "        hidden_units=hidden_units,\n",
        "        spine_type=spine_type,\n",
        "        spine_params=spine_params,\n",
        "        activation_type=activation_type,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_mlp_model(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
        "    hidden_layers = extra.get(\"hidden_layers\", 3)\n",
        "    hidden_units = extra.get(\"hidden_units\", 256)\n",
        "    dropout = extra.get(\"dropout\", 0.1)\n",
        "    return MLPModel(\n",
        "        input_dim=int(np.prod(input_shape)),\n",
        "        output_dim=output_dim,\n",
        "        hidden_layers=hidden_layers,\n",
        "        hidden_units=hidden_units,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_lstm_model(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
        "    sequence_length, channels = input_shape\n",
        "    hidden_units = extra.get(\"hidden_units\", 128)\n",
        "    num_layers = extra.get(\"num_layers\", 1)\n",
        "    bidirectional = extra.get(\"bidirectional\", False)\n",
        "    return LSTMHead(\n",
        "        input_dim=channels,\n",
        "        hidden_units=hidden_units,\n",
        "        num_layers=num_layers,\n",
        "        output_dim=output_dim,\n",
        "        bidirectional=bidirectional,\n",
        "        dropout=extra.get(\"dropout\", 0.1),\n",
        "    )\n",
        "\n",
        "\n",
        "def build_tcn_model(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
        "    sequence_length, channels = input_shape\n",
        "    hidden_channels = extra.get(\"hidden_channels\", 128)\n",
        "    layers = extra.get(\"layers\", 3)\n",
        "    kernel_size = extra.get(\"kernel_size\", 3)\n",
        "    dropout = extra.get(\"dropout\", 0.1)\n",
        "    return TinyTCN(\n",
        "        input_channels=channels,\n",
        "        output_dim=output_dim,\n",
        "        hidden_channels=hidden_channels,\n",
        "        layers=layers,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout=dropout,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "b3f91c9b",
      "metadata": {
        "id": "b3f91c9b"
      },
      "outputs": [],
      "source": [
        "def count_trainable_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def denormalize_regression_outputs(\n",
        "    bundle: DatasetBundle, y_true: np.ndarray, y_pred: np.ndarray\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    scaler = bundle.metadata.get(\"target_scaler\") if getattr(bundle, \"metadata\", None) else None\n",
        "    if not scaler:\n",
        "        return y_true, y_pred\n",
        "    mean = np.asarray(scaler.get(\"mean\", 0.0), dtype=np.float32)\n",
        "    std = np.asarray(scaler.get(\"std\", 1.0), dtype=np.float32)\n",
        "    return y_true * std + mean, y_pred * std + mean\n",
        "\n",
        "\n",
        "def evaluate_model(model: nn.Module, loader: DataLoader, spec: ModelSpec) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    param = next(model.parameters(), None)\n",
        "    model_device = param.device if param is not None else torch.device(\"cpu\")\n",
        "\n",
        "    preds: List[np.ndarray] = []\n",
        "    truths: List[np.ndarray] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(model_device)\n",
        "            y_batch = y_batch.to(model_device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            if spec.task_type != \"classification\":\n",
        "                if outputs.ndim > 2:\n",
        "                    outputs = outputs.view(outputs.size(0), -1)\n",
        "                if y_batch.ndim > 2:\n",
        "                    y_batch = y_batch.view(y_batch.size(0), -1)\n",
        "                elif y_batch.ndim == 1:\n",
        "                    y_batch = y_batch.unsqueeze(-1)\n",
        "\n",
        "            preds.append(outputs.detach().cpu().numpy())\n",
        "            truths.append(y_batch.detach().cpu().numpy())\n",
        "\n",
        "    if not preds:\n",
        "        raise ValueError(\"Evaluation loader produced no batches; check dataset splits and batch size.\")\n",
        "\n",
        "    y_pred = np.concatenate(preds, axis=0)\n",
        "    y_true = np.concatenate(truths, axis=0)\n",
        "    return y_true, y_pred\n",
        "\n",
        "\n",
        "def train_model_on_bundle(bundle: DatasetBundle, spec: ModelSpec, task_name: str) -> Dict[str, Any]:\n",
        "    input_shape = bundle.train[\"X\"].shape[1:]\n",
        "    if spec.task_type == \"classification\":\n",
        "        output_dim = int(bundle.metadata.get(\"n_classes\", np.unique(bundle.train[\"y\"]).size))\n",
        "    else:\n",
        "        output_dim = bundle.train[\"y\"].shape[1] if bundle.train[\"y\"].ndim > 1 else 1\n",
        "\n",
        "    model = spec.builder(input_shape, output_dim, spec.extra)\n",
        "    model.to(DEVICE)\n",
        "    params = count_trainable_parameters(model)\n",
        "\n",
        "    optimizer_cls = torch.optim.AdamW if spec.train_config.weight_decay > 0 else torch.optim.Adam\n",
        "    optimizer = optimizer_cls(\n",
        "        model.parameters(),\n",
        "        lr=spec.train_config.learning_rate,\n",
        "        weight_decay=spec.train_config.weight_decay,\n",
        "    )\n",
        "    scheduler = None\n",
        "    if spec.train_config.scheduler == \"cosine\":\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=spec.train_config.epochs)\n",
        "\n",
        "    train_loader = build_dataloader(\n",
        "        bundle.train[\"X\"],\n",
        "        bundle.train[\"y\"],\n",
        "        spec.train_config.batch_size,\n",
        "        shuffle=True,\n",
        "        task_type=spec.task_type,\n",
        "    )\n",
        "    val_loader = build_dataloader(\n",
        "        bundle.val[\"X\"],\n",
        "        bundle.val[\"y\"],\n",
        "        spec.train_config.batch_size,\n",
        "        shuffle=False,\n",
        "        task_type=spec.task_type,\n",
        "    )\n",
        "    test_loader = build_dataloader(\n",
        "        bundle.test[\"X\"],\n",
        "        bundle.test[\"y\"],\n",
        "        spec.train_config.batch_size,\n",
        "        shuffle=False,\n",
        "        task_type=spec.task_type,\n",
        "    )\n",
        "\n",
        "    best_state = None\n",
        "    best_val_metric = -float(\"inf\")\n",
        "    patience_counter = spec.train_config.patience\n",
        "    history: List[Dict[str, float]] = []\n",
        "    criterion_reg = nn.MSELoss()\n",
        "\n",
        "    with Timer() as timer:\n",
        "        for epoch in range(spec.train_config.epochs):\n",
        "            model.train()\n",
        "            model_device = next(model.parameters(), DEVICE).device\n",
        "            running_loss = 0.0\n",
        "            batches = 0\n",
        "\n",
        "            for step, (X_batch, y_batch) in enumerate(train_loader, start=1):\n",
        "                X_batch = X_batch.to(model_device)\n",
        "                y_batch = y_batch.to(model_device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                if spec.task_type != \"classification\" and outputs.ndim > 2:\n",
        "                    outputs = outputs.view(outputs.size(0), -1)\n",
        "\n",
        "                if spec.task_type == \"classification\":\n",
        "                    loss = nn.functional.cross_entropy(\n",
        "                        outputs,\n",
        "                        y_batch,\n",
        "                        label_smoothing=GLOBAL_CONFIG[\"label_smoothing\"],\n",
        "                    )\n",
        "                else:\n",
        "                    target = y_batch\n",
        "                    if target.ndim > 2:\n",
        "                        target = target.view(target.size(0), -1)\n",
        "                    elif target.ndim == 1:\n",
        "                        target = target.unsqueeze(-1)\n",
        "                    loss = criterion_reg(outputs, target)\n",
        "\n",
        "                loss.backward()\n",
        "                if spec.train_config.gradient_clip is not None:\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), spec.train_config.gradient_clip)\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                batches += 1\n",
        "                if spec.train_config.max_batches_per_epoch and batches >= spec.train_config.max_batches_per_epoch:\n",
        "                    break\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            avg_loss = running_loss / max(1, batches)\n",
        "            val_true, val_pred = evaluate_model(model, val_loader, spec)\n",
        "\n",
        "            if spec.task_type == \"classification\":\n",
        "                metrics = classification_metrics(val_true, val_pred)\n",
        "                score = metrics[\"accuracy\"]\n",
        "            else:\n",
        "                val_true_den, val_pred_den = denormalize_regression_outputs(bundle, val_true, val_pred)\n",
        "                metrics = regression_metrics(val_true_den, val_pred_den)\n",
        "                score = -metrics[\"rmse\"]\n",
        "\n",
        "            history.append({\"epoch\": epoch + 1, \"train_loss\": avg_loss, \"val_score\": score})\n",
        "\n",
        "            if score > best_val_metric:\n",
        "                best_val_metric = score\n",
        "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "                patience_counter = spec.train_config.patience\n",
        "            else:\n",
        "                patience_counter -= 1\n",
        "\n",
        "            if spec.train_config.early_stopping and patience_counter <= 0:\n",
        "                break\n",
        "            if spec.train_config.max_minutes is not None and timer.elapsed / 60.0 > spec.train_config.max_minutes:\n",
        "                print(f\"[INFO] Time budget reached for {spec.name}; stopping at epoch {epoch + 1}.\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    train_true, train_pred = evaluate_model(model, train_loader, spec)\n",
        "    val_true, val_pred = evaluate_model(model, val_loader, spec)\n",
        "    test_true, test_pred = evaluate_model(model, test_loader, spec)\n",
        "\n",
        "    if spec.task_type == \"classification\":\n",
        "        train_metrics = classification_metrics(train_true, train_pred)\n",
        "        val_metrics = classification_metrics(val_true, val_pred)\n",
        "        test_metrics = classification_metrics(test_true, test_pred)\n",
        "\n",
        "        train_true_out, train_pred_out = train_true, train_pred\n",
        "        val_true_out, val_pred_out = val_true, val_pred\n",
        "        test_true_out, test_pred_out = test_true, test_pred\n",
        "    else:\n",
        "        train_true_den, train_pred_den = denormalize_regression_outputs(bundle, train_true, train_pred)\n",
        "        val_true_den, val_pred_den = denormalize_regression_outputs(bundle, val_true, val_pred)\n",
        "        test_true_den, test_pred_den = denormalize_regression_outputs(bundle, test_true, test_pred)\n",
        "\n",
        "        train_metrics = regression_metrics(train_true_den, train_pred_den)\n",
        "        val_metrics = regression_metrics(val_true_den, val_pred_den)\n",
        "        test_metrics = regression_metrics(test_true_den, test_pred_den)\n",
        "\n",
        "        train_true_out, train_pred_out = train_true_den, train_pred_den\n",
        "        val_true_out, val_pred_out = val_true_den, val_pred_den\n",
        "        test_true_out, test_pred_out = test_true_den, test_pred_den\n",
        "\n",
        "    RESULT_LOGGER.append(\n",
        "        ExperimentResult(\n",
        "            dataset=bundle.name,\n",
        "            task=task_name,\n",
        "            model=spec.name,\n",
        "            group=spec.group,\n",
        "            split=\"train\",\n",
        "            params=params,\n",
        "            train_wall_seconds=timer.elapsed,\n",
        "            metrics=train_metrics,\n",
        "            notes=spec.notes,\n",
        "        )\n",
        "    )\n",
        "    RESULT_LOGGER.append(\n",
        "        ExperimentResult(\n",
        "            dataset=bundle.name,\n",
        "            task=task_name,\n",
        "            model=spec.name,\n",
        "            group=spec.group,\n",
        "            split=\"val\",\n",
        "            params=params,\n",
        "            train_wall_seconds=timer.elapsed,\n",
        "            metrics=val_metrics,\n",
        "            notes=spec.notes,\n",
        "        )\n",
        "    )\n",
        "    RESULT_LOGGER.append(\n",
        "        ExperimentResult(\n",
        "            dataset=bundle.name,\n",
        "            task=task_name,\n",
        "            model=spec.name,\n",
        "            group=spec.group,\n",
        "            split=\"test\",\n",
        "            params=params,\n",
        "            train_wall_seconds=timer.elapsed,\n",
        "            metrics=test_metrics,\n",
        "            notes=spec.notes,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    model_cpu = model.to(\"cpu\")\n",
        "\n",
        "    return {\n",
        "        \"model\": model_cpu,\n",
        "        \"train_metrics\": train_metrics,\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"train_true\": train_true_out,\n",
        "        \"train_pred\": train_pred_out,\n",
        "        \"val_true\": val_true_out,\n",
        "        \"val_pred\": val_pred_out,\n",
        "        \"test_true\": test_true_out,\n",
        "        \"test_pred\": test_pred_out,\n",
        "        \"history\": history,\n",
        "        \"params\": params,\n",
        "        \"train_time\": timer.elapsed,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "95235c68",
      "metadata": {
        "id": "95235c68"
      },
      "outputs": [],
      "source": [
        "def permutation_importance(\n",
        "    model: nn.Module,\n",
        "    bundle: DatasetBundle,\n",
        "    spec: ModelSpec,\n",
        "    feature_groups: Dict[str, List[int]],\n",
        "    split: str = \"test\",\n",
        "    n_repeats: int = 5,\n",
        ") -> pd.DataFrame:\n",
        "    data = getattr(bundle, split)\n",
        "    baseline_loader = build_dataloader(\n",
        "        data[\"X\"],\n",
        "        data[\"y\"],\n",
        "        spec.train_config.batch_size,\n",
        "        shuffle=False,\n",
        "        task_type=spec.task_type,\n",
        "    )\n",
        "    y_true, y_pred = evaluate_model(model, baseline_loader, spec)\n",
        "    if spec.task_type == \"classification\":\n",
        "        baseline_metric = classification_metrics(y_true, y_pred)[\"accuracy\"]\n",
        "    else:\n",
        "        baseline_metric = regression_metrics(y_true.squeeze(), y_pred.squeeze())[\"rmse\"]\n",
        "\n",
        "    rows = []\n",
        "    for group_name, columns in feature_groups.items():\n",
        "        deltas = []\n",
        "        cols = np.atleast_1d(columns)\n",
        "        for _ in range(n_repeats):\n",
        "            X_perm = data[\"X\"].copy()\n",
        "            if bundle.input_kind == \"tabular\":\n",
        "                for col in cols:\n",
        "                    np.random.shuffle(X_perm[:, col])\n",
        "            else:\n",
        "                for col in cols:\n",
        "                    np.random.shuffle(X_perm[:, :, col])\n",
        "            loader = build_dataloader(\n",
        "                X_perm,\n",
        "                data[\"y\"],\n",
        "                spec.train_config.batch_size,\n",
        "                shuffle=False,\n",
        "                task_type=spec.task_type,\n",
        "            )\n",
        "            y_true_perm, y_pred_perm = evaluate_model(model, loader, spec)\n",
        "            if spec.task_type == \"classification\":\n",
        "                metric_value = classification_metrics(y_true_perm, y_pred_perm)[\"accuracy\"]\n",
        "                delta = baseline_metric - metric_value\n",
        "            else:\n",
        "                metric_value = regression_metrics(y_true_perm.squeeze(), y_pred_perm.squeeze())[\"rmse\"]\n",
        "                delta = metric_value - baseline_metric\n",
        "            deltas.append(delta)\n",
        "        rows.append(\n",
        "            {\n",
        "                \"group\": group_name,\n",
        "                \"mean_delta\": float(np.mean(deltas)),\n",
        "                \"std_delta\": float(np.std(deltas)),\n",
        "                \"baseline\": baseline_metric,\n",
        "            }\n",
        "        )\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def compute_shap_importance(\n",
        "    model: nn.Module,\n",
        "    bundle: DatasetBundle,\n",
        "    spec: ModelSpec,\n",
        "    split: str = \"val\",\n",
        "    sample_size: int = 512,\n",
        ") -> Dict[str, Any]:\n",
        "    import shap\n",
        "\n",
        "    data = getattr(bundle, split)\n",
        "    X = data[\"X\"]\n",
        "    if len(X) == 0:\n",
        "        raise ValueError(f\"No samples available in {split} split for SHAP computation.\")\n",
        "    sample_size = min(sample_size, len(X))\n",
        "    idx = np.random.choice(len(X), size=sample_size, replace=False)\n",
        "    X_sample = X[idx]\n",
        "\n",
        "    model_cpu = model.to(\"cpu\").eval()\n",
        "\n",
        "    def predict_fn(batch: np.ndarray) -> np.ndarray:\n",
        "        with torch.no_grad():\n",
        "            inputs = torch.from_numpy(batch).float()\n",
        "            outputs = model_cpu(inputs)\n",
        "            if spec.task_type == \"classification\":\n",
        "                return torch.softmax(outputs, dim=-1).numpy()\n",
        "            return outputs.numpy()\n",
        "\n",
        "    if bundle.input_kind == \"tabular\":\n",
        "        background = X_sample[: min(128, sample_size)]\n",
        "        explainer = shap.KernelExplainer(predict_fn, background)\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "    else:\n",
        "        background = torch.from_numpy(X_sample[: min(64, sample_size)]).float()\n",
        "        explainer = shap.DeepExplainer(model_cpu, background)\n",
        "        shap_values = explainer.shap_values(torch.from_numpy(X_sample).float())\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    return {\"explainer\": explainer, \"shap_values\": shap_values, \"sample_indices\": idx}\n",
        "\n",
        "\n",
        "def compute_jacobian_singular_values(model: nn.Module, inputs: torch.Tensor, max_samples: int = 128) -> np.ndarray:\n",
        "    model.eval()\n",
        "    inputs = inputs[:max_samples].to(DEVICE).requires_grad_(True)\n",
        "    outputs = model(inputs)\n",
        "    if outputs.ndim == 1:\n",
        "        outputs = outputs.unsqueeze(-1)\n",
        "    jacobian_rows = []\n",
        "    for i in range(outputs.shape[1]):\n",
        "        grad_outputs = torch.zeros_like(outputs)\n",
        "        grad_outputs[:, i] = 1.0\n",
        "        grads = torch.autograd.grad(outputs, inputs, grad_outputs=grad_outputs, retain_graph=True, create_graph=False)[0]\n",
        "        jacobian_rows.append(grads.reshape(grads.size(0), -1).detach().cpu().numpy())\n",
        "    jacobian = np.concatenate(jacobian_rows, axis=1)\n",
        "    sigma = np.linalg.svd(jacobian, compute_uv=False)\n",
        "    return sigma\n",
        "\n",
        "\n",
        "def participation_ratio(singular_values: np.ndarray) -> float:\n",
        "    if singular_values.size == 0:\n",
        "        return float(\"nan\")\n",
        "    numerator = (singular_values ** 2).sum() ** 2\n",
        "    denominator = (singular_values ** 4).sum() + 1e-8\n",
        "    return float(numerator / denominator)\n",
        "\n",
        "\n",
        "def frequency_response_probe(model: nn.Module, input_dim: int, frequencies: Iterable[float], amplitude: float = 1.0) -> pd.DataFrame:\n",
        "    model.eval()\n",
        "    rows = []\n",
        "    times = torch.linspace(0, 2 * math.pi, steps=512).unsqueeze(0)\n",
        "    for freq in frequencies:\n",
        "        signal = amplitude * torch.sin(freq * times)\n",
        "        if input_dim > 1:\n",
        "            signal = signal.repeat(1, input_dim)\n",
        "        signal = signal.to(DEVICE).float()\n",
        "        with torch.no_grad():\n",
        "            output = model(signal)\n",
        "        energy = output.pow(2).mean().sqrt().item()\n",
        "        rows.append({\"frequency\": freq, \"output_rms\": energy})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def evaluate_robustness(\n",
        "    model: nn.Module,\n",
        "    bundle: DatasetBundle,\n",
        "    spec: ModelSpec,\n",
        "    corruption_fn: Callable[[np.ndarray, float], np.ndarray],\n",
        "    split: str = \"test\",\n",
        "    levels: Iterable[float] = (0.0, 0.1, 0.2, 0.3),\n",
        ") -> pd.DataFrame:\n",
        "    rows = []\n",
        "    base_data = getattr(bundle, split)\n",
        "    for level in levels:\n",
        "        X_corrupted = corruption_fn(base_data[\"X\"], level)\n",
        "        loader = build_dataloader(\n",
        "            X_corrupted,\n",
        "            base_data[\"y\"],\n",
        "            spec.train_config.batch_size,\n",
        "            shuffle=False,\n",
        "            task_type=spec.task_type,\n",
        "        )\n",
        "        y_true, y_pred = evaluate_model(model, loader, spec)\n",
        "        if spec.task_type == \"classification\":\n",
        "            metrics = classification_metrics(y_true, y_pred)\n",
        "        else:\n",
        "            metrics = regression_metrics(y_true.squeeze(), y_pred.squeeze())\n",
        "        row = {\"level\": level}\n",
        "        row.update(metrics)\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "f64a25c2",
      "metadata": {
        "id": "f64a25c2"
      },
      "outputs": [],
      "source": [
        "# Cell (1) — helpers + containers\n",
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "\n",
        "# Where your datasets live (should already be defined in the notebook; override only if missing)\n",
        "try:\n",
        "    DATA_ROOT\n",
        "except NameError:\n",
        "    DATA_ROOT = Path(\"/content/datasets\")\n",
        "\n",
        "# container for results\n",
        "DATA_BUNDLES: Dict[str, \"DatasetBundle\"] = {}\n",
        "\n",
        "def find_key(stations: dict, short_name: str) -> str:\n",
        "    \"\"\"Find the canonical station key by case-insensitive substring match.\n",
        "    Raises KeyError if no match found.\n",
        "    \"\"\"\n",
        "    short = short_name.lower()\n",
        "    matches = [k for k in stations.keys() if short in k.lower()]\n",
        "    if not matches:\n",
        "        raise KeyError(f\"No station matching '{short_name}'\")\n",
        "    # prefer exact prefix match if available (more deterministic)\n",
        "    for m in matches:\n",
        "        if m.lower().startswith(short):\n",
        "            return m\n",
        "    return matches[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "aefaphdaPGoq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aefaphdaPGoq",
        "outputId": "79b996cb-bef1-407f-b6f4-93aace68e729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Loading eaf_temp from /content/datasets/Industrial Data from the Electric Arc Furnace/eaf_temp.csv...\n",
            "Loading eaf_gaslance_mat from /content/datasets/Industrial Data from the Electric Arc Furnace/eaf_gaslance_mat.csv...\n",
            "Loading inj_mat from /content/datasets/Industrial Data from the Electric Arc Furnace/inj_mat.csv...\n",
            "Loading eaf_transformer from /content/datasets/Industrial Data from the Electric Arc Furnace/eaf_transformer.csv...\n",
            "Loading eaf_added_materials from /content/datasets/Industrial Data from the Electric Arc Furnace/eaf_added_materials.csv...\n",
            "Loading basket_charged from /content/datasets/Industrial Data from the Electric Arc Furnace/basket_charged.csv...\n",
            "Loading lf_added_materials from /content/datasets/Industrial Data from the Electric Arc Furnace/lf_added_materials.csv...\n",
            "Loading lf_initial_chemical_measurements from /content/datasets/Industrial Data from the Electric Arc Furnace/lf_initial_chemical_measurements.csv...\n",
            "Loading eaf_final_chemical_measurements from /content/datasets/Industrial Data from the Electric Arc Furnace/eaf_final_chemical_measurements.csv...\n",
            "Loading ladle_tapping from /content/datasets/Industrial Data from the Electric Arc Furnace/ladle_tapping.csv...\n",
            " - EAF bundles ready\n",
            "Loading Beijing station Changping_20130301-20170228...\n",
            "Loading Beijing station Aotizhongxin_20130301-20170228...\n",
            "Loading Beijing station Huairou_20130301-20170228...\n",
            "Loading Beijing station Gucheng_20130301-20170228...\n",
            "Loading Beijing station Wanliu_20130301-20170228...\n",
            "Loading Beijing station Shunyi_20130301-20170228...\n",
            "Loading Beijing station Tiantan_20130301-20170228...\n",
            "Loading Beijing station Dingling_20130301-20170228...\n",
            "Loading Beijing station Dongsi_20130301-20170228...\n",
            "Loading Beijing station Wanshouxigong_20130301-20170228...\n",
            "Loading Beijing station Nongzhanguan_20130301-20170228...\n",
            "Loading Beijing station Guanyuan_20130301-20170228...\n",
            " - Beijing: val=Wanshouxigong_20130301-20170228, test=Huairou_20130301-20170228, train_count=10\n",
            " - Beijing bundle ready\n",
            " - Jena bundle ready\n",
            " - HAR engineered bundle ready\n",
            " - HAR raw bundle ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3127595546.py:6: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train = pd.read_csv(train_path, parse_dates=[\"Date\"])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Rossmann bundle ready\n",
            "\n",
            "Available dataset bundles:\n",
            " - EAF_TEMP_forecast: {'name': 'EAF_TEMP_forecast', 'task_type': 'regression', 'input_kind': 'tabular', 'n_train': 1984, 'n_val': 429, 'n_test': 89, 'input_shape': (34,), 'target_shape': (1,), 'meta_horizon_steps': 1, 'meta_feature_source': 'temp + gas + injection + calendar'}\n",
            " - EAF_VALO2_forecast: {'name': 'EAF_VALO2_forecast', 'task_type': 'regression', 'input_kind': 'tabular', 'n_train': 1984, 'n_val': 429, 'n_test': 89, 'input_shape': (34,), 'target_shape': (1,), 'meta_horizon_steps': 1, 'meta_feature_source': 'temp + gas + injection + calendar'}\n",
            " - EAF_chemistry: {'name': 'EAF_chemistry', 'task_type': 'regression', 'input_kind': 'tabular', 'n_train': 1189, 'n_val': 290, 'n_test': 932, 'input_shape': (33,), 'target_shape': (12,), 'meta_target_dim': 12, 'meta_note': 'heat-level aggregates for final composition'}\n",
            " - Beijing_PM25_24h_ctx_6h_horizon: {'name': 'Beijing_PM25_24h_ctx_6h_horizon', 'task_type': 'regression', 'input_kind': 'sequence', 'n_train': 350340, 'n_val': 35034, 'n_test': 35034, 'input_shape': (24, 30), 'target_shape': (2,), 'meta_context_hours': 24, 'meta_horizon_hours': 6, 'meta_val_station': 'Wanshouxigong_20130301-20170228', 'meta_test_station': 'Huairou_20130301-20170228'}\n",
            " - Jena_tdegc_72ctx_36h: {'name': 'Jena_tdegc_72ctx_36h', 'task_type': 'regression', 'input_kind': 'sequence', 'n_train': 315659, 'n_val': 52560, 'n_test': 52224, 'input_shape': (72, 24), 'target_shape': (1,), 'meta_context_steps': 72, 'meta_horizon_steps': 36, 'meta_resample_factor': 1}\n",
            " - HAR_engineered: {'name': 'HAR_engineered', 'task_type': 'classification', 'input_kind': 'tabular', 'n_train': 6249, 'n_val': 1103, 'n_test': 2947, 'input_shape': (813,), 'target_shape': (1,), 'meta_n_classes': 6}\n",
            " - HAR_raw_sequence: {'name': 'HAR_raw_sequence', 'task_type': 'classification', 'input_kind': 'sequence', 'n_train': 6249, 'n_val': 1103, 'n_test': 2947, 'input_shape': (128, 9), 'target_shape': (1,), 'meta_sequence_length': 128, 'meta_n_channels': 9, 'meta_n_classes': 6}\n",
            " - Rossmann_sales: {'name': 'Rossmann_sales', 'task_type': 'regression', 'input_kind': 'tabular', 'n_train': 663738, 'n_val': 87212, 'n_test': 85637, 'input_shape': (33,), 'target_shape': (1,)}\n"
          ]
        }
      ],
      "source": [
        "# Cell (2) — load all bundles (robust to small naming mismatches)\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# --- EAF bundles (these will use the merge_asof you've patched earlier) ---\n",
        "try:\n",
        "    eaf_tables = load_eaf_tables(DATA_ROOT)\n",
        "    eaf_temp_bundle, eaf_o2_bundle = prepare_eaf_temp_and_o2_bundles(eaf_tables)\n",
        "    eaf_chem_bundle = prepare_eaf_chemistry_bundle(eaf_tables)\n",
        "    DATA_BUNDLES[eaf_temp_bundle.name] = eaf_temp_bundle\n",
        "    DATA_BUNDLES[eaf_o2_bundle.name]   = eaf_o2_bundle\n",
        "    DATA_BUNDLES[eaf_chem_bundle.name] = eaf_chem_bundle\n",
        "    print(\" - EAF bundles ready\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] EAF bundle creation failed:\", repr(e))\n",
        "\n",
        "# --- Beijing cross-station bundle (use robust mapping for station names) ---\n",
        "try:\n",
        "    beijing_stations = load_beijing_stations(DATA_ROOT)\n",
        "    # map the short names you expect to their actual keys\n",
        "    val_key  = find_key(beijing_stations, \"Wanshouxigong\")\n",
        "    test_key = find_key(beijing_stations, \"Huairou\")\n",
        "    train_keys = [k for k in beijing_stations.keys() if k not in {val_key, test_key}]\n",
        "    print(f\" - Beijing: val={val_key}, test={test_key}, train_count={len(train_keys)}\")\n",
        "\n",
        "    beijing_bundle = assemble_beijing_cross_station_bundle(\n",
        "        beijing_stations,\n",
        "        train_stations=train_keys,\n",
        "        val_station=val_key,\n",
        "        test_station=test_key,\n",
        "        target=\"PM2.5\",\n",
        "        context=24,\n",
        "        horizon=6,\n",
        "    )\n",
        "    DATA_BUNDLES[beijing_bundle.name] = beijing_bundle\n",
        "    print(\" - Beijing bundle ready\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] Beijing bundle creation failed:\", repr(e))\n",
        "\n",
        "# --- Jena ---\n",
        "try:\n",
        "    jena_df = load_jena_climate(DATA_ROOT)\n",
        "    jena_bundle = prepare_jena_bundle(jena_df, target=\"T (degC)\", context_steps=72, horizon_steps=36)\n",
        "    DATA_BUNDLES[jena_bundle.name] = jena_bundle\n",
        "    print(\" - Jena bundle ready\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] Jena bundle creation failed:\", repr(e))\n",
        "\n",
        "# --- HAR (engineered + raw) ---\n",
        "try:\n",
        "    har_train_df, har_test_df, har_feature_names = load_har_engineered(DATA_ROOT)\n",
        "    har_engineered_bundle = prepare_har_engineered_bundle(har_train_df, har_test_df, har_feature_names)\n",
        "    DATA_BUNDLES[har_engineered_bundle.name] = har_engineered_bundle\n",
        "    print(\" - HAR engineered bundle ready\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] HAR engineered creation failed:\", repr(e))\n",
        "\n",
        "try:\n",
        "    X_har_train_raw, y_har_train_raw, X_har_test_raw, y_har_test_raw, har_axes = load_har_raw_sequences(DATA_ROOT)\n",
        "    har_raw_bundle = prepare_har_raw_bundle(X_har_train_raw, y_har_train_raw, X_har_test_raw, y_har_test_raw)\n",
        "    DATA_BUNDLES[har_raw_bundle.name] = har_raw_bundle\n",
        "    print(\" - HAR raw bundle ready\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] HAR raw creation failed:\", repr(e))\n",
        "\n",
        "# --- Rossmann ---\n",
        "try:\n",
        "    ross_train, ross_test, ross_store = load_rossmann_frames(DATA_ROOT)\n",
        "    ross_prepared, ross_features, ross_target = preprocess_rossmann(ross_train, ross_store)\n",
        "    ross_bundle = prepare_rossmann_bundle(ross_prepared, ross_features, ross_target)\n",
        "    DATA_BUNDLES[ross_bundle.name] = ross_bundle\n",
        "    print(\" - Rossmann bundle ready\")\n",
        "except Exception as e:\n",
        "    print(\"[warn] Rossmann bundle creation failed:\", repr(e))\n",
        "\n",
        "# --- Summary of what succeeded ---\n",
        "print(\"\\nAvailable dataset bundles:\")\n",
        "for name, bundle in DATA_BUNDLES.items():\n",
        "    try:\n",
        "        print(f\" - {name}: {bundle.summary()}\")\n",
        "    except Exception:\n",
        "        print(f\" - {name}: (created, but summary() failed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "526ead74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "526ead74",
        "outputId": "b6f0b0a9-8435-4832-8758-9669877b28ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registered model specs:\n",
            "- EAF_TEMP_forecast: ['ResPSANN_tabular', 'MLP_baseline']\n",
            "- EAF_VALO2_forecast: ['ResPSANN_tabular', 'MLP_baseline']\n",
            "- EAF_chemistry: ['ResPSANN_tabular', 'MLP_baseline']\n",
            "- Beijing_PM25_24h_ctx_6h_horizon: ['ResPSANN_conv_spine', 'ResPSANN_attention_spine', 'LSTM_baseline', 'TCN_baseline']\n",
            "- Jena_tdegc_72ctx_36h: ['ResPSANN_conv_spine', 'ResPSANN_attention_spine', 'LSTM_baseline', 'TCN_baseline']\n",
            "- HAR_engineered: ['ResPSANN_tabular', 'MLP_baseline']\n",
            "- HAR_raw_sequence: ['ResPSANN_conv_spine', 'ResPSANN_attention_spine', 'LSTM_baseline', 'TCN_baseline']\n",
            "- Rossmann_sales: ['ResPSANN_tabular', 'MLP_baseline']\n"
          ]
        }
      ],
      "source": [
        "EXPERIMENT_REGISTRY: Dict[str, List[ModelSpec]] = {}\n",
        "\n",
        "common_regression_train = TrainConfig(\n",
        "    epochs=60,\n",
        "    batch_size=512,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=10,\n",
        "    max_minutes=GLOBAL_CONFIG[\"max_time_minutes\"],\n",
        "    gradient_clip=1.0,\n",
        ")\n",
        "\n",
        "common_sequence_train = TrainConfig(\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=8,\n",
        "    max_minutes=GLOBAL_CONFIG[\"max_time_minutes\"],\n",
        "    gradient_clip=1.0,\n",
        ")\n",
        "\n",
        "common_classification_train = TrainConfig(\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=5e-5,\n",
        "    patience=8,\n",
        "    max_minutes=GLOBAL_CONFIG[\"max_time_minutes\"],\n",
        "    gradient_clip=1.0,\n",
        ")\n",
        "\n",
        "\n",
        "def register_specs(bundle: DatasetBundle):\n",
        "    specs: List[ModelSpec] = []\n",
        "    if bundle.input_kind == \"tabular\":\n",
        "        train_cfg = common_regression_train if bundle.task_type == \"regression\" else common_classification_train\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=\"ResPSANN_tabular\",\n",
        "                builder=build_psann_tabular,\n",
        "                train_config=train_cfg,\n",
        "                task_type=bundle.task_type,\n",
        "                input_kind=\"tabular\",\n",
        "                group=\"psann\",\n",
        "                extra={\"hidden_layers\": 8, \"hidden_units\": 256},\n",
        "                notes=\"Residual PSANN core\",\n",
        "            )\n",
        "        )\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=\"MLP_baseline\",\n",
        "                builder=build_mlp_model,\n",
        "                train_config=train_cfg,\n",
        "                task_type=bundle.task_type,\n",
        "                input_kind=\"tabular\",\n",
        "                group=\"baseline\",\n",
        "                extra={\"hidden_layers\": 4, \"hidden_units\": 256, \"dropout\": 0.1},\n",
        "                notes=\"ReLU MLP with similar parameter budget\",\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        train_cfg = common_sequence_train if bundle.task_type == \"regression\" else common_classification_train\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=\"ResPSANN_conv_spine\",\n",
        "                builder=build_psann_sequence,\n",
        "                train_config=train_cfg,\n",
        "                task_type=bundle.task_type,\n",
        "                input_kind=\"sequence\",\n",
        "                group=\"psann\",\n",
        "                extra={\n",
        "                    \"hidden_layers\": 6,\n",
        "                    \"hidden_units\": 192,\n",
        "                    \"spine_type\": \"conv\",\n",
        "                    \"spine_params\": {\"channels\": 192, \"depth\": 2, \"kernel_size\": 5, \"stride\": 2},\n",
        "                },\n",
        "                notes=\"ResPSANN with strided Conv1d spine\",\n",
        "            )\n",
        "        )\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=\"ResPSANN_attention_spine\",\n",
        "                builder=build_psann_sequence,\n",
        "                train_config=train_cfg,\n",
        "                task_type=bundle.task_type,\n",
        "                input_kind=\"sequence\",\n",
        "                group=\"psann\",\n",
        "                extra={\n",
        "                    \"hidden_layers\": 6,\n",
        "                    \"hidden_units\": 192,\n",
        "                    \"spine_type\": \"attention\",\n",
        "                    \"spine_params\": {\"num_heads\": 1},\n",
        "                },\n",
        "                notes=\"ResPSANN with single-head attention spine\",\n",
        "            )\n",
        "        )\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=\"LSTM_baseline\",\n",
        "                builder=build_lstm_model,\n",
        "                train_config=train_cfg,\n",
        "                task_type=bundle.task_type,\n",
        "                input_kind=\"sequence\",\n",
        "                group=\"baseline\",\n",
        "                extra={\"hidden_units\": 192, \"num_layers\": 1, \"dropout\": 0.1},\n",
        "                notes=\"Single-layer LSTM baseline\",\n",
        "            )\n",
        "        )\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=\"TCN_baseline\",\n",
        "                builder=build_tcn_model,\n",
        "                train_config=train_cfg,\n",
        "                task_type=bundle.task_type,\n",
        "                input_kind=\"sequence\",\n",
        "                group=\"baseline\",\n",
        "                extra={\"hidden_channels\": 192, \"layers\": 3, \"kernel_size\": 3, \"dropout\": 0.1},\n",
        "                notes=\"Tiny TCN baseline\",\n",
        "            )\n",
        "        )\n",
        "    EXPERIMENT_REGISTRY[bundle.name] = specs\n",
        "\n",
        "\n",
        "for bundle in DATA_BUNDLES.values():\n",
        "    register_specs(bundle)\n",
        "\n",
        "print(\"Registered model specs:\")\n",
        "for dataset_name, specs in EXPERIMENT_REGISTRY.items():\n",
        "    print(f\"- {dataset_name}: {[spec.name for spec in specs]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "fae9ab43",
      "metadata": {
        "id": "fae9ab43"
      },
      "outputs": [],
      "source": [
        "RUN_EXPERIMENTS = {\n",
        "    \"EAF_TEMP_forecast\": True,\n",
        "    \"EAF_VALO2_forecast\": True,\n",
        "    \"EAF_chemistry\": True,\n",
        "    \"Beijing_PM25_24h_ctx_6h_horizon\": True,\n",
        "    \"Jena_tdegc_72ctx_36h\": True,\n",
        "    \"HAR_engineered\": True,\n",
        "    \"HAR_raw_sequence\": True,\n",
        "    \"Rossmann_sales\": True,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "ed27ae56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed27ae56",
        "outputId": "ee3a4aa6-6b61-4e5a-e1e7-f2d533942564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Dataset: EAF_TEMP_forecast (regression, tabular)\n",
            "  -> Training ResPSANN_tabular\n",
            "    Validation metrics: {'rmse': 20.683494547953096, 'mae': 13.403224704982517, 'smape': 0.008165598887099217, 'r2': 0.23956036913762524, 'mase': 0.9367374542330634}\n",
            "    Test metrics       : {'rmse': 24.962252279244773, 'mae': 15.8895263671875, 'smape': 0.009694324685986012, 'r2': -0.03632192811632873, 'mase': 0.7441608939103983}\n",
            "    Saved predictions to /content/colab_results/EAF_TEMP_forecast_ResPSANN_tabular_predictions.npz\n",
            "  -> Training MLP_baseline\n",
            "    Validation metrics: {'rmse': 19.917977433661637, 'mae': 12.470661012164918, 'smape': 0.007600588039037466, 'r2': 0.29480799162011695, 'mase': 0.8715615462894025}\n",
            "    Test metrics       : {'rmse': 25.901293090117896, 'mae': 15.062251744645366, 'smape': 0.009193397514782058, 'r2': -0.11575808915917318, 'mase': 0.70541679239384}\n",
            "    Saved predictions to /content/colab_results/EAF_TEMP_forecast_MLP_baseline_predictions.npz\n",
            "================================================================================\n",
            "Dataset: EAF_VALO2_forecast (regression, tabular)\n",
            "  -> Training ResPSANN_tabular\n",
            "    Validation metrics: {'rmse': 678.111530723209, 'mae': 410.86045599094916, 'smape': 0.4350837223164569, 'r2': -0.4637856987269626, 'mase': 1.4972946709963306}\n",
            "    Test metrics       : {'rmse': 524.9421469104342, 'mae': 420.5056207206812, 'smape': 0.657230005079091, 'r2': -1.2730722571656896, 'mase': 1.477874300975255}\n",
            "    Saved predictions to /content/colab_results/EAF_VALO2_forecast_ResPSANN_tabular_predictions.npz\n",
            "  -> Training MLP_baseline\n",
            "    Validation metrics: {'rmse': 579.8917790682481, 'mae': 302.90845665564905, 'smape': 0.3597705734281229, 'r2': -0.07045679950988615, 'mase': 1.103886274127626}\n",
            "    Test metrics       : {'rmse': 561.6690676459639, 'mae': 457.0367287625088, 'smape': 0.6739175820541188, 'r2': -1.6022640929719887, 'mase': 1.6062635141054857}\n",
            "    Saved predictions to /content/colab_results/EAF_VALO2_forecast_MLP_baseline_predictions.npz\n",
            "================================================================================\n",
            "Dataset: EAF_chemistry (regression, tabular)\n",
            "  -> Training ResPSANN_tabular\n",
            "    Validation metrics: {'rmse': 0.022501056567335104, 'mae': 0.015098357684682817, 'smape': 0.31051763341482824, 'r2': -0.2616068121908149, 'mase': 1.256232447361234}\n",
            "    Test metrics       : {'rmse': 0.03152734172362775, 'mae': 0.015707053572479243, 'smape': 0.38997826041196965, 'r2': -0.8227013110807723, 'mase': 1.527257171278344}\n",
            "    Saved predictions to /content/colab_results/EAF_chemistry_ResPSANN_tabular_predictions.npz\n",
            "  -> Training MLP_baseline\n",
            "    Validation metrics: {'rmse': 0.022289120392053673, 'mae': 0.01439963365308191, 'smape': 0.27796794723891255, 'r2': -0.11629004077898093, 'mase': 1.1333220705525324}\n",
            "    Test metrics       : {'rmse': 0.031043567308819145, 'mae': 0.014603624546336466, 'smape': 0.31105435791658326, 'r2': -0.39266943660784576, 'mase': 1.2851904145120157}\n",
            "    Saved predictions to /content/colab_results/EAF_chemistry_MLP_baseline_predictions.npz\n",
            "================================================================================\n",
            "Dataset: Beijing_PM25_24h_ctx_6h_horizon (regression, sequence)\n",
            "  -> Training ResPSANN_conv_spine\n",
            "[INFO] Time budget reached for ResPSANN_conv_spine; stopping at epoch 11.\n",
            "    Validation metrics: {'rmse': 47.242326951360056, 'mae': 30.470228588328183, 'smape': 0.4630944650777761, 'r2': 0.6985767517969662, 'mase': 2.6949983594143703}\n",
            "    Test metrics       : {'rmse': 46.054452601194065, 'mae': 29.08973394005703, 'smape': 0.49913399256616786, 'r2': 0.5803918973693383, 'mase': 3.2245387783275636}\n",
            "    Saved predictions to /content/colab_results/Beijing_PM25_24h_ctx_6h_horizon_ResPSANN_conv_spine_predictions.npz\n",
            "  -> Training ResPSANN_attention_spine\n",
            "[INFO] Time budget reached for ResPSANN_attention_spine; stopping at epoch 10.\n",
            "    Validation metrics: {'rmse': 63.25929800931421, 'mae': 42.959044215126, 'smape': 0.6057503338110137, 'r2': 0.4595407930614597, 'mase': 3.7995958365117795}\n",
            "    Test metrics       : {'rmse': 53.28866899542433, 'mae': 36.41841336987156, 'smape': 0.6110309939898508, 'r2': 0.43821474404994243, 'mase': 4.036908223303043}\n",
            "    Saved predictions to /content/colab_results/Beijing_PM25_24h_ctx_6h_horizon_ResPSANN_attention_spine_predictions.npz\n",
            "  -> Training LSTM_baseline\n",
            "[INFO] Time budget reached for LSTM_baseline; stopping at epoch 19.\n",
            "    Validation metrics: {'rmse': 58.365052333436225, 'mae': 38.35049711254751, 'smape': 0.5515009574840495, 'r2': 0.5399342093320708, 'mase': 3.391983965641521}\n",
            "    Test metrics       : {'rmse': 47.46308712963247, 'mae': 31.59719163584401, 'smape': 0.5622945265897057, 'r2': 0.5543308662926927, 'mase': 3.502485444726849}\n",
            "    Saved predictions to /content/colab_results/Beijing_PM25_24h_ctx_6h_horizon_LSTM_baseline_predictions.npz\n",
            "  -> Training TCN_baseline\n",
            "    Validation metrics: {'rmse': 35.9429551031605, 'mae': 23.296510591744344, 'smape': 0.4075875993750975, 'r2': 0.8255215704095122, 'mase': 2.0605049825218686}\n",
            "    Test metrics       : {'rmse': 43.87832783127398, 'mae': 28.723817698004943, 'smape': 0.5128409826618476, 'r2': 0.6191089860876097, 'mase': 3.183977695350724}\n",
            "    Saved predictions to /content/colab_results/Beijing_PM25_24h_ctx_6h_horizon_TCN_baseline_predictions.npz\n",
            "================================================================================\n",
            "Dataset: Jena_tdegc_72ctx_36h (regression, sequence)\n",
            "  -> Training ResPSANN_conv_spine\n",
            "[INFO] Time budget reached for ResPSANN_conv_spine; stopping at epoch 12.\n",
            "    Validation metrics: {'rmse': 2.4373232583700415, 'mae': 1.886096081395401, 'smape': 0.35944113630445096, 'r2': 0.9022805767987234, 'mase': 11.088452324253081}\n",
            "    Test metrics       : {'rmse': 2.2745495509987443, 'mae': 1.7867266208012391, 'smape': 0.3689493360049165, 'r2': 0.923553978173709, 'mase': 11.135801798498838}\n",
            "    Saved predictions to /content/colab_results/Jena_tdegc_72ctx_36h_ResPSANN_conv_spine_predictions.npz\n",
            "  -> Training ResPSANN_attention_spine\n",
            "[INFO] Time budget reached for ResPSANN_attention_spine; stopping at epoch 11.\n",
            "    Validation metrics: {'rmse': 3.1550413488732474, 'mae': 2.405025153706606, 'smape': 0.40832179266251506, 'r2': 0.8362562471064081, 'mase': 14.139262054865837}\n",
            "    Test metrics       : {'rmse': 2.867616772928079, 'mae': 2.19658091966962, 'smape': 0.4055310000737099, 'r2': 0.8784915980237009, 'mase': 13.69022516988975}\n",
            "    Saved predictions to /content/colab_results/Jena_tdegc_72ctx_36h_ResPSANN_attention_spine_predictions.npz\n",
            "  -> Training LSTM_baseline\n",
            "[INFO] Time budget reached for LSTM_baseline; stopping at epoch 7.\n",
            "    Validation metrics: {'rmse': 3.0345853797289526, 'mae': 2.283950090937882, 'smape': 0.39761074142799646, 'r2': 0.8485206784011192, 'mase': 13.427455761215256}\n",
            "    Test metrics       : {'rmse': 2.5522628397450537, 'mae': 1.935643553225115, 'smape': 0.3662919500007251, 'r2': 0.9037468657875373, 'mase': 12.063928924723148}\n",
            "    Saved predictions to /content/colab_results/Jena_tdegc_72ctx_36h_LSTM_baseline_predictions.npz\n",
            "  -> Training TCN_baseline\n",
            "    Validation metrics: {'rmse': 11.071543448123421, 'mae': 2.985945511226758, 'smape': 0.4737817817593049, 'r2': -1.016373432630493, 'mase': 17.55452162307649}\n",
            "    Test metrics       : {'rmse': 3.040843339696817, 'mae': 2.3460737692388576, 'smape': 0.45717414622445207, 'r2': 0.8633680777999166, 'mase': 14.621941708791098}\n",
            "    Saved predictions to /content/colab_results/Jena_tdegc_72ctx_36h_TCN_baseline_predictions.npz\n",
            "================================================================================\n",
            "Dataset: HAR_engineered (classification, tabular)\n",
            "  -> Training ResPSANN_tabular\n",
            "    Validation metrics: {'accuracy': 0.986400725294651, 'f1_macro': 0.9872814968339371, 'nll': 0.08921076912462576, 'ece': 0.04460010356721502}\n",
            "    Test metrics       : {'accuracy': 0.9372242958941296, 'f1_macro': 0.9366644827565508, 'nll': 0.22579217945086094, 'ece': 0.02788264418926569}\n",
            "    Saved predictions to /content/colab_results/HAR_engineered_ResPSANN_tabular_predictions.npz\n",
            "  -> Training MLP_baseline\n",
            "    Validation metrics: {'accuracy': 0.9836808703535811, 'f1_macro': 0.9846939859768565, 'nll': 0.09026792057441327, 'ece': 0.046015766811500554}\n",
            "    Test metrics       : {'accuracy': 0.9426535459789617, 'f1_macro': 0.9420061581803884, 'nll': 0.19701738167210717, 'ece': 0.03097683208491143}\n",
            "    Saved predictions to /content/colab_results/HAR_engineered_MLP_baseline_predictions.npz\n",
            "================================================================================\n",
            "Dataset: HAR_raw_sequence (classification, sequence)\n",
            "  -> Training ResPSANN_conv_spine\n",
            "    Validation metrics: {'accuracy': 0.9637352674524026, 'f1_macro': 0.9665829145728644, 'nll': 0.13870868593229752, 'ece': 0.0385365160562078}\n",
            "    Test metrics       : {'accuracy': 0.9229725144214456, 'f1_macro': 0.9242847911336529, 'nll': 0.21739700569268502, 'ece': 0.04288411275952963}\n",
            "    Saved predictions to /content/colab_results/HAR_raw_sequence_ResPSANN_conv_spine_predictions.npz\n",
            "  -> Training ResPSANN_attention_spine\n",
            "    Validation metrics: {'accuracy': 0.9737080689029919, 'f1_macro': 0.9737043544910727, 'nll': 0.11394827108814645, 'ece': 0.04178398431482258}\n",
            "    Test metrics       : {'accuracy': 0.7757041058703766, 'f1_macro': 0.7723894118070591, 'nll': 0.8034769781871232, 'ece': 0.13179676711539068}\n",
            "    Saved predictions to /content/colab_results/HAR_raw_sequence_ResPSANN_attention_spine_predictions.npz\n",
            "  -> Training LSTM_baseline\n",
            "    Validation metrics: {'accuracy': 0.9592021758839528, 'f1_macro': 0.9618804707222286, 'nll': 0.16399757064953316, 'ece': 0.05075013486234467}\n",
            "    Test metrics       : {'accuracy': 0.9009161859518154, 'f1_macro': 0.9025375065108023, 'nll': 0.33276216622696575, 'ece': 0.020670247980000635}\n",
            "    Saved predictions to /content/colab_results/HAR_raw_sequence_LSTM_baseline_predictions.npz\n",
            "  -> Training TCN_baseline\n",
            "    Validation metrics: {'accuracy': 0.9646418857660924, 'f1_macro': 0.9673892013536106, 'nll': 0.15072972328411952, 'ece': 0.06704734518435039}\n",
            "    Test metrics       : {'accuracy': 0.9233118425517476, 'f1_macro': 0.9256951907229128, 'nll': 0.23398740609978572, 'ece': 0.061461983038119464}\n",
            "    Saved predictions to /content/colab_results/HAR_raw_sequence_TCN_baseline_predictions.npz\n",
            "================================================================================\n",
            "Dataset: Rossmann_sales (regression, tabular)\n",
            "  -> Training ResPSANN_tabular\n",
            "[INFO] Time budget reached for ResPSANN_tabular; stopping at epoch 10.\n",
            "    Validation metrics: {'rmse': 478.44637720302757, 'mae': 337.93176128161923, 'smape': 0.05093985861686564, 'r2': 0.9742563979732718, 'mase': 0.12585753210598372}\n",
            "    Test metrics       : {'rmse': 488.9785735172513, 'mae': 348.9749551123583, 'smape': 0.050791488139079244, 'r2': 0.9748466600599208, 'mase': 0.12555739621201267}\n",
            "    Saved predictions to /content/colab_results/Rossmann_sales_ResPSANN_tabular_predictions.npz\n",
            "  -> Training MLP_baseline\n",
            "    Validation metrics: {'rmse': 559.5486651018768, 'mae': 416.9143799399201, 'smape': 0.06397832971376347, 'r2': 0.9647889885859657, 'mase': 0.15527340419182087}\n",
            "    Test metrics       : {'rmse': 573.8418085779969, 'mae': 417.80086683620345, 'smape': 0.06113288401384641, 'r2': 0.9653582059920035, 'mase': 0.15032021125466102}\n",
            "    Saved predictions to /content/colab_results/Rossmann_sales_MLP_baseline_predictions.npz\n"
          ]
        }
      ],
      "source": [
        "EXPERIMENT_ARTIFACTS: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "for dataset_name, run_flag in RUN_EXPERIMENTS.items():\n",
        "    if not run_flag:\n",
        "        continue\n",
        "    if dataset_name not in DATA_BUNDLES:\n",
        "        print(f\"[WARN] Dataset {dataset_name} not loaded; skipping.\")\n",
        "        continue\n",
        "    bundle = DATA_BUNDLES[dataset_name]\n",
        "    specs = EXPERIMENT_REGISTRY.get(dataset_name, [])\n",
        "    if not specs:\n",
        "        print(f\"[WARN] No model specs registered for {dataset_name}; skipping.\")\n",
        "        continue\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Dataset: {dataset_name} ({bundle.task_type}, {bundle.input_kind})\")\n",
        "    for spec in specs:\n",
        "        print(f\"  -> Training {spec.name}\")\n",
        "        result = train_model_on_bundle(bundle, spec, task_name=dataset_name)\n",
        "        EXPERIMENT_ARTIFACTS.setdefault(dataset_name, {})[spec.name] = result\n",
        "        artifact_path = RESULTS_ROOT / f\"{dataset_name}_{spec.name}_predictions.npz\"\n",
        "        np.savez_compressed(\n",
        "            artifact_path,\n",
        "            train_true=result[\"train_true\"],\n",
        "            train_pred=result[\"train_pred\"],\n",
        "            val_true=result[\"val_true\"],\n",
        "            val_pred=result[\"val_pred\"],\n",
        "            test_true=result[\"test_true\"],\n",
        "            test_pred=result[\"test_pred\"],\n",
        "        )\n",
        "        print(f\"    Validation metrics: {result['val_metrics']}\")\n",
        "        print(f\"    Test metrics       : {result['test_metrics']}\")\n",
        "        print(f\"    Saved predictions to {artifact_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "47f24804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "47f24804",
        "outputId": "046dd8fe-a24c-4b4d-cdaf-30b9b62aa130"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              dataset               task             model     group  split  \\\n",
              "0   EAF_TEMP_forecast  EAF_TEMP_forecast  ResPSANN_tabular     psann  train   \n",
              "1   EAF_TEMP_forecast  EAF_TEMP_forecast  ResPSANN_tabular     psann    val   \n",
              "2   EAF_TEMP_forecast  EAF_TEMP_forecast  ResPSANN_tabular     psann   test   \n",
              "3   EAF_TEMP_forecast  EAF_TEMP_forecast      MLP_baseline  baseline  train   \n",
              "4   EAF_TEMP_forecast  EAF_TEMP_forecast      MLP_baseline  baseline    val   \n",
              "..                ...                ...               ...       ...    ...   \n",
              "85     Rossmann_sales     Rossmann_sales  ResPSANN_tabular     psann    val   \n",
              "86     Rossmann_sales     Rossmann_sales  ResPSANN_tabular     psann   test   \n",
              "87     Rossmann_sales     Rossmann_sales      MLP_baseline  baseline  train   \n",
              "88     Rossmann_sales     Rossmann_sales      MLP_baseline  baseline    val   \n",
              "89     Rossmann_sales     Rossmann_sales      MLP_baseline  baseline   test   \n",
              "\n",
              "     params  train_wall_seconds                                   notes  \\\n",
              "0   1076489            7.958187                     Residual PSANN core   \n",
              "1   1076489            7.958187                     Residual PSANN core   \n",
              "2   1076489            7.958187                     Residual PSANN core   \n",
              "3    206593            6.217907  ReLU MLP with similar parameter budget   \n",
              "4    206593            6.217907  ReLU MLP with similar parameter budget   \n",
              "..      ...                 ...                                     ...   \n",
              "85  1076233          321.576789                     Residual PSANN core   \n",
              "86  1076233          321.576789                     Residual PSANN core   \n",
              "87   206337           75.815578  ReLU MLP with similar parameter budget   \n",
              "88   206337           75.815578  ReLU MLP with similar parameter budget   \n",
              "89   206337           75.815578  ReLU MLP with similar parameter budget   \n",
              "\n",
              "          rmse         mae     smape        r2      mase  accuracy  f1_macro  \\\n",
              "0    21.872530   14.257730  0.008698  0.289276  0.541215       NaN       NaN   \n",
              "1    20.517528   13.436105  0.008185  0.251715  0.939035       NaN       NaN   \n",
              "2    23.891865   15.075829  0.009201  0.050648  0.706053       NaN       NaN   \n",
              "3    21.370292   14.206419  0.008670  0.321541  0.538999       NaN       NaN   \n",
              "4    20.418592   13.498109  0.008228  0.258914  0.943369       NaN       NaN   \n",
              "..         ...         ...       ...       ...       ...       ...       ...   \n",
              "85  478.446377  337.931761  0.050940  0.974256  0.125858       NaN       NaN   \n",
              "86  488.978574  348.974955  0.050791  0.974847  0.125557       NaN       NaN   \n",
              "87  548.341991  389.962456  0.058990  0.969114  0.119690       NaN       NaN   \n",
              "88  559.548665  416.914380  0.063978  0.964789  0.155273       NaN       NaN   \n",
              "89  573.841809  417.800867  0.061133  0.965358  0.150320       NaN       NaN   \n",
              "\n",
              "    nll  ece  \n",
              "0   NaN  NaN  \n",
              "1   NaN  NaN  \n",
              "2   NaN  NaN  \n",
              "3   NaN  NaN  \n",
              "4   NaN  NaN  \n",
              "..  ...  ...  \n",
              "85  NaN  NaN  \n",
              "86  NaN  NaN  \n",
              "87  NaN  NaN  \n",
              "88  NaN  NaN  \n",
              "89  NaN  NaN  \n",
              "\n",
              "[90 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1287a421-176f-4140-ada1-cd3c0b884a9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>task</th>\n",
              "      <th>model</th>\n",
              "      <th>group</th>\n",
              "      <th>split</th>\n",
              "      <th>params</th>\n",
              "      <th>train_wall_seconds</th>\n",
              "      <th>notes</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>smape</th>\n",
              "      <th>r2</th>\n",
              "      <th>mase</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>nll</th>\n",
              "      <th>ece</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>ResPSANN_tabular</td>\n",
              "      <td>psann</td>\n",
              "      <td>train</td>\n",
              "      <td>1076489</td>\n",
              "      <td>7.958187</td>\n",
              "      <td>Residual PSANN core</td>\n",
              "      <td>21.872530</td>\n",
              "      <td>14.257730</td>\n",
              "      <td>0.008698</td>\n",
              "      <td>0.289276</td>\n",
              "      <td>0.541215</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>ResPSANN_tabular</td>\n",
              "      <td>psann</td>\n",
              "      <td>val</td>\n",
              "      <td>1076489</td>\n",
              "      <td>7.958187</td>\n",
              "      <td>Residual PSANN core</td>\n",
              "      <td>20.517528</td>\n",
              "      <td>13.436105</td>\n",
              "      <td>0.008185</td>\n",
              "      <td>0.251715</td>\n",
              "      <td>0.939035</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>ResPSANN_tabular</td>\n",
              "      <td>psann</td>\n",
              "      <td>test</td>\n",
              "      <td>1076489</td>\n",
              "      <td>7.958187</td>\n",
              "      <td>Residual PSANN core</td>\n",
              "      <td>23.891865</td>\n",
              "      <td>15.075829</td>\n",
              "      <td>0.009201</td>\n",
              "      <td>0.050648</td>\n",
              "      <td>0.706053</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>MLP_baseline</td>\n",
              "      <td>baseline</td>\n",
              "      <td>train</td>\n",
              "      <td>206593</td>\n",
              "      <td>6.217907</td>\n",
              "      <td>ReLU MLP with similar parameter budget</td>\n",
              "      <td>21.370292</td>\n",
              "      <td>14.206419</td>\n",
              "      <td>0.008670</td>\n",
              "      <td>0.321541</td>\n",
              "      <td>0.538999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>EAF_TEMP_forecast</td>\n",
              "      <td>MLP_baseline</td>\n",
              "      <td>baseline</td>\n",
              "      <td>val</td>\n",
              "      <td>206593</td>\n",
              "      <td>6.217907</td>\n",
              "      <td>ReLU MLP with similar parameter budget</td>\n",
              "      <td>20.418592</td>\n",
              "      <td>13.498109</td>\n",
              "      <td>0.008228</td>\n",
              "      <td>0.258914</td>\n",
              "      <td>0.943369</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>ResPSANN_tabular</td>\n",
              "      <td>psann</td>\n",
              "      <td>val</td>\n",
              "      <td>1076233</td>\n",
              "      <td>321.576789</td>\n",
              "      <td>Residual PSANN core</td>\n",
              "      <td>478.446377</td>\n",
              "      <td>337.931761</td>\n",
              "      <td>0.050940</td>\n",
              "      <td>0.974256</td>\n",
              "      <td>0.125858</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>ResPSANN_tabular</td>\n",
              "      <td>psann</td>\n",
              "      <td>test</td>\n",
              "      <td>1076233</td>\n",
              "      <td>321.576789</td>\n",
              "      <td>Residual PSANN core</td>\n",
              "      <td>488.978574</td>\n",
              "      <td>348.974955</td>\n",
              "      <td>0.050791</td>\n",
              "      <td>0.974847</td>\n",
              "      <td>0.125557</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>MLP_baseline</td>\n",
              "      <td>baseline</td>\n",
              "      <td>train</td>\n",
              "      <td>206337</td>\n",
              "      <td>75.815578</td>\n",
              "      <td>ReLU MLP with similar parameter budget</td>\n",
              "      <td>548.341991</td>\n",
              "      <td>389.962456</td>\n",
              "      <td>0.058990</td>\n",
              "      <td>0.969114</td>\n",
              "      <td>0.119690</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>MLP_baseline</td>\n",
              "      <td>baseline</td>\n",
              "      <td>val</td>\n",
              "      <td>206337</td>\n",
              "      <td>75.815578</td>\n",
              "      <td>ReLU MLP with similar parameter budget</td>\n",
              "      <td>559.548665</td>\n",
              "      <td>416.914380</td>\n",
              "      <td>0.063978</td>\n",
              "      <td>0.964789</td>\n",
              "      <td>0.155273</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>Rossmann_sales</td>\n",
              "      <td>MLP_baseline</td>\n",
              "      <td>baseline</td>\n",
              "      <td>test</td>\n",
              "      <td>206337</td>\n",
              "      <td>75.815578</td>\n",
              "      <td>ReLU MLP with similar parameter budget</td>\n",
              "      <td>573.841809</td>\n",
              "      <td>417.800867</td>\n",
              "      <td>0.061133</td>\n",
              "      <td>0.965358</td>\n",
              "      <td>0.150320</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows × 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1287a421-176f-4140-ada1-cd3c0b884a9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1287a421-176f-4140-ada1-cd3c0b884a9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1287a421-176f-4140-ada1-cd3c0b884a9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6e698636-99aa-4511-b5fa-0a4b0f3f9bc9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e698636-99aa-4511-b5fa-0a4b0f3f9bc9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6e698636-99aa-4511-b5fa-0a4b0f3f9bc9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4b7092e3-f2f8-431f-94ab-5a465ddd662c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4b7092e3-f2f8-431f-94ab-5a465ddd662c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 90,\n  \"fields\": [\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"EAF_VALO2_forecast\",\n          \"Jena_tdegc_72ctx_36h\",\n          \"EAF_TEMP_forecast\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"EAF_VALO2_forecast\",\n          \"Jena_tdegc_72ctx_36h\",\n          \"EAF_TEMP_forecast\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"ResPSANN_tabular\",\n          \"MLP_baseline\",\n          \"TCN_baseline\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"baseline\",\n          \"psann\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"train\",\n          \"val\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 376951,\n        \"min\": 157446,\n        \"max\": 1277198,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          1076489,\n          456759\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_wall_seconds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 137.4940787698296,\n        \"min\": 4.141093297000225,\n        \"max\": 321.5767890269999,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          19.740484281000136,\n          309.1087250769997\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"notes\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Residual PSANN core\",\n          \"ReLU MLP with similar parameter budget\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 318.54664356989474,\n        \"min\": 0.014498550723875403,\n        \"max\": 1127.084230480859,\n        \"num_unique_values\": 72,\n        \"samples\": [\n          20.4185918815282,\n          2.5522628397450537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 179.22561083962543,\n        \"min\": 0.009224878428554786,\n        \"max\": 475.67126580207577,\n        \"num_unique_values\": 72,\n        \"samples\": [\n          13.498109190614073,\n          1.935643553225115\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smape\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2112573482750618,\n        \"min\": 0.007600588039037466,\n        \"max\": 0.6781309161976523,\n        \"num_unique_values\": 72,\n        \"samples\": [\n          0.008228057346459861,\n          0.3662919500007251\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"r2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6605898280949707,\n        \"min\": -1.6022640929719887,\n        \"max\": 0.9880367636040194,\n        \"num_unique_values\": 72,\n        \"samples\": [\n          0.25891420742472326,\n          0.9037468657875373\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mase\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.100976607360784,\n        \"min\": 0.07542295577304148,\n        \"max\": 17.55452162307649,\n        \"num_unique_values\": 72,\n        \"samples\": [\n          0.9433688323881784,\n          12.063928924723148\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04984311808487782,\n        \"min\": 0.7757041058703766,\n        \"max\": 0.9887982077132341,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.9878380540886542,\n          0.986400725294651\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.050797271193308256,\n        \"min\": 0.7723894118070591,\n        \"max\": 0.9896744088701831,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.9886374210186951,\n          0.9872814968339371\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nll\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.167428180195621,\n        \"min\": 0.06595445482510397,\n        \"max\": 0.8034769781871232,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.07516104613207182,\n          0.08921076912462576\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ece\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023514328667513122,\n        \"min\": 0.020670247980000635,\n        \"max\": 0.13179676711539068,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.042770051030010796,\n          0.04460010356721502\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to /content/colab_results/experiment_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "results_df = RESULT_LOGGER.to_frame()\n",
        "results_path = RESULTS_ROOT / \"experiment_metrics.csv\"\n",
        "if not results_df.empty:\n",
        "    results_df.to_csv(results_path, index=False)\n",
        "    display(results_df)\n",
        "    print(f\"Metrics saved to {results_path}\")\n",
        "else:\n",
        "    print(\"No experiments were run yet. Toggle RUN_EXPERIMENTS before executing the training cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "37cb9471",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "37cb9471",
        "outputId": "47401f1a-affe-42ce-9f57-0b3c1c9bf91b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        group  mean_delta  std_delta  baseline\n",
              "0   temp_lags    0.031854   0.007176  0.962133\n",
              "4    calendar    0.018385   0.004915  0.962133\n",
              "2    gas_flow    0.006855   0.003384  0.962133\n",
              "3         inj   -0.006991   0.009792  0.962133\n",
              "1  valo2_lags   -0.007853   0.002448  0.962133"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44e3f1d4-f0db-47f9-ae09-689d0b3a332a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>mean_delta</th>\n",
              "      <th>std_delta</th>\n",
              "      <th>baseline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>temp_lags</td>\n",
              "      <td>0.031854</td>\n",
              "      <td>0.007176</td>\n",
              "      <td>0.962133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>calendar</td>\n",
              "      <td>0.018385</td>\n",
              "      <td>0.004915</td>\n",
              "      <td>0.962133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gas_flow</td>\n",
              "      <td>0.006855</td>\n",
              "      <td>0.003384</td>\n",
              "      <td>0.962133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>inj</td>\n",
              "      <td>-0.006991</td>\n",
              "      <td>0.009792</td>\n",
              "      <td>0.962133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>valo2_lags</td>\n",
              "      <td>-0.007853</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>0.962133</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44e3f1d4-f0db-47f9-ae09-689d0b3a332a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-44e3f1d4-f0db-47f9-ae09-689d0b3a332a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-44e3f1d4-f0db-47f9-ae09-689d0b3a332a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-50929e8b-eaff-43d1-9b30-d47db29b4a7d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50929e8b-eaff-43d1-9b30-d47db29b4a7d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-50929e8b-eaff-43d1-9b30-d47db29b4a7d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"Train the target model first; EXPERIMENT_ARTIFACTS does not contain it yet\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"group\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"calendar\",\n          \"valo2_lags\",\n          \"gas_flow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_delta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016979438122031437,\n        \"min\": -0.007852936297598933,\n        \"max\": 0.03185433129188457,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.01838459000493662,\n          -0.007852936297598933,\n          0.006854753289968429\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_delta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0029730829640524647,\n        \"min\": 0.0024483536563007276,\n        \"max\": 0.009792026615368637,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.004914955603668698,\n          0.0024483536563007276,\n          0.0033835713094108218\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"baseline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.9621327059837288,\n        \"max\": 0.9621327059837288,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9621327059837288\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "TARGET_DATASET = \"EAF_TEMP_forecast\"\n",
        "TARGET_MODEL = \"ResPSANN_tabular\"\n",
        "\n",
        "if TARGET_DATASET in EXPERIMENT_ARTIFACTS and TARGET_MODEL in EXPERIMENT_ARTIFACTS[TARGET_DATASET]:\n",
        "    bundle = DATA_BUNDLES[TARGET_DATASET]\n",
        "    spec = next(spec for spec in EXPERIMENT_REGISTRY[TARGET_DATASET] if spec.name == TARGET_MODEL)\n",
        "    trained_model = EXPERIMENT_ARTIFACTS[TARGET_DATASET][TARGET_MODEL][\"model\"]\n",
        "\n",
        "    prefix_groups = {\n",
        "        \"temp_lags\": [i for i, name in enumerate(bundle.feature_names) if name.startswith(\"TEMP_lag\")],\n",
        "        \"valo2_lags\": [i for i, name in enumerate(bundle.feature_names) if name.startswith(\"VALO2_lag\")],\n",
        "        \"gas_flow\": [i for i, name in enumerate(bundle.feature_names) if \"gas\" in name.lower()],\n",
        "        \"inj\": [i for i, name in enumerate(bundle.feature_names) if \"inj\" in name.lower()],\n",
        "        \"calendar\": [i for i, name in enumerate(bundle.feature_names) if \"DATETIME\" in name],\n",
        "    }\n",
        "\n",
        "    perm_df = permutation_importance(\n",
        "        trained_model,\n",
        "        bundle,\n",
        "        spec,\n",
        "        feature_groups=prefix_groups,\n",
        "        split=\"test\",\n",
        "        n_repeats=5,\n",
        "    )\n",
        "    display(perm_df.sort_values(\"mean_delta\", ascending=False))\n",
        "else:\n",
        "    print(\"Train the target model first; EXPERIMENT_ARTIFACTS does not contain it yet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "c1c6a52d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1c6a52d",
        "outputId": "a116f8ec-32ce-4afa-f725-7d9e03cb1dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Participation ratio: 1.4756\n"
          ]
        }
      ],
      "source": [
        "TARGET_DATASET = \"Jena_tdegc_72ctx_36h\"\n",
        "TARGET_MODEL = \"ResPSANN_conv_spine\"\n",
        "\n",
        "if TARGET_DATASET in EXPERIMENT_ARTIFACTS and TARGET_MODEL in EXPERIMENT_ARTIFACTS[TARGET_DATASET]:\n",
        "    bundle = DATA_BUNDLES[TARGET_DATASET]\n",
        "    spec = next(spec for spec in EXPERIMENT_REGISTRY[TARGET_DATASET] if spec.name == TARGET_MODEL)\n",
        "    trained_model = EXPERIMENT_ARTIFACTS[TARGET_DATASET][TARGET_MODEL][\"model\"].to(DEVICE)\n",
        "    sample_loader = build_dataloader(\n",
        "        bundle.val[\"X\"],\n",
        "        bundle.val[\"y\"],\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        task_type=spec.task_type,\n",
        "    )\n",
        "    sample_batch = next(iter(sample_loader))[0][:64]\n",
        "    singular_values = compute_jacobian_singular_values(trained_model, sample_batch, max_samples=64)\n",
        "    pr = participation_ratio(singular_values)\n",
        "    print(f\"Participation ratio: {pr:.4f}\")\n",
        "    trained_model.to(\"cpu\")\n",
        "else:\n",
        "    print(\"Train the target model first to access EXPERIMENT_ARTIFACTS.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "9afd5b0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9afd5b0b",
        "outputId": "540e61fd-1d8d-4e8f-ceee-019a9668c0d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   level        rmse        mae     smape        r2       mase\n",
              "0    0.0   46.014864  29.070318  0.499069  0.581113   3.222387\n",
              "1    0.1  127.797654  95.996053  1.212224 -2.231074  10.640970\n",
              "2    0.2  110.871029  84.176608  1.199640 -1.431851   9.330809\n",
              "3    0.3   98.701487  73.510134  1.206441 -0.927294   8.148451\n",
              "4    0.4   94.484721  68.612599  1.252539 -0.766135   7.605569"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18afc318-dd98-41e8-a934-b2c322698918\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>smape</th>\n",
              "      <th>r2</th>\n",
              "      <th>mase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>46.014864</td>\n",
              "      <td>29.070318</td>\n",
              "      <td>0.499069</td>\n",
              "      <td>0.581113</td>\n",
              "      <td>3.222387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1</td>\n",
              "      <td>127.797654</td>\n",
              "      <td>95.996053</td>\n",
              "      <td>1.212224</td>\n",
              "      <td>-2.231074</td>\n",
              "      <td>10.640970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2</td>\n",
              "      <td>110.871029</td>\n",
              "      <td>84.176608</td>\n",
              "      <td>1.199640</td>\n",
              "      <td>-1.431851</td>\n",
              "      <td>9.330809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.3</td>\n",
              "      <td>98.701487</td>\n",
              "      <td>73.510134</td>\n",
              "      <td>1.206441</td>\n",
              "      <td>-0.927294</td>\n",
              "      <td>8.148451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4</td>\n",
              "      <td>94.484721</td>\n",
              "      <td>68.612599</td>\n",
              "      <td>1.252539</td>\n",
              "      <td>-0.766135</td>\n",
              "      <td>7.605569</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18afc318-dd98-41e8-a934-b2c322698918')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18afc318-dd98-41e8-a934-b2c322698918 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18afc318-dd98-41e8-a934-b2c322698918');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-36fd093e-1d9a-4ef7-8dd6-55724f055158\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36fd093e-1d9a-4ef7-8dd6-55724f055158')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-36fd093e-1d9a-4ef7-8dd6-55724f055158 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_19e9065e-5a69-4c06-9f78-4040639330ab\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('robustness_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_19e9065e-5a69-4c06-9f78-4040639330ab button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('robustness_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "robustness_df",
              "summary": "{\n  \"name\": \"robustness_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15811388300841897,\n        \"min\": 0.0,\n        \"max\": 0.4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1,\n          0.4,\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.575474718560812,\n        \"min\": 46.01486439628873,\n        \"max\": 127.79765443479715,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          127.79765443479715,\n          94.48472058668499,\n          110.87102858677721\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25.327271595355068,\n        \"min\": 29.070318274944725,\n        \"max\": 95.99605338560534,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          95.99605338560534,\n          68.61259907706207,\n          84.1766082951975\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smape\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3220457251905292,\n        \"min\": 0.4990691613592151,\n        \"max\": 1.2525394925530584,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.2122244069359396,\n          1.2525394925530584,\n          1.1996402064392508\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"r2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0309984934338199,\n        \"min\": -2.2310735453932526,\n        \"max\": 0.5811129745965989,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -2.2310735453932526,\n          -0.7661345380431882,\n          -1.4318511930468296\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mase\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8074773587391744,\n        \"min\": 3.222386590714206,\n        \"max\": 10.6409703624701,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          10.6409703624701,\n          7.605569265835468,\n          9.330808533184609\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "TARGET_DATASET = \"Beijing_PM25_24h_ctx_6h_horizon\"\n",
        "TARGET_MODEL = \"ResPSANN_conv_spine\"\n",
        "\n",
        "if TARGET_DATASET in EXPERIMENT_ARTIFACTS and TARGET_MODEL in EXPERIMENT_ARTIFACTS[TARGET_DATASET]:\n",
        "    bundle = DATA_BUNDLES[TARGET_DATASET]\n",
        "    spec = next(spec for spec in EXPERIMENT_REGISTRY[TARGET_DATASET] if spec.name == TARGET_MODEL)\n",
        "    trained_model = EXPERIMENT_ARTIFACTS[TARGET_DATASET][TARGET_MODEL][\"model\"]\n",
        "\n",
        "    def missingness_fn(X: np.ndarray, level: float) -> np.ndarray:\n",
        "        rng = np.random.default_rng(GLOBAL_CONFIG[\"seed\"])\n",
        "        mask = rng.random(size=X.shape) < level\n",
        "        X_corrupted = X.copy()\n",
        "        X_corrupted[mask] = 0.0\n",
        "        return X_corrupted\n",
        "\n",
        "    robustness_df = evaluate_robustness(\n",
        "        trained_model,\n",
        "        bundle,\n",
        "        spec,\n",
        "        corruption_fn=missingness_fn,\n",
        "        split=\"test\",\n",
        "        levels=[0.0, 0.1, 0.2, 0.3, 0.4],\n",
        "    )\n",
        "    display(robustness_df)\n",
        "else:\n",
        "    print(\"Train the target model before running robustness experiments.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "3a58771f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a58771f",
        "outputId": "aa9c3544-31e8-4ffb-cdaa-2efe8a720034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv spine accuracy: 0.9230\n",
            "Attention spine accuracy: 0.7757\n"
          ]
        }
      ],
      "source": [
        "TARGET_DATASET = \"HAR_raw_sequence\"\n",
        "if TARGET_DATASET in EXPERIMENT_ARTIFACTS:\n",
        "    results = EXPERIMENT_ARTIFACTS[TARGET_DATASET]\n",
        "    if \"ResPSANN_conv_spine\" in results and \"ResPSANN_attention_spine\" in results:\n",
        "        conv_acc = results[\"ResPSANN_conv_spine\"][\"test_metrics\"][\"accuracy\"]\n",
        "        attn_acc = results[\"ResPSANN_attention_spine\"][\"test_metrics\"][\"accuracy\"]\n",
        "        print(f\"Conv spine accuracy: {conv_acc:.4f}\")\n",
        "        print(f\"Attention spine accuracy: {attn_acc:.4f}\")\n",
        "    else:\n",
        "        print(\"Run both PSANN spine variants on HAR_raw_sequence first.\")\n",
        "else:\n",
        "    print(\"Train HAR_raw_sequence models before evaluating H5.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9c00e71",
      "metadata": {
        "id": "a9c00e71"
      },
      "source": [
        "## Notebook Complete\n",
        "All core experiment scaffolding is now in place. Toggle the runs you need, execute the training cell per hypothesis, and archive outputs from `colab_results/` before ending your Colab session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "x6qgdD_WPmvf",
      "metadata": {
        "id": "x6qgdD_WPmvf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "def zip_folder(folder_path: str | Path, output_path: str | Path | None = None, *, include_hidden: bool = True) -> Path:\n",
        "    \"\"\"\n",
        "    Compresses an entire folder (recursively) into a .zip archive.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    folder_path : str or Path\n",
        "        Path to the folder to zip.\n",
        "    output_path : str or Path or None, optional\n",
        "        Output .zip file path. Defaults to \"<folder_name>.zip\" in the same directory.\n",
        "    include_hidden : bool, optional\n",
        "        Whether to include hidden files (those starting with '.').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path\n",
        "        Path to the created .zip file.\n",
        "    \"\"\"\n",
        "    folder_path = Path(folder_path).resolve()\n",
        "    if not folder_path.is_dir():\n",
        "        raise ValueError(f\"{folder_path} is not a valid directory\")\n",
        "\n",
        "    if output_path is None:\n",
        "        output_path = folder_path.with_suffix(\".zip\")\n",
        "    else:\n",
        "        output_path = Path(output_path).resolve()\n",
        "\n",
        "    with zipfile.ZipFile(output_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            # skip hidden dirs/files if requested\n",
        "            if not include_hidden:\n",
        "                dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
        "                files = [f for f in files if not f.startswith(\".\")]\n",
        "\n",
        "            for file in files:\n",
        "                abs_path = Path(root) / file\n",
        "                # relative path inside the zip\n",
        "                rel_path = abs_path.relative_to(folder_path)\n",
        "                zf.write(abs_path, arcname=rel_path)\n",
        "\n",
        "    print(f\"Zipped {folder_path} → {output_path}\")\n",
        "    return output_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_folder(folder_path = '/content/colab_results', output_path = '/content/colab_results.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeG6oGYtM1rd",
        "outputId": "394f511d-7777-4613-8abc-09ecfe6e8ab8"
      },
      "id": "eeG6oGYtM1rd",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipped /content/colab_results → /content/colab_results.zip\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/colab_results.zip')"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKFbc--ONBQa"
      },
      "id": "kKFbc--ONBQa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}