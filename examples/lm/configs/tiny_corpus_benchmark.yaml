model:
  base: waveresnet
  d_model: 256      # was 512
  n_layers: 4       # was 8
  n_heads: 4        # was 8
  d_mlp: 1024       # was 2048
  vocab_size: null  # infer from data prep
  positional_encoding: rope
  sine_params:
    amp_init: 1.0
    freq_init: 1.0
    damp_init: 0.01
    trainable: true

data:
  sources:
    - path: datasets/lm/tiny_books.txt    # create this ~50MB shard before running
  tokenizer: auto
  tokenizer_model_path: null
  max_length: 512      # was 1024
  pack_sequences: true
  val_split: 0.02
  seed: 1337

train:
  epochs: 1            # was 2
  batch_tokens: 32768  # was 131072
  lr: 0.0002
  warmup_steps: 2000
  weight_decay: 0.01
  amp: bf16
  grad_clip: 1.0
  grad_accum_steps: 1
  ddp: "off"           # force single GPU to reduce memory
  grad_checkpoint: true
  checkpoint_dir: runs/lm/bmrk_tiny
  log_interval_steps: 50
  save_interval_steps: 500
