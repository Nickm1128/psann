bench:
  run_name: base_shootout
  out_dir: reports/benchmarks
  bases: [respsann, sgrpsann, waveresnet]
  seeds: [1337]
  reuse_tokenizer: true
  with_lm_eval: false
  lm_eval_tasks: [lambada_openai, hellaswag]
  lm_eval_limit: 256
  lm_eval_num_fewshot: 0

data:
  dataset: iohadrubin/wikitext-103-raw-v1
  name: null
  revision: null
  train_split: train
  val_split: validation
  text_key: text
  streaming: true
  shuffle: true
  shuffle_buffer: 10000
  ascii_only: false
  languages: []
  lang_threshold: 0.8
  max_length: 512

tokenizer:
  backend: tokenizers
  vocab_size: 16384
  min_frequency: 2
  sample_limit: 20000
  save_dir: runs/tokenizers/base_compare_quick
  model_path: null
  special_tokens_map_path: null

train:
  d_model: 256
  n_layers: 4
  n_heads: 4
  d_mlp: 1024
  dropout: 0.0
  positional_encoding: rope
  attn_impl: auto
  batch_tokens: 32768
  grad_accum_steps: 1
  lr: 0.0002
  weight_decay: 0.01
  warmup_steps: 200
  amp: bf16
  grad_checkpoint: false
  ddp: off
  max_steps: 300
  log_interval_steps: 50
  save_interval_steps: 500
  sine_params:
    amp_init: 1.0
    freq_init: 1.0
    damp_init: 0.01
    trainable: true

eval:
  batch_tokens: 32768
  max_tokens: 200000
  max_batches: 0
