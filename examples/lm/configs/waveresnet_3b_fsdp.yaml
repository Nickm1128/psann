model:
  base: waveresnet
  d_model: 3072
  n_layers: 30
  n_heads: 24
  d_mlp: 12288
  positional_encoding: rope
  # vocab_size: 50257  # Optional override (defaults to tokenizer vocab)

data:
  # Note: The YAML trainer CLI loads full files into memory and is not intended
  # for multi-billion token streaming. Prefer scripts/train_psann_lm.py for 3B.
  tokenizer: tokenizers
  tokenizer_model_path: /path/to/tokenizer_final/tokenizer.json
  tokenizer_special_map_path: /path/to/tokenizer_final/special_tokens_map.json
  max_length: 2048
  pack_sequences: true
  val_split: 0.0
  seed: 1337
  sources:
    - datasets/lm/tiny_books.txt  # placeholder; replace with your local corpus

train:
  epochs: 1
  batch_tokens: 65536
  lr: 0.0002
  amp: bf16
  ddp: auto
  checkpoint_dir: runs/lm/3b_yaml

