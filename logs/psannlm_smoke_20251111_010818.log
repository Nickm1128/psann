[2025-11-11T01:08:39+00:00] Starting smoke run
W1111 01:08:45.310000 4672 torch/distributed/run.py:803] 
W1111 01:08:45.310000 4672 torch/distributed/run.py:803] *****************************************
W1111 01:08:45.310000 4672 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1111 01:08:45.310000 4672 torch/distributed/run.py:803] *****************************************








[tokenizer] Trained tokenizer saved to runs/tokenizer_smoke/tokenizer.json
[tokenizer] Trained tokenizer saved to runs/tokenizer_smoke/tokenizer.json

[tokenizer] Trained tokenizer saved to runs/tokenizer_smoke/tokenizer.json
[W1111 01:09:25.828973150 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1111 01:09:25.975311309 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1111 01:09:25.138061670 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1111 01:09:34.750956681 Utils.hpp:112] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[trainer] Gradient checkpointing: enabled via model.enable_gradient_checkpointing()
[rank2]: Traceback (most recent call last):
[rank2]:   File "/workspace/psann/scripts/train_psann_lm.py", line 483, in <module>
[rank2]:     raise SystemExit(main())
[rank2]:                      ^^^^^^
[rank2]:   File "/workspace/psann/scripts/train_psann_lm.py", line 459, in main
[rank2]:     trainer.train(model, dataset, max_length=int(args.max_length), val_dataset=None)
[rank2]:   File "/workspace/psann/src/psann/lm/train/trainer.py", line 206, in train
[rank2]:     wrapped = DDP(
[rank2]:               ^^^^
[rank2]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 858, in __init__
[rank2]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank2]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/utils.py", line 281, in _verify_param_shape_across_processes
[rank2]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: RuntimeError: [2]: params[0] in this process with sizes [47378, 512] appears not to match sizes of the same param in process 0.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/workspace/psann/scripts/train_psann_lm.py", line 483, in <module>
[rank1]:     raise SystemExit(main())
[rank1]:                      ^^^^^^
[rank1]:   File "/workspace/psann/scripts/train_psann_lm.py", line 459, in main
[rank1]:     trainer.train(model, dataset, max_length=int(args.max_length), val_dataset=None)
[rank1]:   File "/workspace/psann/src/psann/lm/train/trainer.py", line 206, in train
[rank1]:     wrapped = DDP(
[rank1]:               ^^^^
[rank1]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 858, in __init__
[rank1]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank1]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/utils.py", line 281, in _verify_param_shape_across_processes
[rank1]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: RuntimeError: [1]: params[0] in this process with sizes [47356, 512] appears not to match sizes of the same param in process 0.
W1111 01:09:35.894000 4672 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 4738 closing signal SIGTERM
W1111 01:09:35.899000 4672 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 4740 closing signal SIGTERM
E1111 01:09:36.118000 4672 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 4739) of binary: /workspace/psann/.venv/bin/python
Traceback (most recent call last):
  File "/workspace/psann/.venv/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/train_psann_lm.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-11_01:09:35
  host      : 27eb2793a45e
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 4739)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
