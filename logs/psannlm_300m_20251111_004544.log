[2025-11-11T00:46:05+00:00] Starting 300M run
W1111 00:46:17.242000 3883 torch/distributed/run.py:803] 
W1111 00:46:17.242000 3883 torch/distributed/run.py:803] *****************************************
W1111 00:46:17.242000 3883 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1111 00:46:17.242000 3883 torch/distributed/run.py:803] *****************************************









[tokenizer] Trained tokenizer saved to runs/tokenizer_300m/tokenizer.json
[tokenizer] Trained tokenizer saved to runs/tokenizer_300m/tokenizer.json
[tokenizer] Trained tokenizer saved to runs/tokenizer_300m/tokenizer.json
[W1111 00:58:02.692303865 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1111 00:58:02.698418618 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1111 00:58:02.783094913 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[trainer] FSDP requested but not available (size_based_auto_wrap_policy() missing 3 required positional arguments: 'module', 'recurse', and 'nonwrapped_numel'); falling back to DDP/model-only.
[trainer] FSDP requested but not available (size_based_auto_wrap_policy() missing 3 required positional arguments: 'module', 'recurse', and 'nonwrapped_numel'); falling back to DDP/model-only.
[trainer] FSDP requested but not available (size_based_auto_wrap_policy() missing 3 required positional arguments: 'module', 'recurse', and 'nonwrapped_numel'); falling back to DDP/model-only.
[rank1]:[W1111 00:58:12.899360515 Utils.hpp:112] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1111 00:58:12.899512843 Utils.hpp:112] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[trainer] Gradient checkpointing: enabled via model.enable_gradient_checkpointing()
[rank0]:[W1111 00:58:12.900139334 Utils.hpp:112] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[trainer] Gradient checkpointing: enabled via model.enable_gradient_checkpointing()
[trainer] Gradient checkpointing: enabled via model.enable_gradient_checkpointing()
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/psann/scripts/train_psann_lm.py", line 483, in <module>
[rank0]:     raise SystemExit(main())
[rank0]:                      ^^^^^^
[rank0]:   File "/workspace/psann/scripts/train_psann_lm.py", line 459, in main
[rank0]:     trainer.train(model, dataset, max_length=int(args.max_length), val_dataset=None)
[rank0]:   File "/workspace/psann/src/psann/lm/train/trainer.py", line 303, in train
[rank0]:     loss.backward()
[rank0]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 0 has a total capacity of 22.06 GiB of which 105.12 MiB is free. Process 1561121 has 21.95 GiB memory in use. Of the allocated memory 18.12 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/workspace/psann/scripts/train_psann_lm.py", line 483, in <module>
[rank2]:     raise SystemExit(main())
[rank2]:                      ^^^^^^
[rank2]:   File "/workspace/psann/scripts/train_psann_lm.py", line 459, in main
[rank2]:     trainer.train(model, dataset, max_length=int(args.max_length), val_dataset=None)
[rank2]:   File "/workspace/psann/src/psann/lm/train/trainer.py", line 303, in train
[rank2]:     loss.backward()
[rank2]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 2 has a total capacity of 22.06 GiB of which 105.12 MiB is free. Process 1561123 has 21.95 GiB memory in use. Of the allocated memory 18.12 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/workspace/psann/scripts/train_psann_lm.py", line 483, in <module>
[rank1]:     raise SystemExit(main())
[rank1]:                      ^^^^^^
[rank1]:   File "/workspace/psann/scripts/train_psann_lm.py", line 459, in main
[rank1]:     trainer.train(model, dataset, max_length=int(args.max_length), val_dataset=None)
[rank1]:   File "/workspace/psann/src/psann/lm/train/trainer.py", line 303, in train
[rank1]:     loss.backward()
[rank1]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 1 has a total capacity of 22.06 GiB of which 105.12 MiB is free. Process 1561122 has 21.95 GiB memory in use. Of the allocated memory 18.12 GiB is allocated by PyTorch, and 3.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1111 00:59:15.434192997 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1111 00:59:18.577000 3883 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 3950 closing signal SIGTERM
W1111 00:59:18.583000 3883 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 3951 closing signal SIGTERM
E1111 00:59:18.750000 3883 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 3949) of binary: /workspace/psann/.venv/bin/python
Traceback (most recent call last):
  File "/workspace/psann/.venv/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/psann/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/train_psann_lm.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-11_00:59:18
  host      : 27eb2793a45e
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3949)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
