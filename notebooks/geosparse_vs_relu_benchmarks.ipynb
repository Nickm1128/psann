{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GeoSparse vs ReLU Baselines (psann==0.12.2)\n",
        "\n",
        "**Goal:** Compare GeoSparse against dense ReLU baselines on diverse regression datasets,\n",
        "with **parameter-matched** models and **wall-clock timing**.\n",
        "\n",
        "**Fairness contract:**\n",
        "- Same optimizer family + schedule (Adam, fixed LR).\n",
        "- Same batch size and **target optimizer steps** across datasets.\n",
        "- Same preprocessing policy (StandardScaler on X; optional y-scaling).\n",
        "- Parameter counts matched within 1%.\n",
        "\n",
        "This notebook is **Colab-ready** and uses the PyPI distribution only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproduce in Colab\n",
        "\n",
        "- Runtime: **GPU** (recommended) or **CPU**.\n",
        "- Python: Colab default (3.10+).\n",
        "- Expected runtime (quick mode): ~5-10 minutes on a free GPU.\n",
        "- Expected runtime (full mode, 3 seeds): ~20-40 minutes.\n",
        "\n",
        "Toggle `QUICK_MODE` in the reproducibility section for a fast smoke run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install minimal dependencies from PyPI only.\n",
        "%pip install -q psann==0.12.2 scikit-learn pandas matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "import psann\n",
        "import torch\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "print(\"python:\", platform.python_version())\n",
        "print(\"psann:\", psann.__version__)\n",
        "print(\"psann file:\", psann.__file__)\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"cuda:\", torch.version.cuda)\n",
        "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"cuda: not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by defining the dataset suite and a loader with a shared signature.\n",
        "Each dataset returns `(X, y, task_type, feature_names, target_name)` so the\n",
        "training harness can be uniform.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset suite (frozen list)\n",
        "\n",
        "**Synthetic (regression-only):**\n",
        "- `syn_sparse_linear`: high-dimensional sparse linear regression (n_features=200, n_informative=10).\n",
        "- `syn_friedman1`: nonlinear interactions via Friedman-1.\n",
        "- `syn_piecewise_sine`: periodic + regime-switch target with noise.\n",
        "\n",
        "**Real (regression-only):**\n",
        "- `real_california_housing`: California Housing (fetch).\n",
        "- `real_diabetes`: Diabetes dataset (load).\n",
        "- `real_linnerud`: Linnerud dataset (load; uses single target column).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import (\n",
        "    make_regression,\n",
        "    make_friedman1,\n",
        "    fetch_california_housing,\n",
        "    load_diabetes,\n",
        "    load_linnerud,\n",
        ")\n",
        "\n",
        "DATASET_NAMES = [\n",
        "    'syn_sparse_linear',\n",
        "    'syn_friedman1',\n",
        "    'syn_piecewise_sine',\n",
        "    'real_california_housing',\n",
        "    'real_diabetes',\n",
        "    'real_linnerud',\n",
        "]\n",
        "\n",
        "def load_dataset(name, seed=1337):\n",
        "    # Returns: X, y, task_type, feature_names, target_name\n",
        "    if name == 'syn_sparse_linear':\n",
        "        X, y = make_regression(\n",
        "            n_samples=5000,\n",
        "            n_features=200,\n",
        "            n_informative=10,\n",
        "            noise=5.0,\n",
        "            bias=10.0,\n",
        "            random_state=seed,\n",
        "        )\n",
        "        feature_names = [f'x{i}' for i in range(X.shape[1])]\n",
        "        target_name = 'y'\n",
        "    elif name == 'syn_friedman1':\n",
        "        X, y = make_friedman1(\n",
        "            n_samples=5000,\n",
        "            n_features=20,\n",
        "            noise=0.5,\n",
        "            random_state=seed,\n",
        "        )\n",
        "        feature_names = [f'x{i}' for i in range(X.shape[1])]\n",
        "        target_name = 'y'\n",
        "    elif name == 'syn_piecewise_sine':\n",
        "        rng = np.random.default_rng(seed)\n",
        "        n_samples = 5000\n",
        "        n_features = 10\n",
        "        X = rng.normal(size=(n_samples, n_features))\n",
        "        t = X[:, 0]\n",
        "        # Regime switch at t=0 with different frequencies and offsets.\n",
        "        y = np.where(\n",
        "            t < 0,\n",
        "            0.5 * np.sin(3.0 * t) + 0.2 * t,\n",
        "            1.0 * np.sin(6.0 * t + 0.5) - 0.1 * t + 0.5,\n",
        "        )\n",
        "        y = y + 0.1 * rng.normal(size=n_samples)\n",
        "        feature_names = [f'x{i}' for i in range(X.shape[1])]\n",
        "        target_name = 'y'\n",
        "    elif name == 'real_california_housing':\n",
        "        data = fetch_california_housing()\n",
        "        X, y = data.data, data.target\n",
        "        feature_names = list(data.feature_names)\n",
        "        target_name = 'MedHouseVal'\n",
        "    elif name == 'real_diabetes':\n",
        "        data = load_diabetes()\n",
        "        X, y = data.data, data.target\n",
        "        feature_names = list(data.feature_names)\n",
        "        target_name = 'disease_progression'\n",
        "    elif name == 'real_linnerud':\n",
        "        data = load_linnerud()\n",
        "        X = data.data\n",
        "        # Use a single target column for regression comparability.\n",
        "        target_index = 0\n",
        "        y = data.target[:, target_index]\n",
        "        feature_names = list(data.feature_names)\n",
        "        target_name = data.target_names[target_index]\n",
        "    else:\n",
        "        raise ValueError(f'Unknown dataset: {name}')\n",
        "\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    task_type = 'regression'\n",
        "    return X, y, task_type, feature_names, target_name\n",
        "\n",
        "print('Datasets:', DATASET_NAMES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We standardize inputs and optionally scale targets. Metrics are always reported\n",
        "on **unscaled** targets so results remain interpretable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing & splits\n",
        "\n",
        "**Policy:**\n",
        "- Train/val/test split uses a fixed seed for reproducibility.\n",
        "- `StandardScaler` is fit on **train** data only, then applied to val/test.\n",
        "- Optional target scaling for regression is supported; metrics should be\n",
        "  computed on **unscaled** targets via inverse-transform.\n",
        "\n",
        "**Shape conventions:**\n",
        "- `X`: `(n_samples, n_features)`\n",
        "- `y`: `(n_samples,)` for regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SPLIT_SEED = 1337\n",
        "TEST_SIZE = 0.2\n",
        "VAL_SIZE = 0.1\n",
        "SCALE_Y = False\n",
        "\n",
        "@dataclass\n",
        "class DatasetSplit:\n",
        "    X_train: np.ndarray\n",
        "    y_train: np.ndarray\n",
        "    X_val: np.ndarray\n",
        "    y_val: np.ndarray\n",
        "    X_test: np.ndarray\n",
        "    y_test: np.ndarray\n",
        "    x_scaler: StandardScaler\n",
        "    y_scaler: StandardScaler | None\n",
        "\n",
        "def split_and_scale(X, y, seed=SPLIT_SEED, test_size=TEST_SIZE, val_size=VAL_SIZE, scale_y=SCALE_Y):\n",
        "    # First split train vs (val+test).\n",
        "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "        X, y, test_size=test_size + val_size, random_state=seed\n",
        "    )\n",
        "    # Split val vs test from the temp set.\n",
        "    val_frac = val_size / (test_size + val_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_tmp, y_tmp, test_size=1.0 - val_frac, random_state=seed\n",
        "    )\n",
        "\n",
        "    x_scaler = StandardScaler().fit(X_train)\n",
        "    X_train_s = x_scaler.transform(X_train).astype(np.float32, copy=False)\n",
        "    X_val_s = x_scaler.transform(X_val).astype(np.float32, copy=False)\n",
        "    X_test_s = x_scaler.transform(X_test).astype(np.float32, copy=False)\n",
        "\n",
        "    y_scaler = None\n",
        "    if scale_y:\n",
        "        y_scaler = StandardScaler().fit(y_train.reshape(-1, 1))\n",
        "        y_train_s = y_scaler.transform(y_train.reshape(-1, 1)).ravel()\n",
        "        y_val_s = y_scaler.transform(y_val.reshape(-1, 1)).ravel()\n",
        "        y_test_s = y_scaler.transform(y_test.reshape(-1, 1)).ravel()\n",
        "    else:\n",
        "        y_train_s, y_val_s, y_test_s = y_train, y_val, y_test\n",
        "\n",
        "    y_train_s = y_train_s.astype(np.float32, copy=False)\n",
        "    y_val_s = y_val_s.astype(np.float32, copy=False)\n",
        "    y_test_s = y_test_s.astype(np.float32, copy=False)\n",
        "\n",
        "    return DatasetSplit(\n",
        "        X_train=X_train_s, y_train=y_train_s,\n",
        "        X_val=X_val_s, y_val=y_val_s,\n",
        "        X_test=X_test_s, y_test=y_test_s,\n",
        "        x_scaler=x_scaler, y_scaler=y_scaler,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models under test\n",
        "\n",
        "We compare **GeoSparse** against a **dense ReLU baseline** using the public\n",
        "`psann` sklearn-style estimators.\n",
        "\n",
        "- **GeoSparse:** `GeoSparseRegressor` (activation_type=`\"psann\"` by default).\n",
        "- **Dense ReLU:** `PSANNRegressor` with `activation_type=\"relu\"`.\n",
        "\n",
        "**Shape note:** `GeoSparseRegressor` requires a 2D shape `(H, W)` such that\n",
        "`H * W == n_features`. For tabular datasets we use `(1, n_features)` by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from psann import GeoSparseRegressor, PSANNRegressor\n",
        "\n",
        "DEFAULT_TRAIN_CFG = dict(\n",
        "    epochs=100,\n",
        "    batch_size=256,\n",
        "    lr=1e-3,\n",
        "    optimizer='adam',\n",
        "    weight_decay=0.0,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    random_state=1337,\n",
        ")\n",
        "\n",
        "def resolve_geo_shape(input_dim, shape=None):\n",
        "    if shape is not None:\n",
        "        return tuple(shape)\n",
        "    return (1, int(input_dim))\n",
        "\n",
        "def build_geosparse_regressor(input_dim, *, shape=None, activation_type='psann', **overrides):\n",
        "    cfg = dict(DEFAULT_TRAIN_CFG)\n",
        "    cfg.update(overrides)\n",
        "    return GeoSparseRegressor(\n",
        "        hidden_layers=4,\n",
        "        activation_type=activation_type,\n",
        "        shape=resolve_geo_shape(input_dim, shape),\n",
        "        k=8,\n",
        "        compute_mode='gather',\n",
        "        **cfg,\n",
        "    )\n",
        "\n",
        "def build_dense_relu_regressor(hidden_layers=2, hidden_units=64, **overrides):\n",
        "    cfg = dict(DEFAULT_TRAIN_CFG)\n",
        "    cfg.update(overrides)\n",
        "    return PSANNRegressor(\n",
        "        hidden_layers=hidden_layers,\n",
        "        hidden_units=hidden_units,\n",
        "        activation_type='relu',\n",
        "        **cfg,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter matching (GeoSparse vs Dense ReLU)\n",
        "\n",
        "We match **total trainable parameters** by counting parameters of the\n",
        "underlying PyTorch modules (including PSANN sine parameters).\n",
        "Dense ReLU width is chosen to minimize absolute mismatch within a tolerance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from psann import count_params\n",
        "from psann.nn import PSANNNet\n",
        "from psann.nn_geo_sparse import GeoSparseNet\n",
        "\n",
        "PARAM_TOL = 0.01  # <= 1% relative mismatch\n",
        "\n",
        "def count_geosparse_params(input_dim, output_dim, *, depth=4, k=8, shape=None, activation_type='psann'):\n",
        "    shape = resolve_geo_shape(input_dim, shape)\n",
        "    model = GeoSparseNet(\n",
        "        int(input_dim),\n",
        "        int(output_dim),\n",
        "        shape=shape,\n",
        "        depth=int(depth),\n",
        "        k=int(k),\n",
        "        activation_type=activation_type,\n",
        "        norm='rms',\n",
        "        drop_path_max=0.0,\n",
        "        residual_alpha_init=0.0,\n",
        "        bias=True,\n",
        "        compute_mode='gather',\n",
        "        seed=1337,\n",
        "    )\n",
        "    return count_params(model, trainable_only=True)\n",
        "\n",
        "def count_dense_params(input_dim, output_dim, *, depth=2, hidden_units=64, activation_type='relu'):\n",
        "    model = PSANNNet(\n",
        "        int(input_dim),\n",
        "        int(output_dim),\n",
        "        hidden_layers=int(depth),\n",
        "        hidden_units=int(hidden_units),\n",
        "        activation_type=activation_type,\n",
        "    )\n",
        "    return count_params(model, trainable_only=True)\n",
        "\n",
        "def match_dense_width_to_geosparse(\n",
        "    input_dim,\n",
        "    output_dim=1,\n",
        "    *,\n",
        "    geo_depth=4,\n",
        "    geo_k=8,\n",
        "    geo_shape=None,\n",
        "    dense_depth=2,\n",
        "    width_candidates=None,\n",
        "    tol=PARAM_TOL,\n",
        "):\n",
        "    target = count_geosparse_params(\n",
        "        input_dim, output_dim, depth=geo_depth, k=geo_k, shape=geo_shape\n",
        "    )\n",
        "    if width_candidates is None:\n",
        "        width_candidates = list(range(8, 2049, 8))\n",
        "    best_width = None\n",
        "    best_params = None\n",
        "    best_mismatch = None\n",
        "    for width in width_candidates:\n",
        "        params = count_dense_params(\n",
        "            input_dim, output_dim, depth=dense_depth, hidden_units=width, activation_type='relu'\n",
        "        )\n",
        "        mismatch = abs(params - target)\n",
        "        if best_mismatch is None or mismatch < best_mismatch:\n",
        "            best_mismatch = mismatch\n",
        "            best_width = width\n",
        "            best_params = params\n",
        "    rel_mismatch = best_mismatch / max(1, target)\n",
        "    if rel_mismatch > tol:\n",
        "        print(f'WARN: best rel mismatch {rel_mismatch:.3f} exceeds tol={tol:.3f}')\n",
        "    return {\n",
        "        'target_params': target,\n",
        "        'dense_width': best_width,\n",
        "        'dense_params': best_params,\n",
        "        'rel_mismatch': rel_mismatch,\n",
        "    }\n",
        "\n",
        "# Example sanity check for a 20-feature dataset\n",
        "example = match_dense_width_to_geosparse(input_dim=20, output_dim=1)\n",
        "example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We match training budgets by optimizer steps instead of epochs to keep\n",
        "comparisons consistent across datasets with different sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop + timing\n",
        "\n",
        "We time the full `fit(...)` call with CUDA synchronization when available.\n",
        "A short *warmup* run is executed on a throwaway model to avoid timing\n",
        "first-iteration kernel compilation.\n",
        "\n",
        "**Budget policy:** target a fixed **number of optimizer steps** per run\n",
        "by adjusting epochs based on the dataset size and batch size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "import types\n",
        "\n",
        "TARGET_STEPS = 400\n",
        "PROGRESS_EVERY_STEPS = 100\n",
        "WARMUP_SAMPLES = 512\n",
        "WARMUP_EPOCHS = 1\n",
        "\n",
        "def _maybe_cuda_sync():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def _subsample(X, y, n_samples, seed=1337):\n",
        "    if n_samples is None or len(X) <= n_samples:\n",
        "        return X, y\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.choice(len(X), size=int(n_samples), replace=False)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "def _attach_progress(model, steps_per_epoch, progress_every_steps=PROGRESS_EVERY_STEPS):\n",
        "    state = {'next_report': progress_every_steps}\n",
        "    def _epoch_callback(self, epoch, train_loss, val_loss, improved, patience_left):\n",
        "        steps_done = (epoch + 1) * steps_per_epoch\n",
        "        if steps_done >= state['next_report']:\n",
        "            msg = f\"[step ~{steps_done}] train_loss={train_loss:.4f}\"\n",
        "            if val_loss is not None:\n",
        "                msg += f\" val_loss={val_loss:.4f}\"\n",
        "            print(msg)\n",
        "            state['next_report'] += progress_every_steps\n",
        "    model.epoch_callback = types.MethodType(_epoch_callback, model)\n",
        "    return model\n",
        "\n",
        "def _compute_epochs(n_train, batch_size, target_steps=TARGET_STEPS):\n",
        "    steps_per_epoch = int(math.ceil(n_train / batch_size))\n",
        "    epochs = int(math.ceil(target_steps / steps_per_epoch))\n",
        "    return max(1, epochs), steps_per_epoch\n",
        "\n",
        "def warmup_fit(model_factory, X_train, y_train, X_val, y_val):\n",
        "    if not torch.cuda.is_available():\n",
        "        return\n",
        "    X_w, y_w = _subsample(X_train, y_train, WARMUP_SAMPLES, seed=123)\n",
        "    model = model_factory(epochs=WARMUP_EPOCHS)\n",
        "    _maybe_cuda_sync()\n",
        "    model.fit(X_w, y_w, validation_data=(X_val, y_val), verbose=0)\n",
        "    _maybe_cuda_sync()\n",
        "\n",
        "def fit_with_timing(model_factory, X_train, y_train, X_val, y_val, *, batch_size):\n",
        "    epochs, steps_per_epoch = _compute_epochs(len(X_train), batch_size)\n",
        "    model = model_factory(epochs=epochs)\n",
        "    _attach_progress(model, steps_per_epoch)\n",
        "\n",
        "    _maybe_cuda_sync()\n",
        "    start = time.perf_counter()\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=0)\n",
        "    _maybe_cuda_sync()\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "    samples_seen = len(X_train) * epochs\n",
        "    steps_per_sec = total_steps / max(elapsed, 1e-9)\n",
        "    samples_per_sec = samples_seen / max(elapsed, 1e-9)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'epochs': epochs,\n",
        "        'steps_per_epoch': steps_per_epoch,\n",
        "        'total_steps': total_steps,\n",
        "        'train_time_s': elapsed,\n",
        "        'steps_per_sec': steps_per_sec,\n",
        "        'samples_per_sec': samples_per_sec,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation & reporting\n",
        "\n",
        "We compute regression metrics on **unscaled** targets (if `SCALE_Y=True`,\n",
        "we inverse-transform `y` and predictions for reporting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def compute_regression_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'mse': mean_squared_error(y_true, y_pred),\n",
        "        'mae': mean_absolute_error(y_true, y_pred),\n",
        "        'r2': r2_score(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "def _unscale_y(y, y_scaler):\n",
        "    if y_scaler is None:\n",
        "        return y\n",
        "    return y_scaler.inverse_transform(y.reshape(-1, 1)).ravel()\n",
        "\n",
        "def evaluate_regression(model, split):\n",
        "    y_true = _unscale_y(split.y_test, split.y_scaler)\n",
        "    y_pred = model.predict(split.X_test)\n",
        "    y_pred = _unscale_y(np.asarray(y_pred, dtype=np.float32), split.y_scaler)\n",
        "    return compute_regression_metrics(y_true, y_pred)\n",
        "\n",
        "def build_results_table(rows):\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return df, df\n",
        "    summary = (\n",
        "        df.groupby(['dataset', 'model'])\n",
        "        .agg(\n",
        "            mse_mean=('mse', 'mean'),\n",
        "            mse_std=('mse', 'std'),\n",
        "            mae_mean=('mae', 'mean'),\n",
        "            mae_std=('mae', 'std'),\n",
        "            r2_mean=('r2', 'mean'),\n",
        "            r2_std=('r2', 'std'),\n",
        "            train_time_mean=('train_time_s', 'mean'),\n",
        "            train_time_std=('train_time_s', 'std'),\n",
        "            params_mean=('params', 'mean'),\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    return df, summary\n",
        "\n",
        "def plot_metric(df, metric, title):\n",
        "    if df.empty:\n",
        "        print('No results to plot yet.')\n",
        "        return\n",
        "    ax = df.pivot_table(index='dataset', columns='model', values=metric).plot(kind='bar')\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel(metric)\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_time(df):\n",
        "    if df.empty:\n",
        "        print('No results to plot yet.')\n",
        "        return\n",
        "    ax = df.pivot_table(index='dataset', columns='model', values='train_time_s').plot(kind='bar')\n",
        "    ax.set_title('Train wall time (s)')\n",
        "    ax.set_ylabel('seconds')\n",
        "    plt.show()\n",
        "\n",
        "# Placeholder for experiment results\n",
        "results = []\n",
        "\n",
        "# Example usage after running experiments:\n",
        "# df, summary = build_results_table(results)\n",
        "# display(df.head())\n",
        "# display(summary)\n",
        "# plot_metric(summary, 'mse_mean', 'MSE by dataset/model')\n",
        "# plot_train_time(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproducibility & stability checks\n",
        "\n",
        "We run multiple seeds and apply simple sanity checks so results are trustworthy.\n",
        "A `QUICK_MODE` option reduces runtime for smoke tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "SEEDS = [0, 1, 2]\n",
        "QUICK_MODE = True\n",
        "\n",
        "if QUICK_MODE:\n",
        "    SEEDS = [0]\n",
        "    TARGET_STEPS = 150\n",
        "    print('QUICK_MODE enabled: seeds=', SEEDS, 'steps=', TARGET_STEPS)\n",
        "\n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def sanity_check_param_matching(input_dim, output_dim=1):\n",
        "    match = match_dense_width_to_geosparse(input_dim=input_dim, output_dim=output_dim)\n",
        "    rel = match['rel_mismatch']\n",
        "    if rel > PARAM_TOL:\n",
        "        print('WARN: param mismatch above tolerance:', match)\n",
        "    return match\n",
        "\n",
        "def sanity_check_results(rows):\n",
        "    if not rows:\n",
        "        print('No results yet; run experiments first.')\n",
        "        return\n",
        "    for row in rows:\n",
        "        if row.get('train_time_s', 0.0) <= 0:\n",
        "            print('WARN: non-positive train_time_s:', row)\n",
        "        if row.get('params') is None:\n",
        "            print('WARN: missing params:', row)\n",
        "\n",
        "# Example: sanity check for each dataset dimension\n",
        "for name in DATASET_NAMES:\n",
        "    X, y, task, feats, target = load_dataset(name)\n",
        "    sanity_check_param_matching(input_dim=X.shape[1], output_dim=1)\n",
        "\n",
        "# After running experiments: sanity_check_results(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions (fill after running)\n",
        "\n",
        "- Summary of GeoSparse vs ReLU performance across datasets.\n",
        "- Where GeoSparse helps or hurts.\n",
        "- Suggested next tests (more datasets, larger budgets, classification add-on).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}