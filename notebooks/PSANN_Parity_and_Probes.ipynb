{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d6a743",
   "metadata": {
    "id": "f3d6a743"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nickm1128/psann/blob/main/notebooks/PSANN_Parity_and_Probes.ipynb)\n",
    "\n# ResPSANN Compute-Parity Experiments (Colab Runner)\n\n",
    "\n> **Setup tip:** Inputs, targets, and any optional context arrays should be provided as `np.float32` before calling PSANN estimators. Set `device` on estimator construction (e.g. `device=\"cuda\"`) to keep training on GPU, and remember that changing context-builder parameters via `set_params` clears the cached builder.\n\n",
    "This notebook orchestrates the experiments described in `plan.txt` using the datasets summarised in `data_descriptions.txt`. Execute it inside Google Colab (GPU runtime recommended).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740b594",
   "metadata": {
    "id": "4740b594"
   },
   "source": [
    "## Run Checklist\n",
    "- Prefer Google Colab with a GPU runtime (recommended) before running any experiments.\n",
    "- Let the setup cell install the latest published `psann` package via `pip`; no repository clone is required.\n",
    "- Upload or mount the dataset directory so that `DATA_ROOT` points to it (defaults to `<working dir>/datasets`).\n",
    "- Adjust `GLOBAL_CONFIG` and the experiment toggles before launching training to stay within the Colab budget.\n",
    "- Keep the heavy training cells disabled until you are ready to execute them in Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b559b11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b559b11",
    "outputId": "c562aa96-69da-451b-cb87-2244d0f3090f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "DEFAULT_PROJECT_ROOT = Path(\"/content\") if COLAB else Path.cwd()\n",
    "PROJECT_ROOT = Path(os.getenv(\"PSANN_PROJECT_ROOT\", DEFAULT_PROJECT_ROOT)).resolve()\n",
    "\n",
    "DATA_ROOT = Path(os.getenv(\"PSANN_DATA_ROOT\", PROJECT_ROOT / \"datasets\")).resolve()\n",
    "RESULTS_ROOT = Path(os.getenv(\"PSANN_RESULTS_ROOT\", PROJECT_ROOT / \"colab_results\")).resolve()\n",
    "FIGURE_ROOT = RESULTS_ROOT / \"figures\"\n",
    "\n",
    "RESULTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "FIGURE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    print(f\"[WARN] DATA_ROOT {DATA_ROOT} does not exist yet. Upload datasets or update PSANN_DATA_ROOT.\")\n",
    "\n",
    "print(f\"Colab runtime         : {COLAB}\")\n",
    "print(f\"Project root          : {PROJECT_ROOT}\")\n",
    "print(f\"Dataset root          : {DATA_ROOT}\")\n",
    "print(f\"Results directory     : {RESULTS_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c824d6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c824d6c",
    "outputId": "203aed73-2478-4de2-a94b-b890ef6791f5"
   },
   "outputs": [],
   "source": [
    "# --- Robust extraction for your datasets.zip layout ---\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "from pathlib import Path, PureWindowsPath\n",
    "\n",
    "# Fallbacks if not already defined in your notebook\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path(\"/content\")\n",
    "try:\n",
    "    DATA_ROOT\n",
    "except NameError:\n",
    "    DATA_ROOT = PROJECT_ROOT / \"datasets\"\n",
    "\n",
    "zip_path = PROJECT_ROOT / \"datasets.zip\"\n",
    "\n",
    "# Canonical names your code expects\n",
    "EXPECTED_FOLDERS = [\n",
    "    \"Industrial Data from the Electric Arc Furnace\",\n",
    "    \"Beijing Air Quality\",\n",
    "    \"Human Activity Recognition\",\n",
    "    \"Jena Climate 2009-2016\",  # we'll normalize any en/em/Unicode minus to a hyphen\n",
    "    \"Kaggle Rossmann Store Sales\",\n",
    "]\n",
    "\n",
    "def _normalize_name(s: str) -> str:\n",
    "    # unify hyphen-like chars, collapse whitespace, lowercase\n",
    "    s = (s.replace(\"\\u2013\", \"-\")  # en dash\n",
    "           .replace(\"\\u2014\", \"-\")  # em dash\n",
    "           .replace(\"\\u2212\", \"-\")  # minus\n",
    "           .replace(\"\\xa0\", \" \"))   # non-breaking space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def _safe_mkdir(path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create directory `path`, removing any FILE that blocks directory creation\n",
    "    at this path or any ancestor.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        return\n",
    "    except NotADirectoryError:\n",
    "        # Find any ancestor that's a file and remove it.\n",
    "        # Include the path itself first, then walk upward.\n",
    "        for ancestor in [path, *path.parents]:\n",
    "            try:\n",
    "                if ancestor.exists() and ancestor.is_file():\n",
    "                    ancestor.unlink()\n",
    "            except Exception:\n",
    "                # If we can't remove, re-raise later when mkdir fails again\n",
    "                pass\n",
    "        # Try once more after clearing blockers\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _canonicalize_top_level_dirs(root: Path, expected_names: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    If top-level dirs exist with dash/space variants, rename them\n",
    "    to the canonical EXPECTED_FOLDERS names so downstream code works.\n",
    "    \"\"\"\n",
    "    if not root.exists():\n",
    "        return\n",
    "    # Map normalized->actual path for current top-level dirs\n",
    "    current = { _normalize_name(p.name): p for p in root.iterdir() if p.is_dir() }\n",
    "    for exp in expected_names:\n",
    "        canonical = root / exp\n",
    "        if canonical.exists():\n",
    "            continue\n",
    "        norm = _normalize_name(exp)\n",
    "        if norm in current and current[norm].exists():\n",
    "            src = current[norm]\n",
    "            # Avoid rename conflict: if a file with target name exists, remove it\n",
    "            if canonical.exists() and canonical.is_file():\n",
    "                canonical.unlink()\n",
    "            print(f\"[rename] {src.name} -> {exp}\")\n",
    "            src.rename(canonical)\n",
    "\n",
    "def extract_datasets(zip_path: Path, target_root: Path) -> None:\n",
    "    scratch_root = target_root.parent / \"_datasets_unpack_tmp\"\n",
    "    if scratch_root.exists():\n",
    "        shutil.rmtree(scratch_root)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        for entry in zf.infolist():\n",
    "            # Normalize path parts from the ZIP (handles both / and \\ separators)\n",
    "            parts = PureWindowsPath(entry.filename).parts\n",
    "            if not parts or parts[0].lower() != \"datasets\":\n",
    "                continue\n",
    "            rel_parts = parts[1:]\n",
    "            if not rel_parts:\n",
    "                continue\n",
    "\n",
    "            dest = scratch_root.joinpath(*rel_parts)\n",
    "\n",
    "            # Treat as directory if zip marks it so OR the path text ends with a slash/backslash\n",
    "            is_dir_entry = entry.is_dir() or entry.filename.endswith(\"/\") or entry.filename.endswith(\"\\\\\")\n",
    "            if is_dir_entry:\n",
    "                _safe_mkdir(dest)\n",
    "            else:\n",
    "                # Ensure parents exist, removing any file that blocks dir creation\n",
    "                _safe_mkdir(dest.parent)\n",
    "                # Extract the file\n",
    "                with zf.open(entry, \"r\") as src, open(dest, \"wb\") as dst:\n",
    "                    shutil.copyfileobj(src, dst)\n",
    "\n",
    "    # Replace/refresh target_root\n",
    "    if target_root.exists():\n",
    "        shutil.rmtree(target_root)\n",
    "    scratch_root.rename(target_root)\n",
    "\n",
    "    # Canonicalize top-level dir names (e.g., Jena dash variants)\n",
    "    _canonicalize_top_level_dirs(target_root, EXPECTED_FOLDERS)\n",
    "\n",
    "def datasets_ready(root: Path) -> bool:\n",
    "    if not root.exists():\n",
    "        return False\n",
    "    # Accept either exact or normalized matches for robustness\n",
    "    have = { _normalize_name(p.name) for p in root.iterdir() if p.is_dir() }\n",
    "    need = { _normalize_name(n) for n in EXPECTED_FOLDERS }\n",
    "    return need.issubset(have)\n",
    "\n",
    "# --- Clean up stray \"datasets\\...\" artefacts before extraction (from Windows zips) ---\n",
    "for leftover in PROJECT_ROOT.iterdir():\n",
    "    if \"\\\\\" in leftover.name and leftover.name.lower().startswith(\"datasets\"):\n",
    "        if leftover.is_dir():\n",
    "            shutil.rmtree(leftover)\n",
    "        else:\n",
    "            leftover.unlink()\n",
    "\n",
    "# --- Run ---\n",
    "if datasets_ready(DATA_ROOT):\n",
    "    print(f\"Datasets already present at {DATA_ROOT}\")\n",
    "elif zip_path.exists():\n",
    "    print(f\"Extracting {zip_path} (normalising Windows/Unicode paths)…\")\n",
    "    extract_datasets(zip_path, DATA_ROOT)\n",
    "    if datasets_ready(DATA_ROOT):\n",
    "        print(f\"Extraction complete. DATA_ROOT now available at {DATA_ROOT}\")\n",
    "        # Optional: quick sanity peek\n",
    "        for p in sorted(DATA_ROOT.iterdir()):\n",
    "            if p.is_dir():\n",
    "                print(\" -\", p.name)\n",
    "    else:\n",
    "        print(\"[WARN] Extraction finished but expected folders are still missing.\")\n",
    "else:\n",
    "    print(f\"Archive {zip_path} not found. Upload datasets.zip or mount the datasets directory.\")\n",
    "\n",
    "\n",
    "# Helper Function for saving outputs\n",
    "def zip_folder(folder_path: str | Path, output_path: str | Path | None = None, *, include_hidden: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Compresses an entire folder (recursively) into a .zip archive.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str or Path\n",
    "        Path to the folder to zip.\n",
    "    output_path : str or Path or None, optional\n",
    "        Output .zip file path. Defaults to \"<folder_name>.zip\" in the same directory.\n",
    "    include_hidden : bool, optional\n",
    "        Whether to include hidden files (those starting with '.').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the created .zip file.\n",
    "    \"\"\"\n",
    "    folder_path = Path(folder_path).resolve()\n",
    "    if not folder_path.is_dir():\n",
    "        raise ValueError(f\"{folder_path} is not a valid directory\")\n",
    "\n",
    "    if output_path is None:\n",
    "        output_path = folder_path.with_suffix(\".zip\")\n",
    "    else:\n",
    "        output_path = Path(output_path).resolve()\n",
    "\n",
    "    with zipfile.ZipFile(output_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            # skip hidden dirs/files if requested\n",
    "            if not include_hidden:\n",
    "                dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "                files = [f for f in files if not f.startswith(\".\")]\n",
    "\n",
    "            for file in files:\n",
    "                abs_path = Path(root) / file\n",
    "                # relative path inside the zip\n",
    "                rel_path = abs_path.relative_to(folder_path)\n",
    "                zf.write(abs_path, arcname=rel_path)\n",
    "\n",
    "    print(f\"Zipped {folder_path} → {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07209e39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07209e39",
    "outputId": "006342a8-7f67-432d-d7bf-78c283ffbeea"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_dependencies():\n",
    "    base_packages = [\n",
    "        \"psann\",\n",
    "        \"pandas>=2.0\",\n",
    "        \"numpy>=1.24\",\n",
    "        \"scikit-learn>=1.3\",\n",
    "        \"torch>=2.1\",\n",
    "        \"torchvision>=0.16\",\n",
    "        \"torchaudio>=2.1\",\n",
    "        \"lightgbm>=4.0\",\n",
    "        \"xgboost>=1.7\",\n",
    "        \"catboost>=1.2\",\n",
    "        \"shap>=0.44\",\n",
    "        \"matplotlib>=3.7\",\n",
    "        \"seaborn>=0.13\",\n",
    "        \"plotly>=5.18\",\n",
    "        \"imbalanced-learn>=0.12\",\n",
    "        \"tqdm>=4.66\",\n",
    "        \"einops>=0.7\",\n",
    "        \"rich>=13.7\",\n",
    "    ]\n",
    "    print(\"Installing psann and supporting packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + base_packages)\n",
    "\n",
    "\n",
    "if COLAB:\n",
    "    install_dependencies()\n",
    "else:\n",
    "    print(\"Skipping dependency installation because we are not inside Colab.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4441a6",
   "metadata": {
    "id": "8c4441a6"
   },
   "outputs": [],
   "source": [
    "# Core dependencies used across the notebook\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772afbf3",
   "metadata": {
    "id": "772afbf3"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    learning_rate: float\n",
    "    weight_decay: float = 0.0\n",
    "    max_minutes: Optional[float] = None\n",
    "    early_stopping: bool = True\n",
    "    patience: int = 10\n",
    "    gradient_clip: Optional[float] = None\n",
    "    scheduler: Optional[str] = None\n",
    "    scheduler_params: Optional[Dict[str, Any]] = None\n",
    "    warmup_steps: int = 0\n",
    "    max_batches_per_epoch: Optional[int] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    name: str\n",
    "    builder: Callable[[Tuple[int, ...], int, Dict[str, Any]], nn.Module]\n",
    "    train_config: TrainConfig\n",
    "    task_type: Literal[\"regression\", \"classification\", \"multitask\"]\n",
    "    input_kind: Literal[\"tabular\", \"sequence\"]\n",
    "    group: str = \"baseline\"\n",
    "    extra: Dict[str, Any] = field(default_factory=dict)\n",
    "    param_target: Optional[int] = None\n",
    "    notes: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetBundle:\n",
    "    name: str\n",
    "    task_type: Literal[\"regression\", \"classification\", \"multitask\"]\n",
    "    input_kind: Literal[\"tabular\", \"sequence\"]\n",
    "    feature_names: List[str]\n",
    "    target_names: List[str]\n",
    "    train: Dict[str, np.ndarray]\n",
    "    val: Dict[str, np.ndarray]\n",
    "    test: Dict[str, np.ndarray]\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def summary(self) -> Dict[str, Any]:\n",
    "        info = {\n",
    "            \"name\": self.name,\n",
    "            \"task_type\": self.task_type,\n",
    "            \"input_kind\": self.input_kind,\n",
    "            \"n_train\": len(self.train[\"X\"]),\n",
    "            \"n_val\": len(self.val[\"X\"]),\n",
    "            \"n_test\": len(self.test[\"X\"]),\n",
    "            \"input_shape\": tuple(self.train[\"X\"].shape[1:]),\n",
    "            \"target_shape\": tuple(self.train[\"y\"].shape[1:]) if self.train[\"y\"].ndim > 1 else (),\n",
    "        }\n",
    "        info.update({f\"meta_{k}\": v for k, v in self.metadata.items() if isinstance(v, (int, float, str))})\n",
    "        return info\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    dataset: str\n",
    "    task: str\n",
    "    model: str\n",
    "    group: str\n",
    "    split: str\n",
    "    metrics: Dict[str, float]\n",
    "    params: int\n",
    "    train_wall_seconds: float\n",
    "    notes: str = \"\"\n",
    "\n",
    "\n",
    "class ResultLogger:\n",
    "    def __init__(self) -> None:\n",
    "        self._rows: List[ExperimentResult] = []\n",
    "\n",
    "    def append(self, row: ExperimentResult) -> None:\n",
    "        self._rows.append(row)\n",
    "\n",
    "    def to_frame(self) -> pd.DataFrame:\n",
    "        records = []\n",
    "        for row in self._rows:\n",
    "            rec = {\n",
    "                \"dataset\": row.dataset,\n",
    "                \"task\": row.task,\n",
    "                \"model\": row.model,\n",
    "                \"group\": row.group,\n",
    "                \"split\": row.split,\n",
    "                \"params\": row.params,\n",
    "                \"train_wall_seconds\": row.train_wall_seconds,\n",
    "                \"notes\": row.notes,\n",
    "            }\n",
    "            rec.update(row.metrics)\n",
    "            records.append(rec)\n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "RESULT_LOGGER = ResultLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94828a93",
   "metadata": {
    "id": "94828a93"
   },
   "outputs": [],
   "source": [
    "def load_jena_climate(data_root: Path) -> pd.DataFrame:\n",
    "    path = data_root / \"Jena Climate 2009-2016\" / \"jena_climate_2009_2016.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Jena climate CSV not found at {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"Date Time\"], dayfirst=True)\n",
    "    df = df.drop(columns=[\"Date Time\"])\n",
    "    numeric_cols = [col for col in df.columns if col != \"datetime\"]\n",
    "    df[numeric_cols] = df[numeric_cols].astype(np.float32)\n",
    "    df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_jena_bundle(\n",
    "    df: pd.DataFrame,\n",
    "    target: str = \"T (degC)\",\n",
    "    context_steps: int = 72,\n",
    "    horizon_steps: int = 36,\n",
    "    resample_factor: int = 1,\n",
    ") -> DatasetBundle:\n",
    "    df = df.copy()\n",
    "    if resample_factor > 1:\n",
    "        df = df.iloc[::resample_factor].reset_index(drop=True)\n",
    "    df = add_calendar_features(df, \"datetime\")\n",
    "    feature_cols = [c for c in df.columns if c not in (\"datetime\", target)]\n",
    "    df[feature_cols] = df[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    values = df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    target_values = df[target].to_numpy(dtype=np.float32)\n",
    "    timestamps = df[\"datetime\"].to_numpy()\n",
    "\n",
    "    windows = []\n",
    "    targets = []\n",
    "    ts_list = []\n",
    "    for idx in range(context_steps, len(df) - horizon_steps):\n",
    "        window = values[idx - context_steps : idx]\n",
    "        target_value = target_values[idx + horizon_steps]\n",
    "        windows.append(window)\n",
    "        targets.append(target_value)\n",
    "        ts_list.append(timestamps[idx])\n",
    "    X = np.stack(windows)\n",
    "    y = np.asarray(targets, dtype=np.float32)[:, None]\n",
    "    ts = np.asarray(ts_list)\n",
    "\n",
    "    df_windows = pd.DataFrame({\"datetime\": ts})\n",
    "    train_df, val_df, test_df = train_val_test_split_by_time(\n",
    "        df_windows, \"datetime\", \"2015-01-01\", \"2016-01-01\"\n",
    "    )\n",
    "    train_idx = train_df.index.to_numpy()\n",
    "    val_idx = val_df.index.to_numpy()\n",
    "    test_idx = test_df.index.to_numpy()\n",
    "\n",
    "    target_slug = (\n",
    "        target.lower()\n",
    "        .replace(\" \", \"\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\"/\", \"\")\n",
    "    )\n",
    "    bundle_name = f\"Jena_{target_slug}_{context_steps}ctx_{horizon_steps}h\"\n",
    "\n",
    "    bundle = DatasetBundle(\n",
    "        name=bundle_name,\n",
    "        task_type=\"regression\",\n",
    "        input_kind=\"sequence\",\n",
    "        feature_names=feature_cols,\n",
    "        target_names=[target],\n",
    "        train={\"X\": X[train_idx], \"y\": y[train_idx]},\n",
    "        val={\"X\": X[val_idx], \"y\": y[val_idx]},\n",
    "        test={\"X\": X[test_idx], \"y\": y[test_idx]},\n",
    "        metadata={\n",
    "            \"context_steps\": context_steps,\n",
    "            \"horizon_steps\": horizon_steps,\n",
    "            \"resample_factor\": resample_factor,\n",
    "        },\n",
    "    )\n",
    "    return bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf214b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdf214b6",
    "outputId": "5f0bbe2a-62dd-46ce-98cb-856b203db1aa"
   },
   "outputs": [],
   "source": [
    "def load_har_engineered(data_root: Path):\n",
    "    base = data_root / \"Human Activity Recognition\" / \"UCI HAR Dataset\"\n",
    "    X_train = pd.read_csv(base / \"train\" / \"X_train.txt\", sep='\\s+', header=None)\n",
    "    y_train = pd.read_csv(base / \"train\" / \"y_train.txt\", header=None).squeeze(\"columns\")\n",
    "    subject_train = pd.read_csv(base / \"train\" / \"subject_train.txt\", header=None).squeeze(\"columns\")\n",
    "\n",
    "    X_test = pd.read_csv(base / \"test\" / \"X_test.txt\", sep='\\s+', header=None)\n",
    "    y_test = pd.read_csv(base / \"test\" / \"y_test.txt\", header=None).squeeze(\"columns\")\n",
    "    subject_test = pd.read_csv(base / \"test\" / \"subject_test.txt\", header=None).squeeze(\"columns\")\n",
    "\n",
    "    y_train = y_train.values.astype(int) - 1\n",
    "    y_test = y_test.values.astype(int) - 1\n",
    "\n",
    "    features = (base / \"features.txt\").read_text().strip().splitlines()\n",
    "    feature_names = [line.split()[1] for line in features]\n",
    "\n",
    "    X_train.columns = feature_names\n",
    "    X_test.columns = feature_names\n",
    "\n",
    "    train_df = X_train.copy()\n",
    "    test_df = X_test.copy()\n",
    "    train_df[\"label\"] = y_train\n",
    "    train_df[\"subject\"] = subject_train.values\n",
    "    test_df[\"label\"] = y_test\n",
    "    test_df[\"subject\"] = subject_test.values\n",
    "\n",
    "    return train_df, test_df, feature_names\n",
    "\n",
    "\n",
    "def prepare_har_engineered_bundle(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    feature_names: List[str],\n",
    "    val_fraction: float = 0.15,\n",
    ") -> DatasetBundle:\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    X = train_df[feature_names].to_numpy(dtype=np.float32)\n",
    "    y = train_df[\"label\"].to_numpy(dtype=np.int64)\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_fraction, random_state=GLOBAL_CONFIG[\"seed\"])\n",
    "    train_idx, val_idx = next(splitter.split(X, y))\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx][:, None]\n",
    "    X_val = X[val_idx]\n",
    "    y_val = y[val_idx][:, None]\n",
    "\n",
    "    X_test = test_df[feature_names].to_numpy(dtype=np.float32)\n",
    "    y_test = test_df[\"label\"].to_numpy(dtype=np.int64)[:, None]\n",
    "\n",
    "    bundle = DatasetBundle(\n",
    "        name=\"HAR_engineered\",\n",
    "        task_type=\"classification\",\n",
    "        input_kind=\"tabular\",\n",
    "        feature_names=feature_names,\n",
    "        target_names=[\"activity\"],\n",
    "        train={\"X\": X_train, \"y\": y_train},\n",
    "        val={\"X\": X_val, \"y\": y_val},\n",
    "        test={\"X\": X_test, \"y\": y_test},\n",
    "        metadata={\n",
    "            \"n_classes\": 6,\n",
    "            \"label_mapping\": {\n",
    "                0: \"WALKING\",\n",
    "                1: \"WALKING_UPSTAIRS\",\n",
    "                2: \"WALKING_DOWNSTAIRS\",\n",
    "                3: \"SITTING\",\n",
    "                4: \"STANDING\",\n",
    "                5: \"LAYING\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return bundle\n",
    "\n",
    "\n",
    "def load_har_raw_sequences(data_root: Path):\n",
    "    base = data_root / \"Human Activity Recognition\" / \"UCI HAR Dataset\"\n",
    "    axes = [\n",
    "        \"body_acc_x\",\n",
    "        \"body_acc_y\",\n",
    "        \"body_acc_z\",\n",
    "        \"body_gyro_x\",\n",
    "        \"body_gyro_y\",\n",
    "        \"body_gyro_z\",\n",
    "        \"total_acc_x\",\n",
    "        \"total_acc_y\",\n",
    "        \"total_acc_z\",\n",
    "    ]\n",
    "\n",
    "    def load_split(split: str):\n",
    "        signals = []\n",
    "        for axis in axes:\n",
    "            path = base / split / \"Inertial Signals\" / f\"{axis}_{split}.txt\"\n",
    "            arr = np.loadtxt(path)\n",
    "            signals.append(arr[:, :, None])\n",
    "        X = np.concatenate(signals, axis=2).astype(np.float32)\n",
    "        y = np.loadtxt(base / split / f\"y_{split}.txt\").astype(int) - 1\n",
    "        return X, y\n",
    "\n",
    "    X_train, y_train = load_split(\"train\")\n",
    "    X_test, y_test = load_split(\"test\")\n",
    "    return X_train, y_train, X_test, y_test, axes\n",
    "\n",
    "\n",
    "def prepare_har_raw_bundle(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    val_fraction: float = 0.15,\n",
    ") -> DatasetBundle:\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_fraction, random_state=GLOBAL_CONFIG[\"seed\"])\n",
    "    train_idx, val_idx = next(splitter.split(X_train, y_train))\n",
    "    bundle = DatasetBundle(\n",
    "        name=\"HAR_raw_sequence\",\n",
    "        task_type=\"classification\",\n",
    "        input_kind=\"sequence\",\n",
    "        feature_names=[f\"axis_{i}\" for i in range(X_train.shape[2])],\n",
    "        target_names=[\"activity\"],\n",
    "        train={\"X\": X_train[train_idx], \"y\": y_train[train_idx][:, None]},\n",
    "        val={\"X\": X_train[val_idx], \"y\": y_train[val_idx][:, None]},\n",
    "        test={\"X\": X_test, \"y\": y_test[:, None]},\n",
    "        metadata={\n",
    "            \"sequence_length\": X_train.shape[1],\n",
    "            \"n_channels\": X_train.shape[2],\n",
    "            \"n_classes\": 6,\n",
    "        },\n",
    "    )\n",
    "    return bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da5943",
   "metadata": {
    "id": "b1da5943"
   },
   "outputs": [],
   "source": [
    "def load_rossmann_frames(data_root: Path):\n",
    "    base = data_root / \"Kaggle Rossmann Store Sales\" / \"rossmann-store-sales\"\n",
    "    train_path = base / \"train.csv\"\n",
    "    test_path = base / \"test.csv\"\n",
    "    store_path = base / \"store.csv\"\n",
    "    train = pd.read_csv(train_path, parse_dates=[\"Date\"])\n",
    "    test = pd.read_csv(test_path, parse_dates=[\"Date\"])\n",
    "    store = pd.read_csv(store_path)\n",
    "    return train, test, store\n",
    "\n",
    "\n",
    "def is_promo2_active(row: pd.Series) -> int:\n",
    "    if not row.get(\"Promo2\", 0):\n",
    "        return 0\n",
    "    month = row[\"Date\"].month\n",
    "    if isinstance(row[\"PromoInterval\"], str) and row[\"PromoInterval\"]:\n",
    "        month_map = {name: idx for idx, name in enumerate([\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], start=1)}\n",
    "        promo_months = [month_map.get(m.strip(), 0) for m in row[\"PromoInterval\"].split(\",\")]\n",
    "        return int(month in promo_months)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def preprocess_rossmann(train: pd.DataFrame, store: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], str]:\n",
    "    df = train.merge(store, on=\"Store\", how=\"left\")\n",
    "    df = df[df[\"Open\"] != 0].copy()\n",
    "\n",
    "    median_distance = df[\"CompetitionDistance\"].median()\n",
    "    df[\"CompetitionDistance\"] = df[\"CompetitionDistance\"].fillna(median_distance)\n",
    "    df[\"CompetitionOpenSinceYear\"] = df[\"CompetitionOpenSinceYear\"].fillna(df[\"CompetitionOpenSinceYear\"].median())\n",
    "    df[\"CompetitionOpenSinceMonth\"] = df[\"CompetitionOpenSinceMonth\"].fillna(df[\"CompetitionOpenSinceMonth\"].median())\n",
    "    df[\"Promo2SinceWeek\"] = df[\"Promo2SinceWeek\"].fillna(0)\n",
    "    df[\"Promo2SinceYear\"] = df[\"Promo2SinceYear\"].fillna(0)\n",
    "    df[\"PromoInterval\"] = df[\"PromoInterval\"].fillna(\"\")\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"Year\"] = df[\"Date\"].dt.year\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    df[\"Day\"] = df[\"Date\"].dt.day\n",
    "    df[\"WeekOfYear\"] = df[\"Date\"].dt.isocalendar().week.astype(int)\n",
    "    df[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
    "\n",
    "    df[\"IsPromo2Month\"] = df.apply(is_promo2_active, axis=1)\n",
    "\n",
    "    state_holiday_map = {\"0\": \"None\", \"a\": \"PublicHoliday\", \"b\": \"EasterHoliday\", \"c\": \"Christmas\"}\n",
    "    df[\"StateHoliday\"] = df[\"StateHoliday\"].replace(state_holiday_map)\n",
    "\n",
    "    categorical_cols = [\"StoreType\", \"Assortment\", \"StateHoliday\", \"PromoInterval\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    df[\"CustomersLag7\"] = df.groupby(\"Store\")[\"Customers\"].shift(7)\n",
    "    df[\"SalesLag7\"] = df.groupby(\"Store\")[\"Sales\"].shift(7)\n",
    "    df[\"SalesMA14\"] = df.groupby(\"Store\")[\"Sales\"].transform(lambda s: s.rolling(14, min_periods=1).mean())\n",
    "    df[\"PromoMovingAvg\"] = df.groupby(\"Store\")[\"Promo\"].transform(lambda s: s.rolling(30, min_periods=1).mean())\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in (\"Sales\", \"Date\")]\n",
    "    target_col = \"Sales\"\n",
    "    return df, feature_cols, target_col\n",
    "\n",
    "\n",
    "def prepare_rossmann_bundle(df: pd.DataFrame, feature_cols: List[str], target_col: str) -> DatasetBundle:\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "    unique_dates = np.sort(df[\"Date\"].unique())\n",
    "    if unique_dates.size < 3:\n",
    "        raise ValueError(\"Rossmann dataset requires at least three distinct dates to form train/val/test splits.\")\n",
    "\n",
    "    train_cut_idx = max(1, int(0.8 * unique_dates.size))\n",
    "    val_cut_idx = max(train_cut_idx + 1, int(0.9 * unique_dates.size))\n",
    "    if val_cut_idx >= unique_dates.size:\n",
    "        val_cut_idx = unique_dates.size - 1\n",
    "    train_end = unique_dates[train_cut_idx]\n",
    "    val_end = unique_dates[val_cut_idx]\n",
    "\n",
    "    train_mask = df[\"Date\"] < train_end\n",
    "    val_mask = (df[\"Date\"] >= train_end) & (df[\"Date\"] < val_end)\n",
    "    test_mask = df[\"Date\"] >= val_end\n",
    "\n",
    "    train_df = df[train_mask].copy()\n",
    "    val_df = df[val_mask].copy()\n",
    "    test_df = df[test_mask].copy()\n",
    "\n",
    "    if train_df.empty or val_df.empty or test_df.empty:\n",
    "        raise ValueError(\"Rossmann split produced an empty partition; adjust quantiles or check input data.\")\n",
    "\n",
    "    X_train = train_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    y_train = train_df[target_col].to_numpy(dtype=np.float32)[:, None]\n",
    "    X_val = val_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    y_val = val_df[target_col].to_numpy(dtype=np.float32)[:, None]\n",
    "    X_test = test_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    y_test = test_df[target_col].to_numpy(dtype=np.float32)[:, None]\n",
    "\n",
    "    feature_mean = X_train.mean(axis=0, keepdims=True)\n",
    "    feature_std = X_train.std(axis=0, keepdims=True)\n",
    "    feature_std = np.where(feature_std < 1e-6, 1.0, feature_std)\n",
    "\n",
    "    target_mean = y_train.mean(axis=0, keepdims=True)\n",
    "    target_std = y_train.std(axis=0, keepdims=True)\n",
    "    target_std = np.where(target_std < 1e-6, 1.0, target_std)\n",
    "\n",
    "    def _normalize(arr: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "        return ((arr - mean) / std).astype(np.float32)\n",
    "\n",
    "    X_train = _normalize(X_train, feature_mean, feature_std)\n",
    "    X_val = _normalize(X_val, feature_mean, feature_std)\n",
    "    X_test = _normalize(X_test, feature_mean, feature_std)\n",
    "\n",
    "    y_train_norm = _normalize(y_train, target_mean, target_std)\n",
    "    y_val_norm = _normalize(y_val, target_mean, target_std)\n",
    "    y_test_norm = _normalize(y_test, target_mean, target_std)\n",
    "\n",
    "    bundle = DatasetBundle(\n",
    "        name=\"Rossmann_sales\",\n",
    "        task_type=\"regression\",\n",
    "        input_kind=\"tabular\",\n",
    "        feature_names=feature_cols,\n",
    "        target_names=[target_col],\n",
    "        train={\"X\": X_train, \"y\": y_train_norm},\n",
    "        val={\"X\": X_val, \"y\": y_val_norm},\n",
    "        test={\"X\": X_test, \"y\": y_test_norm},\n",
    "        metadata={\n",
    "            \"train_range\": [str(train_df[\"Date\"].min()), str(train_df[\"Date\"].max())],\n",
    "            \"val_range\": [str(val_df[\"Date\"].min()), str(val_df[\"Date\"].max())],\n",
    "            \"test_range\": [str(test_df[\"Date\"].min()), str(test_df[\"Date\"].max())],\n",
    "            \"feature_scaler\": {\n",
    "                \"mean\": feature_mean.flatten().astype(np.float32).tolist(),\n",
    "                \"std\": feature_std.flatten().astype(np.float32).tolist(),\n",
    "            },\n",
    "            \"target_scaler\": {\n",
    "                \"mean\": target_mean.flatten().astype(np.float32).tolist(),\n",
    "                \"std\": target_std.flatten().astype(np.float32).tolist(),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bcf91",
   "metadata": {
    "id": "b83bcf91"
   },
   "outputs": [],
   "source": [
    "def load_beijing_stations(data_root: Path) -> Dict[str, pd.DataFrame]:\n",
    "    base = data_root / \"Beijing Air Quality\"\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Beijing Air Quality directory not found at {base}\")\n",
    "    stations: Dict[str, pd.DataFrame] = {}\n",
    "    for csv_path in base.glob(\"PRSA_Data_*.csv\"):\n",
    "        station_name = csv_path.stem.replace(\"PRSA_Data_\", \"\")\n",
    "        print(f\"Loading Beijing station {station_name}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df[\"datetime\"] = pd.to_datetime(\n",
    "            df[[\"year\", \"month\", \"day\", \"hour\"]].rename(columns=str)\n",
    "        )\n",
    "        df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "        if \"No\" in df.columns:\n",
    "            df = df.drop(columns=[\"No\"])\n",
    "        stations[station_name] = df\n",
    "    return stations\n",
    "\n",
    "\n",
    "def preprocess_beijing_station(df: pd.DataFrame, target_col: str = \"PM2.5\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = df.copy()\n",
    "    pollutant_cols = [\"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\", \"O3\"]\n",
    "    meteorology_cols = [\"PRES\", \"DEWP\", \"TEMP\", \"RAIN\", \"WSPM\"]\n",
    "    for col in pollutant_cols + meteorology_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    mask = df[pollutant_cols + meteorology_cols].isna()\n",
    "    df[pollutant_cols + meteorology_cols] = df[pollutant_cols + meteorology_cols].interpolate(limit=6, limit_direction=\"both\")\n",
    "    df[pollutant_cols + meteorology_cols] = df[pollutant_cols + meteorology_cols].ffill().bfill()\n",
    "\n",
    "    calendar = pd.DataFrame(\n",
    "        {\n",
    "            \"hour\": df[\"datetime\"].dt.hour,\n",
    "            \"dow\": df[\"datetime\"].dt.dayofweek,\n",
    "            \"month\": df[\"datetime\"].dt.month,\n",
    "        }\n",
    "    )\n",
    "    calendar[\"hour_sin\"] = np.sin(2 * np.pi * calendar[\"hour\"] / 24.0)\n",
    "    calendar[\"hour_cos\"] = np.cos(2 * np.pi * calendar[\"hour\"] / 24.0)\n",
    "    calendar[\"dow_sin\"] = np.sin(2 * np.pi * calendar[\"dow\"] / 7.0)\n",
    "    calendar[\"dow_cos\"] = np.cos(2 * np.pi * calendar[\"dow\"] / 7.0)\n",
    "    calendar[\"month_sin\"] = np.sin(2 * np.pi * calendar[\"month\"] / 12.0)\n",
    "    calendar[\"month_cos\"] = np.cos(2 * np.pi * calendar[\"month\"] / 12.0)\n",
    "\n",
    "    feature_frame = pd.concat(\n",
    "        [df[[\"datetime\", target_col]], df[pollutant_cols + meteorology_cols], calendar],\n",
    "        axis=1,\n",
    "    )\n",
    "    mask_frame = mask.astype(np.float32)\n",
    "    mask_frame.columns = [f\"{col}_mask\" for col in mask_frame.columns]\n",
    "    feature_frame = pd.concat([feature_frame, mask_frame], axis=1)\n",
    "    return feature_frame, mask_frame\n",
    "\n",
    "\n",
    "def build_temporal_windows(frame, target_col, feature_cols, context, horizon, drop_na=True):\n",
    "    \"\"\"\n",
    "    Returns (windows, targets, indices) where:\n",
    "      - windows: list/array of shape (n_windows, context, n_features)\n",
    "      - targets: list/array of target values aligned at idx+horizon\n",
    "      - indices: original indices of the window end (optional)\n",
    "    This robustly handles scalar/array targets and missing values.\n",
    "    \"\"\"\n",
    "    values = frame[feature_cols].to_numpy(dtype=np.float32)\n",
    "    targets = frame[target_col].to_numpy(dtype=np.float32)\n",
    "    windows = []\n",
    "    target_list = []\n",
    "    idxs = []\n",
    "\n",
    "    n = len(values)\n",
    "    for idx in range(context, n - horizon):\n",
    "        window = values[idx - context : idx]\n",
    "        target = targets[idx + horizon]\n",
    "\n",
    "        if drop_na:\n",
    "            # Use pd.isna then np.any so this works if `target` is scalar or array-like\n",
    "            if np.any(pd.isna(window)) or np.any(pd.isna(target)):\n",
    "                continue\n",
    "\n",
    "        windows.append(window)\n",
    "        target_list.append(target)\n",
    "        idxs.append(idx)\n",
    "\n",
    "    X = np.stack(windows).astype(np.float32) if windows else np.empty((0, context, values.shape[1]), dtype=np.float32)\n",
    "    y = np.array(target_list, dtype=np.float32)\n",
    "    return X, y, np.array(idxs)\n",
    "\n",
    "\n",
    "def assemble_beijing_cross_station_bundle(\n",
    "    stations: Dict[str, pd.DataFrame],\n",
    "    train_stations: List[str],\n",
    "    val_station: str,\n",
    "    test_station: str,\n",
    "    target: str = \"PM2.5\",\n",
    "    context: int = 24,\n",
    "    horizon: int = 6,\n",
    ") -> DatasetBundle:\n",
    "    feature_frames: Dict[str, pd.DataFrame] = {}\n",
    "    feature_cols: Optional[List[str]] = None\n",
    "    for name, df in stations.items():\n",
    "        features, _ = preprocess_beijing_station(df, target_col=target)\n",
    "        feature_frames[name] = features\n",
    "        if feature_cols is None:\n",
    "            feature_cols = [col for col in features.columns if col not in (\"datetime\", target)]\n",
    "    assert feature_cols is not None\n",
    "\n",
    "    def collect(names: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        arrays = []\n",
    "        targets = []\n",
    "        for station_name in names:\n",
    "            frame = feature_frames[station_name]\n",
    "            X, y, _ = build_temporal_windows(frame, target, feature_cols, context, horizon)\n",
    "            arrays.append(X)\n",
    "            targets.append(y)\n",
    "        if arrays:\n",
    "            X_all = np.concatenate(arrays, axis=0).astype(np.float32)\n",
    "            y_all = np.concatenate(targets, axis=0)\n",
    "            if y_all.ndim == 1:\n",
    "                y_all = y_all[:, None]\n",
    "            else:\n",
    "                y_all = y_all.reshape(y_all.shape[0], -1)\n",
    "            y_all = y_all.astype(np.float32)\n",
    "        else:\n",
    "            X_all = np.empty((0, context, len(feature_cols)), dtype=np.float32)\n",
    "            y_all = np.empty((0, 1), dtype=np.float32)\n",
    "        return X_all, y_all\n",
    "\n",
    "    X_train, y_train = collect(train_stations)\n",
    "    X_val, y_val = collect([val_station])\n",
    "    X_test, y_test = collect([test_station])\n",
    "\n",
    "    bundle = DatasetBundle(\n",
    "        name=f\"Beijing_PM25_{context}h_ctx_{horizon}h_horizon\",\n",
    "        task_type=\"regression\",\n",
    "        input_kind=\"sequence\",\n",
    "        feature_names=feature_cols,\n",
    "        target_names=[target],\n",
    "        train={\"X\": X_train, \"y\": y_train},\n",
    "        val={\"X\": X_val, \"y\": y_val},\n",
    "        test={\"X\": X_test, \"y\": y_test},\n",
    "        metadata={\n",
    "            \"context_hours\": context,\n",
    "            \"horizon_hours\": horizon,\n",
    "            \"train_stations\": train_stations,\n",
    "            \"val_station\": val_station,\n",
    "            \"test_station\": test_station,\n",
    "        },\n",
    "    )\n",
    "    return bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367c9e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5367c9e2",
    "outputId": "9a6cd544-52df-4a93-c16f-b6b5797c3abe"
   },
   "outputs": [],
   "source": [
    "SEED = int(os.getenv(\"PSANN_GLOBAL_SEED\", \"2025\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "GLOBAL_CONFIG: Dict[str, Any] = {\n",
    "    \"seed\": SEED,\n",
    "    \"device\": DEVICE,\n",
    "    \"default_epochs\": 100,\n",
    "    \"default_lr\": 1e-3,\n",
    "    \"default_weight_decay\": 0.0,\n",
    "    \"default_batch_size\": 256,\n",
    "    \"max_time_minutes\": 5.0,\n",
    "    \"num_workers\": 2 if DEVICE.type == \"cuda\" else 0,\n",
    "    \"label_smoothing\": 0.05,\n",
    "    \"results_root\": RESULTS_ROOT,\n",
    "    \"figure_root\": FIGURE_ROOT,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025e56a",
   "metadata": {
    "id": "8025e56a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "\n",
    "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "\n",
    "def smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + 1e-8) / 2.0\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / denom))\n",
    "\n",
    "\n",
    "def r2_score_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return float(1 - ss_res / ss_tot) if ss_tot != 0 else float('nan')\n",
    "\n",
    "\n",
    "def mase(y_true: np.ndarray, y_pred: np.ndarray, seasonal_period: int = 1) -> float:\n",
    "    if len(y_true) <= seasonal_period:\n",
    "        return float('nan')\n",
    "    naive = np.mean(np.abs(np.diff(y_true, n=seasonal_period)))\n",
    "    return float(np.mean(np.abs(y_true - y_pred)) / (naive + 1e-8))\n",
    "\n",
    "\n",
    "def expected_calibration_error(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15) -> float:\n",
    "    confidences = probs.max(axis=1)\n",
    "    predictions = probs.argmax(axis=1)\n",
    "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences >= bin_edges[i]) & (confidences < bin_edges[i + 1])\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        bin_acc = np.mean(predictions[mask] == y_true[mask])\n",
    "        bin_conf = np.mean(confidences[mask])\n",
    "        ece += np.abs(bin_acc - bin_conf) * np.mean(mask)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def classification_metrics(y_true: np.ndarray, logits: np.ndarray, average: str = 'macro') -> Dict[str, float]:\n",
    "    probs = torch.softmax(torch.from_numpy(logits), dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=1)\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy_score(y_true, preds)),\n",
    "        'f1_macro': float(f1_score(y_true, preds, average=average)),\n",
    "        'nll': float(log_loss(y_true, probs, labels=list(range(probs.shape[1])))),\n",
    "    }\n",
    "    metrics['ece'] = expected_calibration_error(probs, y_true, n_bins=15)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _prepare_regression_arrays(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_true.ndim > 2:\n",
    "        y_true = y_true.reshape(y_true.shape[0], -1)\n",
    "    if y_pred.ndim > 2:\n",
    "        y_pred = y_pred.reshape(y_pred.shape[0], -1)\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true[:, None]\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred[:, None]\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f'Regression metric shape mismatch: {y_true.shape} vs {y_pred.shape}')\n",
    "    return y_true.astype(np.float64), y_pred.astype(np.float64)\n",
    "\n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray, seasonal_period: int = 1) -> Dict[str, float]:\n",
    "    y_true, y_pred = _prepare_regression_arrays(y_true, y_pred)\n",
    "    rmse_vals = []\n",
    "    mae_vals = []\n",
    "    smape_vals = []\n",
    "    r2_vals = []\n",
    "    mase_vals = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        yt = y_true[:, i]\n",
    "        yp = y_pred[:, i]\n",
    "        rmse_vals.append(rmse(yt, yp))\n",
    "        mae_vals.append(mae(yt, yp))\n",
    "        smape_vals.append(smape(yt, yp))\n",
    "        r2_vals.append(r2_score_np(yt, yp))\n",
    "        mase_vals.append(mase(yt, yp, seasonal_period=seasonal_period))\n",
    "    return {\n",
    "        'rmse': float(np.mean(rmse_vals)),\n",
    "        'mae': float(np.mean(mae_vals)),\n",
    "        'smape': float(np.mean(smape_vals)),\n",
    "        'r2': float(np.mean(r2_vals)),\n",
    "        'mase': float(np.mean(mase_vals)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff9d3d",
   "metadata": {
    "id": "d7ff9d3d"
   },
   "outputs": [],
   "source": [
    "def build_dataloader(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    task_type: Literal[\"regression\", \"classification\", \"multitask\"] = \"regression\",\n",
    "    drop_last: bool = False,\n",
    ") -> DataLoader:\n",
    "    X_tensor = torch.from_numpy(X).float()\n",
    "    if task_type == \"classification\":\n",
    "        y_tensor = torch.from_numpy(y.squeeze()).long()\n",
    "    else:\n",
    "        y_tensor = torch.from_numpy(y.astype(np.float32))\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=GLOBAL_CONFIG[\"num_workers\"],\n",
    "        pin_memory=(DEVICE.type == \"cuda\"),\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.end = time.perf_counter()\n",
    "\n",
    "    @property\n",
    "    def elapsed(self) -> float:\n",
    "        return getattr(self, \"end\", time.perf_counter()) - getattr(self, \"start\", time.perf_counter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b3093",
   "metadata": {
    "id": "c32b3093"
   },
   "outputs": [],
   "source": [
    "def coerce_decimal(series: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        return series\n",
    "    as_str = series.astype(str).str.replace(\" \", \"\")\n",
    "    as_str = as_str.replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "    as_str = as_str.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(as_str, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def coerce_datetime(series: pd.Series) -> pd.Series:\n",
    "    as_str = series.astype(str).str.strip()\n",
    "    as_str = as_str.replace({\"nan\": np.nan, \"NaT\": np.nan})\n",
    "    as_str = as_str.str.replace(\",\", \".\", n=1, regex=False)\n",
    "    return pd.to_datetime(as_str, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def ensure_float(df: pd.DataFrame, columns: Iterable[str]) -> pd.DataFrame:\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = coerce_decimal(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, columns: Iterable[str]) -> pd.DataFrame:\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = coerce_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_calendar_features(frame: pd.DataFrame, timestamp_col: str) -> pd.DataFrame:\n",
    "    ts = pd.to_datetime(frame[timestamp_col])\n",
    "    frame[f\"{timestamp_col}_year\"] = ts.dt.year\n",
    "    frame[f\"{timestamp_col}_month\"] = ts.dt.month\n",
    "    frame[f\"{timestamp_col}_day\"] = ts.dt.day\n",
    "    frame[f\"{timestamp_col}_hour\"] = ts.dt.hour\n",
    "    frame[f\"{timestamp_col}_dow\"] = ts.dt.dayofweek\n",
    "    frame[f\"{timestamp_col}_week\"] = ts.dt.isocalendar().week.astype(int)\n",
    "    frame[f\"{timestamp_col}_dayofyear\"] = ts.dt.dayofyear\n",
    "    frame[f\"{timestamp_col}_sin_hour\"] = np.sin(2 * np.pi * frame[f\"{timestamp_col}_hour\"] / 24.0)\n",
    "    frame[f\"{timestamp_col}_cos_hour\"] = np.cos(2 * np.pi * frame[f\"{timestamp_col}_hour\"] / 24.0)\n",
    "    frame[f\"{timestamp_col}_sin_dayofyear\"] = np.sin(2 * np.pi * frame[f\"{timestamp_col}_dayofyear\"] / 365.25)\n",
    "    frame[f\"{timestamp_col}_cos_dayofyear\"] = np.cos(2 * np.pi * frame[f\"{timestamp_col}_dayofyear\"] / 365.25)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def train_val_test_split_by_time(df: pd.DataFrame, time_col: str, train_end: str, val_end: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    ts = pd.to_datetime(df[time_col])\n",
    "    train_mask = ts < pd.to_datetime(train_end)\n",
    "    val_mask = (ts >= pd.to_datetime(train_end)) & (ts < pd.to_datetime(val_end))\n",
    "    test_mask = ts >= pd.to_datetime(val_end)\n",
    "    return df[train_mask].copy(), df[val_mask].copy(), df[test_mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bd4b5",
   "metadata": {
    "id": "3e7bd4b5"
   },
   "outputs": [],
   "source": [
    "EAF_TABLES = [\n",
    "    \"eaf_temp\",\n",
    "    \"eaf_gaslance_mat\",\n",
    "    \"inj_mat\",\n",
    "    \"eaf_transformer\",\n",
    "    \"eaf_added_materials\",\n",
    "    \"basket_charged\",\n",
    "    \"lf_added_materials\",\n",
    "    \"lf_initial_chemical_measurements\",\n",
    "    \"eaf_final_chemical_measurements\",\n",
    "    \"ladle_tapping\",\n",
    "]\n",
    "\n",
    "\n",
    "def parse_duration_minutes(value: Any) -> Optional[float]:\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return np.nan\n",
    "    s = s.replace(\" \", \"\")\n",
    "    if \":\" not in s:\n",
    "        return coerce_decimal(pd.Series([s])).iloc[0]\n",
    "    parts = s.split(\":\")\n",
    "    try:\n",
    "        hours = float(parts[0])\n",
    "        minutes = float(parts[1])\n",
    "        return hours * 60.0 + minutes\n",
    "    except Exception:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55a25b",
   "metadata": {
    "id": "7a55a25b"
   },
   "outputs": [],
   "source": [
    "def load_eaf_tables(data_root: Path) -> Dict[str, pd.DataFrame]:\n",
    "    candidate_dirs = [\n",
    "        data_root / \"Industrial Data from the Electric Arc Furnace\",\n",
    "        data_root / \"Industrial_Data_from_the_Electric_Arc_Furnace\",\n",
    "    ]\n",
    "    base = next((path for path in candidate_dirs if path.exists()), None)\n",
    "\n",
    "    table_paths: Dict[str, Path] = {}\n",
    "    if base is not None:\n",
    "        table_paths = {name: base / f\"{name}.csv\" for name in EAF_TABLES}\n",
    "    else:\n",
    "        print(f\"[WARN] Expected EAF directory missing under {data_root}. Falling back to glob search.\")\n",
    "        for name in EAF_TABLES:\n",
    "            path = next(\n",
    "                (\n",
    "                    candidate\n",
    "                    for candidate in [\n",
    "                        data_root / f\"{name}.csv\",\n",
    "                        data_root / f\"{name.upper()}.csv\",\n",
    "                    ]\n",
    "                    if candidate.exists()\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if path is None:\n",
    "                matches = list(data_root.rglob(f\"{name}.csv\"))\n",
    "                if matches:\n",
    "                    path = matches[0]\n",
    "            table_paths[name] = path\n",
    "\n",
    "    missing = [name for name, path in table_paths.items() if path is None or not path.exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(\n",
    "            \"Unable to locate EAF tables: \" + \", \".join(missing) + f\". Ensure the CSV files are present under {data_root}.\"\n",
    "        )\n",
    "\n",
    "    tables: Dict[str, pd.DataFrame] = {}\n",
    "    for name, path in table_paths.items():\n",
    "        if path is None:\n",
    "            continue\n",
    "        print(f\"Loading {name} from {path}...\")\n",
    "        if name in {\"eaf_gaslance_mat\", \"inj_mat\"}:\n",
    "            df = pd.read_csv(path, dtype=str)\n",
    "            df = ensure_datetime(df, [\"REVTIME\"])\n",
    "            if \"DATETIME\" not in df.columns and \"REVTIME\" in df.columns:\n",
    "                df[\"DATETIME\"] = df[\"REVTIME\"]\n",
    "            numeric_cols = [c for c in df.columns if c not in (\"REVTIME\", \"HEATID\", \"DATETIME\")]\n",
    "            df = ensure_float(df, numeric_cols)\n",
    "        elif name == \"eaf_temp\":\n",
    "            df = pd.read_csv(path)\n",
    "            df = ensure_datetime(df, [\"DATETIME\"])\n",
    "            numeric_cols = [c for c in df.columns if c not in (\"HEATID\", \"DATETIME\")]\n",
    "            df = ensure_float(df, numeric_cols)\n",
    "        elif name == \"eaf_transformer\":\n",
    "            df = pd.read_csv(path, dtype=str)\n",
    "            df = ensure_datetime(df, [\"STARTTIME\"])\n",
    "            if \"DATETIME\" not in df.columns and \"STARTTIME\" in df.columns:\n",
    "                df[\"DATETIME\"] = df[\"STARTTIME\"]\n",
    "            df[\"DURATION_MIN\"] = df[\"DURATION\"].astype(str).str.replace(\" \", \"\")\n",
    "            df[\"DURATION_MIN\"] = df[\"DURATION_MIN\"].apply(parse_duration_minutes)\n",
    "            df = ensure_float(df, [\"DURATION_MIN\", \"MW\"])\n",
    "        else:\n",
    "            df = pd.read_csv(path, dtype=str)\n",
    "            datetime_cols = [c for c in df.columns if \"DATE\" in c.upper() or \"TIME\" in c.upper()]\n",
    "            if datetime_cols:\n",
    "                df = ensure_datetime(df, datetime_cols)\n",
    "                if \"DATETIME\" not in df.columns:\n",
    "                    df[\"DATETIME\"] = df[datetime_cols[0]]\n",
    "            numeric_cols = [\n",
    "                c\n",
    "                for c in df.columns\n",
    "                if c not in datetime_cols and c not in (\"HEATID\", \"RECID\", \"POSITIONROW\", \"DATETIME\")\n",
    "            ]\n",
    "            df = ensure_float(df, numeric_cols)\n",
    "        tables[name] = df\n",
    "    return tables\n",
    "\n",
    "\n",
    "def compute_heatwise_aggregates(df: pd.DataFrame, heat_col: str, aggregations: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    grouped = df.groupby(heat_col).agg(aggregations)\n",
    "    grouped.columns = [f\"{col}_{agg}\" for col, agg in grouped.columns]\n",
    "    grouped = grouped.reset_index()\n",
    "    return grouped\n",
    "\n",
    "# Fixed vectorized merge_asof_multikey (searchsorted-based, dtype-safe assignments)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "def merge_asof_multikey(\n",
    "    left: pd.DataFrame,\n",
    "    right: pd.DataFrame,\n",
    "    *,\n",
    "    on: str,\n",
    "    by: str,\n",
    "    suffix: str = \"rhs\",\n",
    "    tolerance: Optional[pd.Timedelta] = None,\n",
    "    direction: str = \"backward\",\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorized as-of merge on (by, on) using numpy.searchsorted on structured keys.\n",
    "    Supports direction='backward' (most common for telemetry alignment).\n",
    "    Returns left with right's non-key columns suffixed by _{suffix}.\n",
    "    \"\"\"\n",
    "    if right is None or len(right) == 0:\n",
    "        return left.copy()\n",
    "    if direction != \"backward\":\n",
    "        raise NotImplementedError(\"Only 'backward' direction supported in this implementation\")\n",
    "\n",
    "    # Basic checks\n",
    "    if on not in left.columns or on not in right.columns or by not in left.columns or by not in right.columns:\n",
    "        raise KeyError(f\"Both frames must contain columns '{by}' and '{on}'\")\n",
    "\n",
    "    # Work on copies\n",
    "    L = left.copy()\n",
    "    R = right.copy()\n",
    "\n",
    "    # Mask and filter rows missing keys (we'll reattach them at the end)\n",
    "    mask_L_valid = L[on].notna() & L[by].notna()\n",
    "    mask_R_valid = R[on].notna() & R[by].notna()\n",
    "    L_valid = L.loc[mask_L_valid].copy()\n",
    "    R_valid = R.loc[mask_R_valid].copy()\n",
    "\n",
    "    # Coerce datetimes\n",
    "    L_valid[on] = pd.to_datetime(L_valid[on], errors=\"coerce\")\n",
    "    R_valid[on] = pd.to_datetime(R_valid[on], errors=\"coerce\")\n",
    "    L_valid = L_valid[L_valid[on].notna()]\n",
    "    R_valid = R_valid[R_valid[on].notna()]\n",
    "\n",
    "    # If no valid rows remain on left, return original left with NaNs for merge cols\n",
    "    if L_valid.empty:\n",
    "        merged = L.copy()\n",
    "        merge_cols = [c for c in R.columns if c not in (by, on)]\n",
    "        for c in merge_cols:\n",
    "            merged[f\"{c}_{suffix}\"] = np.nan\n",
    "        return merged\n",
    "\n",
    "    # Dedupe right on (by, on) keeping last measurement (reduces search space)\n",
    "    R_valid = R_valid.sort_values([by, on], kind=\"mergesort\").drop_duplicates(subset=[by, on], keep=\"last\")\n",
    "\n",
    "    # Factorize right groups to compact integer ids\n",
    "    right_labels = pd.unique(R_valid[by].astype(object))  # preserve order\n",
    "    group_to_id = {val: i for i, val in enumerate(right_labels)}\n",
    "    # Map right group ids\n",
    "    right_group_ids = np.array([group_to_id[v] for v in R_valid[by].astype(object)], dtype=np.int32)\n",
    "\n",
    "    # Map left group ids; groups not in right get -1\n",
    "    left_group_values = L_valid[by].astype(object).values\n",
    "    left_group_ids = np.array([group_to_id.get(v, -1) for v in left_group_values], dtype=np.int32)\n",
    "\n",
    "    # Convert times to int64 ns\n",
    "    left_times_ns = L_valid[on].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "    right_times_ns = R_valid[on].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "\n",
    "    # Build structured keys and sort by (group,time)\n",
    "    key_dtype = np.dtype([(\"g\", np.int32), (\"t\", np.int64)])\n",
    "    right_keys = np.empty(len(right_group_ids), dtype=key_dtype)\n",
    "    right_keys[\"g\"] = right_group_ids\n",
    "    right_keys[\"t\"] = right_times_ns\n",
    "    order = np.argsort(right_keys, order=(\"g\", \"t\"))\n",
    "    right_keys_sorted = right_keys[order]\n",
    "    R_sorted = R_valid.reset_index(drop=True).iloc[order].reset_index(drop=True)\n",
    "    right_times_sorted = right_keys_sorted[\"t\"]\n",
    "\n",
    "    # Left keys structured array (same dtype)\n",
    "    left_keys = np.empty(len(left_group_ids), dtype=key_dtype)\n",
    "    left_keys[\"g\"] = left_group_ids\n",
    "    left_keys[\"t\"] = left_times_ns\n",
    "\n",
    "    # Searchsorted to find previous (backward) right index for each left key\n",
    "    idxs = np.searchsorted(right_keys_sorted, left_keys, side=\"right\") - 1\n",
    "\n",
    "    # Initialize keep_mask (default False)\n",
    "    keep_mask = np.zeros(len(idxs), dtype=bool)\n",
    "\n",
    "    # valid where idxs >= 0\n",
    "    valid_mask = idxs >= 0\n",
    "    if valid_mask.any():\n",
    "        # Check matched group's id equals left group id (otherwise it's from a different group)\n",
    "        matched_group_ids = right_keys_sorted[\"g\"][idxs[valid_mask]]\n",
    "        left_group_ids_valid = left_keys[\"g\"][valid_mask]\n",
    "        same_group = matched_group_ids == left_group_ids_valid\n",
    "        # Set keep_mask True only where same_group is True\n",
    "        keep_mask[np.flatnonzero(valid_mask)[same_group]] = True\n",
    "\n",
    "    # Apply tolerance if provided (left_time - matched_right_time must be <= tol and >=0)\n",
    "    if tolerance is not None and keep_mask.any():\n",
    "        tol_ns = int(pd.to_timedelta(tolerance).to_timedelta64().astype(\"timedelta64[ns]\") / np.timedelta64(1, \"ns\"))\n",
    "        kept_positions = np.flatnonzero(keep_mask)\n",
    "        matched_right_times = right_times_sorted[idxs[kept_positions]]\n",
    "        left_times_for_kept = left_keys[\"t\"][kept_positions]\n",
    "        diffs = left_times_for_kept - matched_right_times\n",
    "        tol_ok = (diffs >= 0) & (diffs <= tol_ns)\n",
    "        # Zero out positions violating tolerance\n",
    "        if not np.all(tol_ok):\n",
    "            keep_mask[kept_positions[~tol_ok]] = False\n",
    "\n",
    "    # Prepare result skeleton using proper dtypes (avoid assigning arrays of incompatible dtype)\n",
    "    merge_cols = [c for c in R_sorted.columns if c not in (by, on)]\n",
    "    result = L_valid.copy()\n",
    "    for c in merge_cols:\n",
    "        src_dtype = R_sorted[c].dtype\n",
    "        try:\n",
    "            result[f\"{c}_{suffix}\"] = pd.Series(index=result.index, dtype=src_dtype)\n",
    "        except Exception:\n",
    "            result[f\"{c}_{suffix}\"] = pd.Series(index=result.index, dtype=\"object\")\n",
    "\n",
    "    # Fill merged columns for kept matches\n",
    "    kept_positions = np.flatnonzero(keep_mask)\n",
    "    if kept_positions.size:\n",
    "        matched_idxs = idxs[kept_positions]  # indices into R_sorted\n",
    "        for col in merge_cols:\n",
    "            vals = R_sorted.iloc[matched_idxs][col].values\n",
    "            s = pd.Series(vals, index=result.index[kept_positions])\n",
    "            # If target dtype is datetime, ensure series is datetime\n",
    "            if np.issubdtype(result[f\"{col}_{suffix}\"].dtype, np.datetime64):\n",
    "                s = pd.to_datetime(s)\n",
    "            result.loc[result.index[kept_positions], f\"{col}_{suffix}\"] = s\n",
    "\n",
    "    # Rows that were invalid (no match) remain NaN in merged cols\n",
    "\n",
    "    # Reattach left rows that were dropped due to missing keys\n",
    "    if mask_L_valid.sum() != len(L):\n",
    "        dropped = L.loc[~mask_L_valid].copy()\n",
    "        for c in merge_cols:\n",
    "            dropped[f\"{c}_{suffix}\"] = np.nan\n",
    "        combined = pd.concat([result, dropped]).loc[L.index]\n",
    "    else:\n",
    "        combined = result\n",
    "\n",
    "    # Reindex to original left.index to preserve order\n",
    "    combined = combined.reindex(left.index)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfea05",
   "metadata": {
    "id": "12cfea05"
   },
   "outputs": [],
   "source": [
    "def prepare_eaf_temp_and_o2_bundles(\n",
    "    tables: Dict[str, pd.DataFrame],\n",
    "    history_lags: List[int] = (1, 2, 3, 6),\n",
    "    horizon: int = 1,\n",
    ") -> Tuple[DatasetBundle, DatasetBundle]:\n",
    "    temp = tables[\"eaf_temp\"].copy()\n",
    "    temp[\"DATETIME\"] = pd.to_datetime(temp[\"DATETIME\"])\n",
    "    temp = temp.sort_values([\"HEATID\", \"DATETIME\"]).reset_index(drop=True)\n",
    "    temp = temp.drop_duplicates(subset=[\"HEATID\", \"DATETIME\"], keep=\"last\")\n",
    "\n",
    "    for lag in history_lags:\n",
    "        temp[f\"TEMP_lag_{lag}\"] = temp.groupby(\"HEATID\")[\"TEMP\"].shift(lag)\n",
    "        temp[f\"VALO2_lag_{lag}\"] = temp.groupby(\"HEATID\")[\"VALO2_PPM\"].shift(lag)\n",
    "\n",
    "    temp[\"TEMP_target\"] = temp.groupby(\"HEATID\")[\"TEMP\"].shift(-horizon)\n",
    "    temp[\"VALO2_target\"] = temp.groupby(\"HEATID\")[\"VALO2_PPM\"].shift(-horizon)\n",
    "\n",
    "    temp[\"HEAT_START\"] = temp.groupby(\"HEATID\")[\"DATETIME\"].transform(\"min\")\n",
    "    temp[\"minutes_from_heat_start\"] = (temp[\"DATETIME\"] - temp[\"HEAT_START\"]).dt.total_seconds() / 60.0\n",
    "    temp[\"sample_index\"] = temp.groupby(\"HEATID\").cumcount()\n",
    "    temp[\"minutes_between_samples\"] = temp.groupby(\"HEATID\")[\"DATETIME\"].diff().dt.total_seconds().fillna(0.0) / 60.0\n",
    "\n",
    "    gas = tables.get(\"eaf_gaslance_mat\")\n",
    "    if gas is not None and not gas.empty:\n",
    "        gas = gas.sort_values([\"HEATID\", \"REVTIME\"])\n",
    "        for col in [\"O2_AMOUNT\", \"GAS_AMOUNT\", \"O2_FLOW\", \"GAS_FLOW\"]:\n",
    "            if col in gas.columns:\n",
    "                gas[f\"{col}_cum\"] = gas.groupby(\"HEATID\")[col].cumsum()\n",
    "        temp = merge_asof_multikey(\n",
    "            temp,\n",
    "            gas,\n",
    "            on=\"DATETIME\",\n",
    "            by=\"HEATID\",\n",
    "            suffix=\"gas\",\n",
    "            tolerance=pd.Timedelta(minutes=30),\n",
    "        )\n",
    "\n",
    "    inj = tables.get(\"inj_mat\")\n",
    "    if inj is not None and not inj.empty:\n",
    "        inj = inj.sort_values([\"HEATID\", \"REVTIME\"])\n",
    "        for col in [\"INJ_AMOUNT_CARBON\", \"INJ_FLOW_CARBON\"]:\n",
    "            if col in inj.columns:\n",
    "                inj[f\"{col}_cum\"] = inj.groupby(\"HEATID\")[col].cumsum()\n",
    "        temp = merge_asof_multikey(\n",
    "            temp,\n",
    "            inj,\n",
    "            on=\"DATETIME\",\n",
    "            by=\"HEATID\",\n",
    "            suffix=\"inj\",\n",
    "            tolerance=pd.Timedelta(minutes=30),\n",
    "        )\n",
    "\n",
    "    transformer = tables.get(\"eaf_transformer\")\n",
    "    if transformer is not None and not transformer.empty:\n",
    "        transformer = transformer.sort_values([\"HEATID\", \"STARTTIME\"])\n",
    "        temp = merge_asof_multikey(\n",
    "            temp,\n",
    "            transformer,\n",
    "            on=\"DATETIME\",\n",
    "            by=\"HEATID\",\n",
    "            suffix=\"xfmr\",\n",
    "            tolerance=pd.Timedelta(hours=2),\n",
    "        )\n",
    "\n",
    "    temp = add_calendar_features(temp, \"DATETIME\")\n",
    "    feature_cols = [\n",
    "        col\n",
    "        for col in temp.columns\n",
    "        if col\n",
    "        not in {\n",
    "            \"TEMP\",\n",
    "            \"VALO2_PPM\",\n",
    "            \"TEMP_target\",\n",
    "            \"VALO2_target\",\n",
    "            \"HEATID\",\n",
    "            \"HEAT_START\",\n",
    "            \"DATETIME\",\n",
    "        }\n",
    "        and not col.endswith(\"_xfmr\")\n",
    "    ]\n",
    "    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(temp[c])]\n",
    "\n",
    "    temp[feature_cols] = temp[feature_cols].astype(np.float32)\n",
    "\n",
    "    temp = temp.dropna(subset=feature_cols + [\"TEMP_target\", \"VALO2_target\"]).reset_index(drop=True)\n",
    "\n",
    "    temp[\"year\"] = temp[\"DATETIME\"].dt.year\n",
    "    heat_year = temp.groupby(\"HEATID\")[\"year\"].max().reset_index().rename(columns={\"year\": \"heat_year\"})\n",
    "    temp = temp.merge(heat_year, on=\"HEATID\", how=\"left\")\n",
    "\n",
    "    train_mask = temp[\"heat_year\"] <= 2016\n",
    "    val_mask = temp[\"heat_year\"] == 2017\n",
    "    test_mask = temp[\"heat_year\"] >= 2018\n",
    "\n",
    "\n",
    "    feature_mean = temp.loc[train_mask, feature_cols].mean()\n",
    "    feature_std = temp.loc[train_mask, feature_cols].std().replace(0.0, 1.0)\n",
    "    temp[feature_cols] = (temp[feature_cols] - feature_mean) / feature_std\n",
    "\n",
    "    scaler_meta = {\n",
    "        \"feature_mean\": {k: float(v) for k, v in feature_mean.items()},\n",
    "        \"feature_std\": {k: float(v) for k, v in feature_std.items()},\n",
    "    }\n",
    "\n",
    "    def build_split(mask: pd.Series) -> Dict[str, np.ndarray]:\n",
    "        X = temp.loc[mask, feature_cols].to_numpy(dtype=np.float32)\n",
    "        y_temp = temp.loc[mask, \"TEMP_target\"].to_numpy(dtype=np.float32)[:, None]\n",
    "        y_o2 = temp.loc[mask, \"VALO2_target\"].to_numpy(dtype=np.float32)[:, None]\n",
    "        return {\"X\": X, \"y_temp\": y_temp, \"y_o2\": y_o2}\n",
    "\n",
    "    train_split = build_split(train_mask)\n",
    "    val_split = build_split(val_mask)\n",
    "    test_split = build_split(test_mask)\n",
    "\n",
    "    temp_target_mean = train_split[\"y_temp\"].mean(axis=0, keepdims=True)\n",
    "    temp_target_std = train_split[\"y_temp\"].std(axis=0, keepdims=True)\n",
    "    temp_target_std = np.where(temp_target_std < 1e-6, 1.0, temp_target_std)\n",
    "\n",
    "    o2_target_mean = train_split[\"y_o2\"].mean(axis=0, keepdims=True)\n",
    "    o2_target_std = train_split[\"y_o2\"].std(axis=0, keepdims=True)\n",
    "    o2_target_std = np.where(o2_target_std < 1e-6, 1.0, o2_target_std)\n",
    "\n",
    "    def _normalize_target(split: Dict[str, np.ndarray], key: str, mean: np.ndarray, std: np.ndarray) -> None:\n",
    "        split[key] = ((split[key] - mean) / std).astype(np.float32)\n",
    "\n",
    "    for split_dict in (train_split, val_split, test_split):\n",
    "        _normalize_target(split_dict, \"y_temp\", temp_target_mean, temp_target_std)\n",
    "        _normalize_target(split_dict, \"y_o2\", o2_target_mean, o2_target_std)\n",
    "\n",
    "    temp_bundle = DatasetBundle(\n",
    "        name=\"EAF_TEMP_forecast\",\n",
    "        task_type=\"regression\",\n",
    "        input_kind=\"tabular\",\n",
    "        feature_names=feature_cols,\n",
    "        target_names=[\"TEMP_target\"],\n",
    "        train={\"X\": train_split[\"X\"], \"y\": train_split[\"y_temp\"]},\n",
    "        val={\"X\": val_split[\"X\"], \"y\": val_split[\"y_temp\"]},\n",
    "        test={\"X\": test_split[\"X\"], \"y\": test_split[\"y_temp\"]},\n",
    "        metadata={\n",
    "            \"horizon_steps\": horizon,\n",
    "            \"history_lags\": list(history_lags),\n",
    "            \"feature_source\": \"temp + gas + injection + calendar\",\n",
    "            **scaler_meta,\n",
    "            \"target_scaler\": {\n",
    "                \"mean\": float(temp_target_mean.squeeze()),\n",
    "                \"std\": float(temp_target_std.squeeze()),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    o2_bundle = DatasetBundle(\n",
    "        name=\"EAF_VALO2_forecast\",\n",
    "        task_type=\"regression\",\n",
    "        input_kind=\"tabular\",\n",
    "        feature_names=feature_cols,\n",
    "        target_names=[\"VALO2_target\"],\n",
    "        train={\"X\": train_split[\"X\"], \"y\": train_split[\"y_o2\"]},\n",
    "        val={\"X\": val_split[\"X\"], \"y\": val_split[\"y_o2\"]},\n",
    "        test={\"X\": test_split[\"X\"], \"y\": test_split[\"y_o2\"]},\n",
    "        metadata={\n",
    "            \"horizon_steps\": horizon,\n",
    "            \"history_lags\": list(history_lags),\n",
    "            \"feature_source\": \"temp + gas + injection + calendar\",\n",
    "            **scaler_meta,\n",
    "            \"target_scaler\": {\n",
    "                \"mean\": float(o2_target_mean.squeeze()),\n",
    "                \"std\": float(o2_target_std.squeeze()),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return temp_bundle, o2_bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10e15f",
   "metadata": {
    "id": "9a10e15f"
   },
   "outputs": [],
   "source": [
    "def prepare_eaf_chemistry_bundle(tables: Dict[str, pd.DataFrame]) -> DatasetBundle:\n",
    "    chem = tables[\"eaf_final_chemical_measurements\"].copy()\n",
    "    chem = ensure_datetime(chem, [\"DATETIME\"])\n",
    "    chem = chem.sort_values([\"HEATID\", \"DATETIME\"])\n",
    "    chem = chem.drop_duplicates(subset=[\"HEATID\"], keep=\"last\")\n",
    "\n",
    "    target_cols = [c for c in chem.columns if c not in (\"HEATID\", \"POSITIONROW\", \"DATETIME\")]\n",
    "    chem = ensure_float(chem, target_cols)\n",
    "\n",
    "    temp = tables[\"eaf_temp\"].copy()\n",
    "    temp = ensure_datetime(temp, [\"DATETIME\"])\n",
    "    temp = temp.sort_values([\"HEATID\", \"DATETIME\"])\n",
    "    temp[\"sample_index\"] = temp.groupby(\"HEATID\").cumcount()\n",
    "    temp = add_calendar_features(temp, \"DATETIME\")\n",
    "    temp_aggs = compute_heatwise_aggregates(\n",
    "        temp,\n",
    "        \"HEATID\",\n",
    "        {\n",
    "            \"TEMP\": [\"mean\", \"max\", \"min\", \"last\"],\n",
    "            \"VALO2_PPM\": [\"mean\", \"max\", \"last\"],\n",
    "            \"DATETIME_month\": [\"last\"],\n",
    "            \"DATETIME_hour\": [\"mean\"],\n",
    "            \"sample_index\": [\"max\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def safe_aggregates(frame: Optional[pd.DataFrame], aggregations: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "        if frame is None or frame.empty:\n",
    "            columns = [\"HEATID\"] + [f\"{feature}_{agg}\" for feature, aggs in aggregations.items() for agg in aggs]\n",
    "            return pd.DataFrame(columns=columns)\n",
    "        frame = frame.copy()\n",
    "        datetime_cols = [c for c in frame.columns if \"TIME\" in c.upper() or \"DATE\" in c.upper()]\n",
    "        if datetime_cols:\n",
    "            frame = ensure_datetime(frame, datetime_cols)\n",
    "        numeric_cols = [c for c in frame.columns if c not in (\"HEATID\", \"REVTIME\", \"STARTTIME\")]\n",
    "        frame = ensure_float(frame, numeric_cols)\n",
    "        return compute_heatwise_aggregates(frame, \"HEATID\", aggregations)\n",
    "\n",
    "    gas_aggs = safe_aggregates(\n",
    "        tables.get(\"eaf_gaslance_mat\"),\n",
    "        {\n",
    "            \"O2_AMOUNT\": [\"max\"],\n",
    "            \"GAS_AMOUNT\": [\"max\"],\n",
    "            \"O2_FLOW\": [\"mean\", \"max\"],\n",
    "            \"GAS_FLOW\": [\"mean\", \"max\"],\n",
    "        },\n",
    "    )\n",
    "    inj_aggs = safe_aggregates(\n",
    "        tables.get(\"inj_mat\"),\n",
    "        {\n",
    "            \"INJ_AMOUNT_CARBON\": [\"max\"],\n",
    "            \"INJ_FLOW_CARBON\": [\"mean\", \"max\"],\n",
    "        },\n",
    "    )\n",
    "    transformer_aggs = safe_aggregates(\n",
    "        tables.get(\"eaf_transformer\"),\n",
    "        {\n",
    "            \"MW\": [\"mean\", \"max\"],\n",
    "            \"DURATION_MIN\": [\"sum\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    features = chem[[\"HEATID\", \"DATETIME\"]].merge(temp_aggs, on=\"HEATID\", how=\"left\")\n",
    "    features = features.merge(gas_aggs, on=\"HEATID\", how=\"left\")\n",
    "    features = features.merge(inj_aggs, on=\"HEATID\", how=\"left\")\n",
    "    features = features.merge(transformer_aggs, on=\"HEATID\", how=\"left\")\n",
    "\n",
    "    numeric_feature_cols = [c for c in features.columns if c not in (\"HEATID\", \"DATETIME\")]\n",
    "    features = ensure_float(features, numeric_feature_cols)\n",
    "    features = add_calendar_features(features, \"DATETIME\")\n",
    "    feature_cols = [c for c in features.columns if c not in (\"HEATID\", \"DATETIME\")]\n",
    "\n",
    "    merged = features.merge(chem[[\"HEATID\"] + target_cols], on=\"HEATID\", how=\"inner\")\n",
    "    merged = merged.dropna(subset=feature_cols + target_cols).reset_index(drop=True)\n",
    "\n",
    "    merged[\"year\"] = pd.to_datetime(merged[\"DATETIME\"]).dt.year\n",
    "    train_mask = merged[\"year\"] <= 2016\n",
    "    val_mask = merged[\"year\"] == 2017\n",
    "    test_mask = merged[\"year\"] >= 2018\n",
    "\n",
    "    X_train = merged.loc[train_mask, feature_cols].to_numpy(dtype=np.float32)\n",
    "    y_train = merged.loc[train_mask, target_cols].to_numpy(dtype=np.float32)\n",
    "    X_val = merged.loc[val_mask, feature_cols].to_numpy(dtype=np.float32)\n",
    "    y_val = merged.loc[val_mask, target_cols].to_numpy(dtype=np.float32)\n",
    "    X_test = merged.loc[test_mask, feature_cols].to_numpy(dtype=np.float32)\n",
    "    y_test = merged.loc[test_mask, target_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if X_train.size == 0 or X_val.size == 0 or X_test.size == 0:\n",
    "        raise ValueError(\"EAF chemistry splits produced empty partitions; check year filters.\")\n",
    "\n",
    "    feature_mean = X_train.mean(axis=0, keepdims=True)\n",
    "    feature_std = X_train.std(axis=0, keepdims=True)\n",
    "    feature_std = np.where(feature_std < 1e-6, 1.0, feature_std)\n",
    "\n",
    "    target_mean = y_train.mean(axis=0, keepdims=True)\n",
    "    target_std = y_train.std(axis=0, keepdims=True)\n",
    "    target_std = np.where(target_std < 1e-6, 1.0, target_std)\n",
    "\n",
    "    def _normalize(arr: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "        return ((arr - mean) / std).astype(np.float32)\n",
    "\n",
    "    X_train = _normalize(X_train, feature_mean, feature_std)\n",
    "    X_val = _normalize(X_val, feature_mean, feature_std)\n",
    "    X_test = _normalize(X_test, feature_mean, feature_std)\n",
    "\n",
    "    y_train_norm = _normalize(y_train, target_mean, target_std)\n",
    "    y_val_norm = _normalize(y_val, target_mean, target_std)\n",
    "    y_test_norm = _normalize(y_test, target_mean, target_std)\n",
    "\n",
    "    bundle = DatasetBundle(\n",
    "        name=\"EAF_chemistry\",\n",
    "        task_type=\"regression\",\n",
    "        input_kind=\"tabular\",\n",
    "        feature_names=feature_cols,\n",
    "        target_names=target_cols,\n",
    "        train={\"X\": X_train, \"y\": y_train_norm},\n",
    "        val={\"X\": X_val, \"y\": y_val_norm},\n",
    "        test={\"X\": X_test, \"y\": y_test_norm},\n",
    "        metadata={\n",
    "            \"target_dim\": len(target_cols),\n",
    "            \"note\": \"heat-level aggregates for final composition\",\n",
    "            \"feature_scaler\": {\n",
    "                \"mean\": feature_mean.flatten().astype(np.float32).tolist(),\n",
    "                \"std\": feature_std.flatten().astype(np.float32).tolist(),\n",
    "            },\n",
    "            \"target_scaler\": {\n",
    "                \"mean\": target_mean.flatten().astype(np.float32).tolist(),\n",
    "                \"std\": target_std.flatten().astype(np.float32).tolist(),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9d6d0",
   "metadata": {
    "id": "98e9d6d0"
   },
   "outputs": [],
   "source": [
    "from psann.nn import ResidualPSANNNet\n",
    "from psann.models.wave_resnet import WaveResNet\n",
    "\n",
    "\n",
    "class IdentitySpine(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim == 3:\n",
    "            return x.reshape(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TemporalConvSpine(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        hidden_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 2,\n",
    "        depth: int = 2,\n",
    "        activation: Callable[[], nn.Module] = nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        channels = input_channels\n",
    "        for _ in range(depth):\n",
    "            layers.append(\n",
    "                nn.Conv1d(\n",
    "                    channels,\n",
    "                    hidden_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=kernel_size // 2,\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm1d(hidden_channels))\n",
    "            layers.append(activation())\n",
    "            channels = hidden_channels\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = x.transpose(1, 2)\n",
    "        z = self.net(z)\n",
    "        z = self.pool(z).squeeze(-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class TemporalAttentionSpine(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_heads: int = 1, ff_factor: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, batch_first=True, dropout=dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, ff_factor * input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_factor * input_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.norm(x)\n",
    "        attn_out, _ = self.attn(z, z, z)\n",
    "        z = z + attn_out\n",
    "        z = z + self.ff(z)\n",
    "        return z.mean(dim=1)\n",
    "\n",
    "\n",
    "class FlattenSpine(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim == 3:\n",
    "            return x.reshape(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SequencePSANNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int, ...],\n",
    "        output_dim: int,\n",
    "        *,\n",
    "        hidden_layers: int,\n",
    "        hidden_units: int,\n",
    "        spine_type: str = \"flatten\",\n",
    "        spine_params: Optional[Dict[str, Any]] = None,\n",
    "        activation_type: str = \"psann\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        spine_params = spine_params or {}\n",
    "        time_steps, channels = input_shape\n",
    "        if spine_type == \"conv\":\n",
    "            self.spine = TemporalConvSpine(\n",
    "                channels,\n",
    "                spine_params.get(\"channels\", hidden_units),\n",
    "                kernel_size=spine_params.get(\"kernel_size\", 5),\n",
    "                stride=spine_params.get(\"stride\", 2),\n",
    "                depth=spine_params.get(\"depth\", 2),\n",
    "            )\n",
    "            psann_input_dim = spine_params.get(\"channels\", hidden_units)\n",
    "        elif spine_type == \"attention\":\n",
    "            self.spine = TemporalAttentionSpine(\n",
    "                input_dim=channels,\n",
    "                num_heads=spine_params.get(\"num_heads\", 1),\n",
    "                ff_factor=spine_params.get(\"ff_factor\", 2),\n",
    "                dropout=spine_params.get(\"dropout\", 0.1),\n",
    "            )\n",
    "            psann_input_dim = channels\n",
    "        elif spine_type == \"flatten\":\n",
    "            self.spine = FlattenSpine()\n",
    "            psann_input_dim = time_steps * channels\n",
    "        else:\n",
    "            self.spine = IdentitySpine()\n",
    "            psann_input_dim = time_steps * channels\n",
    "        self.core = ResidualPSANNNet(\n",
    "            psann_input_dim,\n",
    "            output_dim,\n",
    "            hidden_layers=hidden_layers,\n",
    "            hidden_units=hidden_units,\n",
    "            hidden_width=hidden_units,\n",
    "            activation_type=activation_type,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim == 3:\n",
    "            z = self.spine(x)\n",
    "        else:\n",
    "            z = x\n",
    "        return self.core(z)\n",
    "\n",
    "\n",
    "class TabularPSANNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        *,\n",
    "        hidden_layers: int,\n",
    "        hidden_units: int,\n",
    "        activation_type: str = \"psann\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.core = ResidualPSANNNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            hidden_layers=hidden_layers,\n",
    "            hidden_units=hidden_units,\n",
    "            hidden_width=hidden_units,\n",
    "            activation_type=activation_type,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim > 2:\n",
    "            x = x.reshape(x.size(0), -1)\n",
    "        return self.core(x)\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_layers: int = 3, hidden_units: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_units\n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim > 2:\n",
    "            x = x.reshape(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class LSTMHead(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_units: int, num_layers: int, output_dim: int, bidirectional: bool = False, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_units,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        out_dim = hidden_units * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(out_dim),\n",
    "            nn.Linear(out_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        z = h_n[-1]\n",
    "        return self.head(z)\n",
    "\n",
    "\n",
    "class TinyTCNBlock(nn.Module):\n",
    "    def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, kernel_size, padding=\"same\", dilation=dilation),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(channels, channels, kernel_size, padding=\"same\", dilation=dilation),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.conv(x)\n",
    "\n",
    "\n",
    "class TinyTCN(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_dim: int, hidden_channels: int = 128, layers: int = 3, kernel_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.pre = nn.Conv1d(input_channels, hidden_channels, kernel_size=1)\n",
    "        blocks = []\n",
    "        for i in range(layers):\n",
    "            blocks.append(TinyTCNBlock(hidden_channels, kernel_size, dilation=2 ** i, dropout=dropout))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = x.transpose(1, 2)\n",
    "        z = self.pre(z)\n",
    "        z = self.blocks(z)\n",
    "        z = self.head(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "def build_psann_tabular(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
    "    hidden_layers = extra.get(\"hidden_layers\", 8)\n",
    "    hidden_units = extra.get(\"hidden_units\", 256)\n",
    "    activation_type = extra.get(\"activation_type\", \"psann\")\n",
    "    return TabularPSANNModel(\n",
    "        input_dim=int(np.prod(input_shape)),\n",
    "        output_dim=output_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        hidden_units=hidden_units,\n",
    "        activation_type=activation_type,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_psann_sequence(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
    "    hidden_layers = extra.get(\"hidden_layers\", 8)\n",
    "    hidden_units = extra.get(\"hidden_units\", 256)\n",
    "    spine_type = extra.get(\"spine_type\", \"flatten\")\n",
    "    spine_params = extra.get(\"spine_params\", {})\n",
    "    activation_type = extra.get(\"activation_type\", \"psann\")\n",
    "    return SequencePSANNModel(\n",
    "        input_shape,\n",
    "        output_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        hidden_units=hidden_units,\n",
    "        spine_type=spine_type,\n",
    "        spine_params=spine_params,\n",
    "        activation_type=activation_type,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_mlp_model(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
    "    hidden_layers = extra.get(\"hidden_layers\", 3)\n",
    "    hidden_units = extra.get(\"hidden_units\", 256)\n",
    "    dropout = extra.get(\"dropout\", 0.1)\n",
    "    return MLPModel(\n",
    "        input_dim=int(np.prod(input_shape)),\n",
    "        output_dim=output_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        hidden_units=hidden_units,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_lstm_model(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
    "    sequence_length, channels = input_shape\n",
    "    hidden_units = extra.get(\"hidden_units\", 128)\n",
    "    num_layers = extra.get(\"num_layers\", 1)\n",
    "    bidirectional = extra.get(\"bidirectional\", False)\n",
    "    return LSTMHead(\n",
    "        input_dim=channels,\n",
    "        hidden_units=hidden_units,\n",
    "        num_layers=num_layers,\n",
    "        output_dim=output_dim,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=extra.get(\"dropout\", 0.1),\n",
    "    )\n",
    "\n",
    "\n",
    "def build_tcn_model(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
    "    sequence_length, channels = input_shape\n",
    "    hidden_channels = extra.get(\"hidden_channels\", 128)\n",
    "    layers = extra.get(\"layers\", 3)\n",
    "    kernel_size = extra.get(\"kernel_size\", 3)\n",
    "    dropout = extra.get(\"dropout\", 0.1)\n",
    "    return TinyTCN(\n",
    "        input_channels=channels,\n",
    "        output_dim=output_dim,\n",
    "        hidden_channels=hidden_channels,\n",
    "        layers=layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "\n",
    "class WaveResNetSequenceModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int, ...],\n",
    "        output_dim: int,\n",
    "        *,\n",
    "        aggregator: str = \"conv\",\n",
    "        aggregator_params: Optional[Dict[str, Any]] = None,\n",
    "        wave_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        aggregator_params = aggregator_params or {}\n",
    "        wave_kwargs = wave_kwargs or {}\n",
    "        time_steps, channels = input_shape\n",
    "        if aggregator == \"conv\":\n",
    "            hidden_channels = aggregator_params.get(\"channels\", wave_kwargs.get(\"hidden_dim\", 128))\n",
    "            activation = aggregator_params.get(\"activation\", nn.GELU)\n",
    "            self.spine = TemporalConvSpine(\n",
    "                channels,\n",
    "                hidden_channels,\n",
    "                kernel_size=aggregator_params.get(\"kernel_size\", 5),\n",
    "                stride=aggregator_params.get(\"stride\", 2),\n",
    "                depth=aggregator_params.get(\"depth\", 2),\n",
    "                activation=activation,\n",
    "            )\n",
    "            wave_input_dim = hidden_channels\n",
    "        elif aggregator == \"flatten\":\n",
    "            self.spine = FlattenSpine()\n",
    "            wave_input_dim = time_steps * channels\n",
    "        elif aggregator == \"identity\":\n",
    "            self.spine = IdentitySpine()\n",
    "            wave_input_dim = time_steps * channels\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregator '{aggregator}' for WaveResNetSequenceModel.\")\n",
    "        final_wave_kwargs = wave_kwargs.copy()\n",
    "        final_wave_kwargs[\"input_dim\"] = wave_input_dim\n",
    "        final_wave_kwargs[\"output_dim\"] = output_dim\n",
    "        self.core = WaveResNet(**final_wave_kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.spine(x) if x.ndim == 3 else x\n",
    "        if z.ndim > 2:\n",
    "            z = z.view(z.size(0), -1)\n",
    "        return self.core(z)\n",
    "\n",
    "\n",
    "def _count_params(module: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "def build_wave_resnet_tabular(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
    "    hidden_dims = extra.get(\"hidden_dims\", [192, 224, 256])\n",
    "    depths = extra.get(\"depths\", [4, 6, 8])\n",
    "    target_params = extra.get(\"target_params\")\n",
    "    tol = extra.get(\"param_tol\", 0.15)\n",
    "    dropout = extra.get(\"dropout\", 0.05)\n",
    "    first_layer_w0 = extra.get(\"first_layer_w0\", 30.0)\n",
    "    hidden_w0 = extra.get(\"hidden_w0\", 1.0)\n",
    "    input_dim = int(np.prod(input_shape))\n",
    "    best_model = None\n",
    "    best_gap = float(\"inf\")\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for depth in depths:\n",
    "            wave_kwargs = {\n",
    "                \"input_dim\": input_dim,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"depth\": depth,\n",
    "                \"output_dim\": output_dim,\n",
    "                \"dropout\": dropout,\n",
    "                \"first_layer_w0\": first_layer_w0,\n",
    "                \"hidden_w0\": hidden_w0,\n",
    "            }\n",
    "            candidate = WaveResNet(**wave_kwargs)\n",
    "            params = _count_params(candidate)\n",
    "            if target_params:\n",
    "                gap = abs(params - target_params)\n",
    "                if target_params > 0 and gap / target_params <= tol:\n",
    "                    return candidate\n",
    "                if gap < best_gap:\n",
    "                    best_model = candidate\n",
    "                    best_gap = gap\n",
    "            elif best_model is None:\n",
    "                best_model = candidate\n",
    "    if best_model is None:\n",
    "        raise RuntimeError(\"Unable to construct WaveResNet tabular model with the provided search space.\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def build_wave_resnet_sequence(input_shape: Tuple[int, ...], output_dim: int, extra: Dict[str, Any]) -> nn.Module:\n",
    "    hidden_dims = extra.get(\"hidden_dims\", [160, 192, 224])\n",
    "    depths = extra.get(\"depths\", [4, 6, 8])\n",
    "    target_params = extra.get(\"target_params\")\n",
    "    tol = extra.get(\"param_tol\", 0.15)\n",
    "    aggregator = extra.get(\"aggregator\", \"conv\")\n",
    "    aggregator_params = extra.get(\"aggregator_params\", {})\n",
    "    dropout = extra.get(\"dropout\", 0.05)\n",
    "    first_layer_w0 = extra.get(\"first_layer_w0\", 30.0)\n",
    "    hidden_w0 = extra.get(\"hidden_w0\", 1.0)\n",
    "    best_model = None\n",
    "    best_gap = float(\"inf\")\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for depth in depths:\n",
    "            wave_kwargs = {\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"depth\": depth,\n",
    "                \"dropout\": dropout,\n",
    "                \"first_layer_w0\": first_layer_w0,\n",
    "                \"hidden_w0\": hidden_w0,\n",
    "            }\n",
    "            agg_params = dict(aggregator_params)\n",
    "            if aggregator == \"conv\":\n",
    "                agg_params.setdefault(\"channels\", hidden_dim)\n",
    "            candidate = WaveResNetSequenceModel(\n",
    "                input_shape,\n",
    "                output_dim,\n",
    "                aggregator=aggregator,\n",
    "                aggregator_params=agg_params,\n",
    "                wave_kwargs=wave_kwargs,\n",
    "            )\n",
    "            params = _count_params(candidate)\n",
    "            if target_params:\n",
    "                gap = abs(params - target_params)\n",
    "                if target_params > 0 and gap / target_params <= tol:\n",
    "                    return candidate\n",
    "                if gap < best_gap:\n",
    "                    best_model = candidate\n",
    "                    best_gap = gap\n",
    "            elif best_model is None:\n",
    "                best_model = candidate\n",
    "    if best_model is None:\n",
    "        raise RuntimeError(\"Unable to construct WaveResNet sequence model with the provided search space.\")\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f91c9b",
   "metadata": {
    "id": "b3f91c9b"
   },
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def denormalize_regression_outputs(\n",
    "    bundle: DatasetBundle, y_true: np.ndarray, y_pred: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    scaler = bundle.metadata.get(\"target_scaler\") if getattr(bundle, \"metadata\", None) else None\n",
    "    if not scaler:\n",
    "        return y_true, y_pred\n",
    "    mean = np.asarray(scaler.get(\"mean\", 0.0), dtype=np.float32)\n",
    "    std = np.asarray(scaler.get(\"std\", 1.0), dtype=np.float32)\n",
    "    return y_true * std + mean, y_pred * std + mean\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader, spec: ModelSpec) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    param = next(model.parameters(), None)\n",
    "    model_device = param.device if param is not None else torch.device(\"cpu\")\n",
    "\n",
    "    preds: List[np.ndarray] = []\n",
    "    truths: List[np.ndarray] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(model_device)\n",
    "            y_batch = y_batch.to(model_device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            if spec.task_type != \"classification\":\n",
    "                if outputs.ndim > 2:\n",
    "                    outputs = outputs.view(outputs.size(0), -1)\n",
    "                if y_batch.ndim > 2:\n",
    "                    y_batch = y_batch.view(y_batch.size(0), -1)\n",
    "                elif y_batch.ndim == 1:\n",
    "                    y_batch = y_batch.unsqueeze(-1)\n",
    "\n",
    "            preds.append(outputs.detach().cpu().numpy())\n",
    "            truths.append(y_batch.detach().cpu().numpy())\n",
    "\n",
    "    if not preds:\n",
    "        raise ValueError(\"Evaluation loader produced no batches; check dataset splits and batch size.\")\n",
    "\n",
    "    y_pred = np.concatenate(preds, axis=0)\n",
    "    y_true = np.concatenate(truths, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def train_model_on_bundle(bundle: DatasetBundle, spec: ModelSpec, task_name: str) -> Dict[str, Any]:\n",
    "    input_shape = bundle.train[\"X\"].shape[1:]\n",
    "    if spec.task_type == \"classification\":\n",
    "        output_dim = int(bundle.metadata.get(\"n_classes\", np.unique(bundle.train[\"y\"]).size))\n",
    "    else:\n",
    "        output_dim = bundle.train[\"y\"].shape[1] if bundle.train[\"y\"].ndim > 1 else 1\n",
    "\n",
    "    model = spec.builder(input_shape, output_dim, spec.extra)\n",
    "    model.to(DEVICE)\n",
    "    params = count_trainable_parameters(model)\n",
    "\n",
    "    optimizer_cls = torch.optim.AdamW if spec.train_config.weight_decay > 0 else torch.optim.Adam\n",
    "    optimizer = optimizer_cls(\n",
    "        model.parameters(),\n",
    "        lr=spec.train_config.learning_rate,\n",
    "        weight_decay=spec.train_config.weight_decay,\n",
    "    )\n",
    "    scheduler = None\n",
    "    if spec.train_config.scheduler == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=spec.train_config.epochs)\n",
    "\n",
    "    train_loader = build_dataloader(\n",
    "        bundle.train[\"X\"],\n",
    "        bundle.train[\"y\"],\n",
    "        spec.train_config.batch_size,\n",
    "        shuffle=True,\n",
    "        task_type=spec.task_type,\n",
    "    )\n",
    "    val_loader = build_dataloader(\n",
    "        bundle.val[\"X\"],\n",
    "        bundle.val[\"y\"],\n",
    "        spec.train_config.batch_size,\n",
    "        shuffle=False,\n",
    "        task_type=spec.task_type,\n",
    "    )\n",
    "    test_loader = build_dataloader(\n",
    "        bundle.test[\"X\"],\n",
    "        bundle.test[\"y\"],\n",
    "        spec.train_config.batch_size,\n",
    "        shuffle=False,\n",
    "        task_type=spec.task_type,\n",
    "    )\n",
    "\n",
    "    best_state = None\n",
    "    best_val_metric = -float(\"inf\")\n",
    "    patience_counter = spec.train_config.patience\n",
    "    history: List[Dict[str, float]] = []\n",
    "    criterion_reg = nn.MSELoss()\n",
    "\n",
    "    with Timer() as timer:\n",
    "        for epoch in range(spec.train_config.epochs):\n",
    "            model.train()\n",
    "            model_device = next(model.parameters(), DEVICE).device\n",
    "            running_loss = 0.0\n",
    "            batches = 0\n",
    "\n",
    "            for step, (X_batch, y_batch) in enumerate(train_loader, start=1):\n",
    "                X_batch = X_batch.to(model_device)\n",
    "                y_batch = y_batch.to(model_device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "                if spec.task_type != \"classification\" and outputs.ndim > 2:\n",
    "                    outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "                if spec.task_type == \"classification\":\n",
    "                    loss = nn.functional.cross_entropy(\n",
    "                        outputs,\n",
    "                        y_batch,\n",
    "                        label_smoothing=GLOBAL_CONFIG[\"label_smoothing\"],\n",
    "                    )\n",
    "                else:\n",
    "                    target = y_batch\n",
    "                    if target.ndim > 2:\n",
    "                        target = target.view(target.size(0), -1)\n",
    "                    elif target.ndim == 1:\n",
    "                        target = target.unsqueeze(-1)\n",
    "                    loss = criterion_reg(outputs, target)\n",
    "\n",
    "                loss.backward()\n",
    "                if spec.train_config.gradient_clip is not None:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), spec.train_config.gradient_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                batches += 1\n",
    "                if spec.train_config.max_batches_per_epoch and batches >= spec.train_config.max_batches_per_epoch:\n",
    "                    break\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            avg_loss = running_loss / max(1, batches)\n",
    "            val_true, val_pred = evaluate_model(model, val_loader, spec)\n",
    "\n",
    "            if spec.task_type == \"classification\":\n",
    "                metrics = classification_metrics(val_true, val_pred)\n",
    "                score = metrics[\"accuracy\"]\n",
    "            else:\n",
    "                val_true_den, val_pred_den = denormalize_regression_outputs(bundle, val_true, val_pred)\n",
    "                metrics = regression_metrics(val_true_den, val_pred_den)\n",
    "                score = -metrics[\"rmse\"]\n",
    "\n",
    "            history.append({\"epoch\": epoch + 1, \"train_loss\": avg_loss, \"val_score\": score})\n",
    "\n",
    "            if score > best_val_metric:\n",
    "                best_val_metric = score\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                patience_counter = spec.train_config.patience\n",
    "            else:\n",
    "                patience_counter -= 1\n",
    "\n",
    "            if spec.train_config.early_stopping and patience_counter <= 0:\n",
    "                break\n",
    "            if spec.train_config.max_minutes is not None and timer.elapsed / 60.0 > spec.train_config.max_minutes:\n",
    "                print(f\"[INFO] Time budget reached for {spec.name}; stopping at epoch {epoch + 1}.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    train_true, train_pred = evaluate_model(model, train_loader, spec)\n",
    "    val_true, val_pred = evaluate_model(model, val_loader, spec)\n",
    "    test_true, test_pred = evaluate_model(model, test_loader, spec)\n",
    "\n",
    "    if spec.task_type == \"classification\":\n",
    "        train_metrics = classification_metrics(train_true, train_pred)\n",
    "        val_metrics = classification_metrics(val_true, val_pred)\n",
    "        test_metrics = classification_metrics(test_true, test_pred)\n",
    "\n",
    "        train_true_out, train_pred_out = train_true, train_pred\n",
    "        val_true_out, val_pred_out = val_true, val_pred\n",
    "        test_true_out, test_pred_out = test_true, test_pred\n",
    "    else:\n",
    "        train_true_den, train_pred_den = denormalize_regression_outputs(bundle, train_true, train_pred)\n",
    "        val_true_den, val_pred_den = denormalize_regression_outputs(bundle, val_true, val_pred)\n",
    "        test_true_den, test_pred_den = denormalize_regression_outputs(bundle, test_true, test_pred)\n",
    "\n",
    "        train_metrics = regression_metrics(train_true_den, train_pred_den)\n",
    "        val_metrics = regression_metrics(val_true_den, val_pred_den)\n",
    "        test_metrics = regression_metrics(test_true_den, test_pred_den)\n",
    "\n",
    "        train_true_out, train_pred_out = train_true_den, train_pred_den\n",
    "        val_true_out, val_pred_out = val_true_den, val_pred_den\n",
    "        test_true_out, test_pred_out = test_true_den, test_pred_den\n",
    "\n",
    "    RESULT_LOGGER.append(\n",
    "        ExperimentResult(\n",
    "            dataset=bundle.name,\n",
    "            task=task_name,\n",
    "            model=spec.name,\n",
    "            group=spec.group,\n",
    "            split=\"train\",\n",
    "            params=params,\n",
    "            train_wall_seconds=timer.elapsed,\n",
    "            metrics=train_metrics,\n",
    "            notes=spec.notes,\n",
    "        )\n",
    "    )\n",
    "    RESULT_LOGGER.append(\n",
    "        ExperimentResult(\n",
    "            dataset=bundle.name,\n",
    "            task=task_name,\n",
    "            model=spec.name,\n",
    "            group=spec.group,\n",
    "            split=\"val\",\n",
    "            params=params,\n",
    "            train_wall_seconds=timer.elapsed,\n",
    "            metrics=val_metrics,\n",
    "            notes=spec.notes,\n",
    "        )\n",
    "    )\n",
    "    RESULT_LOGGER.append(\n",
    "        ExperimentResult(\n",
    "            dataset=bundle.name,\n",
    "            task=task_name,\n",
    "            model=spec.name,\n",
    "            group=spec.group,\n",
    "            split=\"test\",\n",
    "            params=params,\n",
    "            train_wall_seconds=timer.elapsed,\n",
    "            metrics=test_metrics,\n",
    "            notes=spec.notes,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_cpu = model.to(\"cpu\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_cpu,\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"train_true\": train_true_out,\n",
    "        \"train_pred\": train_pred_out,\n",
    "        \"val_true\": val_true_out,\n",
    "        \"val_pred\": val_pred_out,\n",
    "        \"test_true\": test_true_out,\n",
    "        \"test_pred\": test_pred_out,\n",
    "        \"history\": history,\n",
    "        \"params\": params,\n",
    "        \"train_time\": timer.elapsed,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95235c68",
   "metadata": {
    "id": "95235c68"
   },
   "outputs": [],
   "source": [
    "def permutation_importance(\n",
    "    model: nn.Module,\n",
    "    bundle: DatasetBundle,\n",
    "    spec: ModelSpec,\n",
    "    feature_groups: Dict[str, List[int]],\n",
    "    split: str = \"test\",\n",
    "    n_repeats: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    data = getattr(bundle, split)\n",
    "    baseline_loader = build_dataloader(\n",
    "        data[\"X\"],\n",
    "        data[\"y\"],\n",
    "        spec.train_config.batch_size,\n",
    "        shuffle=False,\n",
    "        task_type=spec.task_type,\n",
    "    )\n",
    "    y_true, y_pred = evaluate_model(model, baseline_loader, spec)\n",
    "    if spec.task_type == \"classification\":\n",
    "        baseline_metric = classification_metrics(y_true, y_pred)[\"accuracy\"]\n",
    "    else:\n",
    "        baseline_metric = regression_metrics(y_true.squeeze(), y_pred.squeeze())[\"rmse\"]\n",
    "\n",
    "    rows = []\n",
    "    for group_name, columns in feature_groups.items():\n",
    "        deltas = []\n",
    "        cols = np.atleast_1d(columns)\n",
    "        for _ in range(n_repeats):\n",
    "            X_perm = data[\"X\"].copy()\n",
    "            if bundle.input_kind == \"tabular\":\n",
    "                for col in cols:\n",
    "                    np.random.shuffle(X_perm[:, col])\n",
    "            else:\n",
    "                for col in cols:\n",
    "                    np.random.shuffle(X_perm[:, :, col])\n",
    "            loader = build_dataloader(\n",
    "                X_perm,\n",
    "                data[\"y\"],\n",
    "                spec.train_config.batch_size,\n",
    "                shuffle=False,\n",
    "                task_type=spec.task_type,\n",
    "            )\n",
    "            y_true_perm, y_pred_perm = evaluate_model(model, loader, spec)\n",
    "            if spec.task_type == \"classification\":\n",
    "                metric_value = classification_metrics(y_true_perm, y_pred_perm)[\"accuracy\"]\n",
    "                delta = baseline_metric - metric_value\n",
    "            else:\n",
    "                metric_value = regression_metrics(y_true_perm.squeeze(), y_pred_perm.squeeze())[\"rmse\"]\n",
    "                delta = metric_value - baseline_metric\n",
    "            deltas.append(delta)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"group\": group_name,\n",
    "                \"mean_delta\": float(np.mean(deltas)),\n",
    "                \"std_delta\": float(np.std(deltas)),\n",
    "                \"baseline\": baseline_metric,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def compute_shap_importance(\n",
    "    model: nn.Module,\n",
    "    bundle: DatasetBundle,\n",
    "    spec: ModelSpec,\n",
    "    split: str = \"val\",\n",
    "    sample_size: int = 512,\n",
    ") -> Dict[str, Any]:\n",
    "    import shap\n",
    "\n",
    "    data = getattr(bundle, split)\n",
    "    X = data[\"X\"]\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(f\"No samples available in {split} split for SHAP computation.\")\n",
    "    sample_size = min(sample_size, len(X))\n",
    "    idx = np.random.choice(len(X), size=sample_size, replace=False)\n",
    "    X_sample = X[idx]\n",
    "\n",
    "    model_cpu = model.to(\"cpu\").eval()\n",
    "\n",
    "    def predict_fn(batch: np.ndarray) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(batch).float()\n",
    "            outputs = model_cpu(inputs)\n",
    "            if spec.task_type == \"classification\":\n",
    "                return torch.softmax(outputs, dim=-1).numpy()\n",
    "            return outputs.numpy()\n",
    "\n",
    "    if bundle.input_kind == \"tabular\":\n",
    "        background = X_sample[: min(128, sample_size)]\n",
    "        explainer = shap.KernelExplainer(predict_fn, background)\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "    else:\n",
    "        background = torch.from_numpy(X_sample[: min(64, sample_size)]).float()\n",
    "        explainer = shap.DeepExplainer(model_cpu, background)\n",
    "        shap_values = explainer.shap_values(torch.from_numpy(X_sample).float())\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    return {\"explainer\": explainer, \"shap_values\": shap_values, \"sample_indices\": idx}\n",
    "\n",
    "\n",
    "def compute_jacobian_singular_values(model: nn.Module, inputs: torch.Tensor, max_samples: int = 128) -> np.ndarray:\n",
    "    model.eval()\n",
    "    inputs = inputs[:max_samples].to(DEVICE).requires_grad_(True)\n",
    "    outputs = model(inputs)\n",
    "    if outputs.ndim == 1:\n",
    "        outputs = outputs.unsqueeze(-1)\n",
    "    jacobian_rows = []\n",
    "    for i in range(outputs.shape[1]):\n",
    "        grad_outputs = torch.zeros_like(outputs)\n",
    "        grad_outputs[:, i] = 1.0\n",
    "        grads = torch.autograd.grad(outputs, inputs, grad_outputs=grad_outputs, retain_graph=True, create_graph=False)[0]\n",
    "        jacobian_rows.append(grads.reshape(grads.size(0), -1).detach().cpu().numpy())\n",
    "    jacobian = np.concatenate(jacobian_rows, axis=1)\n",
    "    sigma = np.linalg.svd(jacobian, compute_uv=False)\n",
    "    return sigma\n",
    "\n",
    "\n",
    "def participation_ratio(singular_values: np.ndarray) -> float:\n",
    "    if singular_values.size == 0:\n",
    "        return float(\"nan\")\n",
    "    numerator = (singular_values ** 2).sum() ** 2\n",
    "    denominator = (singular_values ** 4).sum() + 1e-8\n",
    "    return float(numerator / denominator)\n",
    "\n",
    "\n",
    "def frequency_response_probe(model: nn.Module, input_dim: int, frequencies: Iterable[float], amplitude: float = 1.0) -> pd.DataFrame:\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    times = torch.linspace(0, 2 * math.pi, steps=512).unsqueeze(0)\n",
    "    for freq in frequencies:\n",
    "        signal = amplitude * torch.sin(freq * times)\n",
    "        if input_dim > 1:\n",
    "            signal = signal.repeat(1, input_dim)\n",
    "        signal = signal.to(DEVICE).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(signal)\n",
    "        energy = output.pow(2).mean().sqrt().item()\n",
    "        rows.append({\"frequency\": freq, \"output_rms\": energy})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate_robustness(\n",
    "    model: nn.Module,\n",
    "    bundle: DatasetBundle,\n",
    "    spec: ModelSpec,\n",
    "    corruption_fn: Callable[[np.ndarray, float], np.ndarray],\n",
    "    split: str = \"test\",\n",
    "    levels: Iterable[float] = (0.0, 0.1, 0.2, 0.3),\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    base_data = getattr(bundle, split)\n",
    "    for level in levels:\n",
    "        X_corrupted = corruption_fn(base_data[\"X\"], level)\n",
    "        loader = build_dataloader(\n",
    "            X_corrupted,\n",
    "            base_data[\"y\"],\n",
    "            spec.train_config.batch_size,\n",
    "            shuffle=False,\n",
    "            task_type=spec.task_type,\n",
    "        )\n",
    "        y_true, y_pred = evaluate_model(model, loader, spec)\n",
    "        if spec.task_type == \"classification\":\n",
    "            metrics = classification_metrics(y_true, y_pred)\n",
    "        else:\n",
    "            metrics = regression_metrics(y_true.squeeze(), y_pred.squeeze())\n",
    "        row = {\"level\": level}\n",
    "        row.update(metrics)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a25c2",
   "metadata": {
    "id": "f64a25c2"
   },
   "outputs": [],
   "source": [
    "# Cell (1) — helpers + containers\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Where your datasets live (should already be defined in the notebook; override only if missing)\n",
    "try:\n",
    "    DATA_ROOT\n",
    "except NameError:\n",
    "    DATA_ROOT = Path(\"/content/datasets\")\n",
    "\n",
    "# container for results\n",
    "DATA_BUNDLES: Dict[str, \"DatasetBundle\"] = {}\n",
    "\n",
    "def find_key(stations: dict, short_name: str) -> str:\n",
    "    \"\"\"Find the canonical station key by case-insensitive substring match.\n",
    "    Raises KeyError if no match found.\n",
    "    \"\"\"\n",
    "    short = short_name.lower()\n",
    "    matches = [k for k in stations.keys() if short in k.lower()]\n",
    "    if not matches:\n",
    "        raise KeyError(f\"No station matching '{short_name}'\")\n",
    "    # prefer exact prefix match if available (more deterministic)\n",
    "    for m in matches:\n",
    "        if m.lower().startswith(short):\n",
    "            return m\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefaphdaPGoq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aefaphdaPGoq",
    "outputId": "cfbebf2f-d8bc-4d43-cb61-79913addf1ef"
   },
   "outputs": [],
   "source": [
    "# Cell (2) — load all bundles (robust to small naming mismatches)\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# --- EAF bundles (these will use the merge_asof you've patched earlier) ---\n",
    "try:\n",
    "    eaf_tables = load_eaf_tables(DATA_ROOT)\n",
    "    eaf_temp_bundle, eaf_o2_bundle = prepare_eaf_temp_and_o2_bundles(eaf_tables)\n",
    "    eaf_chem_bundle = prepare_eaf_chemistry_bundle(eaf_tables)\n",
    "    DATA_BUNDLES[eaf_temp_bundle.name] = eaf_temp_bundle\n",
    "    DATA_BUNDLES[eaf_o2_bundle.name]   = eaf_o2_bundle\n",
    "    DATA_BUNDLES[eaf_chem_bundle.name] = eaf_chem_bundle\n",
    "    print(\" - EAF bundles ready\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] EAF bundle creation failed:\", repr(e))\n",
    "\n",
    "# --- Beijing cross-station bundle (use robust mapping for station names) ---\n",
    "try:\n",
    "    beijing_stations = load_beijing_stations(DATA_ROOT)\n",
    "    # map the short names you expect to their actual keys\n",
    "    val_key  = find_key(beijing_stations, \"Wanshouxigong\")\n",
    "    test_key = find_key(beijing_stations, \"Huairou\")\n",
    "    train_keys = [k for k in beijing_stations.keys() if k not in {val_key, test_key}]\n",
    "    print(f\" - Beijing: val={val_key}, test={test_key}, train_count={len(train_keys)}\")\n",
    "\n",
    "    beijing_bundle = assemble_beijing_cross_station_bundle(\n",
    "        beijing_stations,\n",
    "        train_stations=train_keys,\n",
    "        val_station=val_key,\n",
    "        test_station=test_key,\n",
    "        target=\"PM2.5\",\n",
    "        context=24,\n",
    "        horizon=6,\n",
    "    )\n",
    "    DATA_BUNDLES[beijing_bundle.name] = beijing_bundle\n",
    "    print(\" - Beijing bundle ready\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] Beijing bundle creation failed:\", repr(e))\n",
    "\n",
    "# --- Jena ---\n",
    "try:\n",
    "    jena_df = load_jena_climate(DATA_ROOT)\n",
    "    jena_bundle = prepare_jena_bundle(jena_df, target=\"T (degC)\", context_steps=72, horizon_steps=36)\n",
    "    DATA_BUNDLES[jena_bundle.name] = jena_bundle\n",
    "    print(\" - Jena bundle ready\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] Jena bundle creation failed:\", repr(e))\n",
    "\n",
    "# --- HAR (engineered + raw) ---\n",
    "try:\n",
    "    har_train_df, har_test_df, har_feature_names = load_har_engineered(DATA_ROOT)\n",
    "    har_engineered_bundle = prepare_har_engineered_bundle(har_train_df, har_test_df, har_feature_names)\n",
    "    DATA_BUNDLES[har_engineered_bundle.name] = har_engineered_bundle\n",
    "    print(\" - HAR engineered bundle ready\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] HAR engineered creation failed:\", repr(e))\n",
    "\n",
    "try:\n",
    "    X_har_train_raw, y_har_train_raw, X_har_test_raw, y_har_test_raw, har_axes = load_har_raw_sequences(DATA_ROOT)\n",
    "    har_raw_bundle = prepare_har_raw_bundle(X_har_train_raw, y_har_train_raw, X_har_test_raw, y_har_test_raw)\n",
    "    DATA_BUNDLES[har_raw_bundle.name] = har_raw_bundle\n",
    "    print(\" - HAR raw bundle ready\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] HAR raw creation failed:\", repr(e))\n",
    "\n",
    "# --- Rossmann ---\n",
    "try:\n",
    "    ross_train, ross_test, ross_store = load_rossmann_frames(DATA_ROOT)\n",
    "    ross_prepared, ross_features, ross_target = preprocess_rossmann(ross_train, ross_store)\n",
    "    ross_bundle = prepare_rossmann_bundle(ross_prepared, ross_features, ross_target)\n",
    "    DATA_BUNDLES[ross_bundle.name] = ross_bundle\n",
    "    print(\" - Rossmann bundle ready\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] Rossmann bundle creation failed:\", repr(e))\n",
    "\n",
    "# --- Summary of what succeeded ---\n",
    "print(\"\\nAvailable dataset bundles:\")\n",
    "for name, bundle in DATA_BUNDLES.items():\n",
    "    try:\n",
    "        print(f\" - {name}: {bundle.summary()}\")\n",
    "    except Exception:\n",
    "        print(f\" - {name}: (created, but summary() failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ead74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "526ead74",
    "outputId": "1f730e08-5a7a-4915-e756-5c187d5d980f"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_REGISTRY: Dict[str, List[ModelSpec]] = {}\n",
    "\n",
    "common_regression_train = TrainConfig(\n",
    "    epochs=60,\n",
    "    batch_size=512,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=10,\n",
    "    max_minutes=GLOBAL_CONFIG[\"max_time_minutes\"],\n",
    "    gradient_clip=1.0,\n",
    ")\n",
    "\n",
    "common_sequence_train = TrainConfig(\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=8,\n",
    "    max_minutes=GLOBAL_CONFIG[\"max_time_minutes\"],\n",
    "    gradient_clip=1.0,\n",
    ")\n",
    "\n",
    "common_classification_train = TrainConfig(\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=5e-5,\n",
    "    patience=8,\n",
    "    max_minutes=GLOBAL_CONFIG[\"max_time_minutes\"],\n",
    "    gradient_clip=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "def register_specs(bundle: DatasetBundle):\n",
    "    specs: List[ModelSpec] = []\n",
    "    if bundle.input_kind == \"tabular\":\n",
    "        train_cfg = common_regression_train if bundle.task_type == \"regression\" else common_classification_train\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"ResPSANN_tabular\",\n",
    "                builder=build_psann_tabular,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"tabular\",\n",
    "                group=\"psann\",\n",
    "                extra={\"hidden_layers\": 8, \"hidden_units\": 256},\n",
    "                notes=\"Residual PSANN core\",\n",
    "            )\n",
    "        )\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"MLP_baseline\",\n",
    "                builder=build_mlp_model,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"tabular\",\n",
    "                group=\"baseline\",\n",
    "                extra={\"hidden_layers\": 4, \"hidden_units\": 256, \"dropout\": 0.1},\n",
    "                notes=\"ReLU MLP with similar parameter budget\",\n",
    "            )\n",
    "        )\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"WaveResNet_tabular\",\n",
    "                builder=build_wave_resnet_tabular,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"tabular\",\n",
    "                group=\"baseline\",\n",
    "                extra={\n",
    "                    \"hidden_dims\": [192, 224, 256],\n",
    "                    \"depths\": [4, 6, 8],\n",
    "                    \"target_params\": 400_000,\n",
    "                    \"param_tol\": 0.2,\n",
    "                    \"dropout\": 0.05,\n",
    "                    \"first_layer_w0\": 30.0,\n",
    "                    \"hidden_w0\": 1.0,\n",
    "                },\n",
    "                notes=\"WaveResNet baseline with sine residual blocks\",\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        train_cfg = common_sequence_train if bundle.task_type == \"regression\" else common_classification_train\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"ResPSANN_conv_spine\",\n",
    "                builder=build_psann_sequence,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"sequence\",\n",
    "                group=\"psann\",\n",
    "                extra={\n",
    "                    \"hidden_layers\": 6,\n",
    "                    \"hidden_units\": 192,\n",
    "                    \"spine_type\": \"conv\",\n",
    "                    \"spine_params\": {\"channels\": 192, \"depth\": 2, \"kernel_size\": 5, \"stride\": 2},\n",
    "                },\n",
    "                notes=\"ResPSANN with strided Conv1d spine\",\n",
    "            )\n",
    "        )\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"ResPSANN_attention_spine\",\n",
    "                builder=build_psann_sequence,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"sequence\",\n",
    "                group=\"psann\",\n",
    "                extra={\n",
    "                    \"hidden_layers\": 6,\n",
    "                    \"hidden_units\": 192,\n",
    "                    \"spine_type\": \"attention\",\n",
    "                    \"spine_params\": {\"num_heads\": 1},\n",
    "                },\n",
    "                notes=\"ResPSANN with single-head attention spine\",\n",
    "            )\n",
    "        )\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"LSTM_baseline\",\n",
    "                builder=build_lstm_model,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"sequence\",\n",
    "                group=\"baseline\",\n",
    "                extra={\"hidden_units\": 192, \"num_layers\": 1, \"dropout\": 0.1},\n",
    "                notes=\"Single-layer LSTM baseline\",\n",
    "            )\n",
    "        )\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"TCN_baseline\",\n",
    "                builder=build_tcn_model,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"sequence\",\n",
    "                group=\"baseline\",\n",
    "                extra={\"hidden_channels\": 192, \"layers\": 3, \"kernel_size\": 3, \"dropout\": 0.1},\n",
    "                notes=\"Tiny TCN baseline\",\n",
    "            )\n",
    "        )\n",
    "        specs.append(\n",
    "            ModelSpec(\n",
    "                name=\"WaveResNet_sequence\",\n",
    "                builder=build_wave_resnet_sequence,\n",
    "                train_config=train_cfg,\n",
    "                task_type=bundle.task_type,\n",
    "                input_kind=\"sequence\",\n",
    "                group=\"baseline\",\n",
    "                extra={\n",
    "                    \"hidden_dims\": [160, 192, 224],\n",
    "                    \"depths\": [4, 6, 8],\n",
    "                    \"aggregator\": \"conv\",\n",
    "                    \"aggregator_params\": {\"depth\": 2, \"kernel_size\": 5, \"stride\": 2},\n",
    "                    \"target_params\": 350_000,\n",
    "                    \"param_tol\": 0.2,\n",
    "                    \"dropout\": 0.05,\n",
    "                    \"first_layer_w0\": 30.0,\n",
    "                    \"hidden_w0\": 1.0,\n",
    "                    \"use_film\":True,\n",
    "                    \"use_phase_shift\":True\n",
    "                },\n",
    "                notes=\"WaveResNet baseline with a lightweight temporal spine\",\n",
    "            )\n",
    "        )\n",
    "        EXPERIMENT_REGISTRY[bundle.name] = specs\n",
    "\n",
    "\n",
    "for bundle in DATA_BUNDLES.values():\n",
    "    register_specs(bundle)\n",
    "\n",
    "print(\"Registered model specs:\")\n",
    "for dataset_name, specs in EXPERIMENT_REGISTRY.items():\n",
    "    print(f\"- {dataset_name}: {[spec.name for spec in specs]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9ab43",
   "metadata": {
    "id": "fae9ab43"
   },
   "outputs": [],
   "source": [
    "RUN_EXPERIMENTS = {\n",
    "    \"EAF_TEMP_forecast\": True,\n",
    "    \"EAF_VALO2_forecast\": True,\n",
    "    \"EAF_chemistry\": True,\n",
    "    \"Beijing_PM25_24h_ctx_6h_horizon\": True,\n",
    "    \"Jena_tdegc_72ctx_36h\": True,\n",
    "    \"HAR_engineered\": True,\n",
    "    \"HAR_raw_sequence\": True,\n",
    "    \"Rossmann_sales\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27ae56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed27ae56",
    "outputId": "2c75e2e9-ea9c-4c82-cc77-5fa7c5ad4319"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_ARTIFACTS: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for dataset_name, run_flag in RUN_EXPERIMENTS.items():\n",
    "    if not run_flag:\n",
    "        continue\n",
    "    if dataset_name not in DATA_BUNDLES:\n",
    "        print(f\"[WARN] Dataset {dataset_name} not loaded; skipping.\")\n",
    "        continue\n",
    "    bundle = DATA_BUNDLES[dataset_name]\n",
    "    specs = EXPERIMENT_REGISTRY.get(dataset_name, [])\n",
    "    if not specs:\n",
    "        print(f\"[WARN] No model specs registered for {dataset_name}; skipping.\")\n",
    "        continue\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Dataset: {dataset_name} ({bundle.task_type}, {bundle.input_kind})\")\n",
    "    for spec in specs:\n",
    "        print(f\"  -> Training {spec.name}\")\n",
    "        result = train_model_on_bundle(bundle, spec, task_name=dataset_name)\n",
    "        EXPERIMENT_ARTIFACTS.setdefault(dataset_name, {})[spec.name] = result\n",
    "        artifact_path = RESULTS_ROOT / f\"{dataset_name}_{spec.name}_predictions.npz\"\n",
    "        np.savez_compressed(\n",
    "            artifact_path,\n",
    "            train_true=result[\"train_true\"],\n",
    "            train_pred=result[\"train_pred\"],\n",
    "            val_true=result[\"val_true\"],\n",
    "            val_pred=result[\"val_pred\"],\n",
    "            test_true=result[\"test_true\"],\n",
    "            test_pred=result[\"test_pred\"],\n",
    "        )\n",
    "        print(f\"    Validation metrics: {result['val_metrics']}\")\n",
    "        print(f\"    Test metrics       : {result['test_metrics']}\")\n",
    "        print(f\"    Saved predictions to {artifact_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f24804",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "47f24804",
    "outputId": "ff3d933c-2cf2-4f64-dfd8-2acad7fc21a4"
   },
   "outputs": [],
   "source": [
    "results_df = RESULT_LOGGER.to_frame()\n",
    "results_path = RESULTS_ROOT / \"experiment_metrics.csv\"\n",
    "if not results_df.empty:\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    display(results_df)\n",
    "    print(f\"Metrics saved to {results_path}\")\n",
    "else:\n",
    "    print(\"No experiments were run yet. Toggle RUN_EXPERIMENTS before executing the training cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb9471",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37cb9471",
    "outputId": "f75187a2-a765-4fcf-f46d-219da76bc533"
   },
   "outputs": [],
   "source": [
    "TARGET_DATASET = \"EAF_TEMP_forecast\"\n",
    "TARGET_MODEL = \"ResPSANN_tabular\"\n",
    "\n",
    "if TARGET_DATASET in EXPERIMENT_ARTIFACTS and TARGET_MODEL in EXPERIMENT_ARTIFACTS[TARGET_DATASET]:\n",
    "    bundle = DATA_BUNDLES[TARGET_DATASET]\n",
    "    spec = next(spec for spec in EXPERIMENT_REGISTRY[TARGET_DATASET] if spec.name == TARGET_MODEL)\n",
    "    trained_model = EXPERIMENT_ARTIFACTS[TARGET_DATASET][TARGET_MODEL][\"model\"]\n",
    "\n",
    "    prefix_groups = {\n",
    "        \"temp_lags\": [i for i, name in enumerate(bundle.feature_names) if name.startswith(\"TEMP_lag\")],\n",
    "        \"valo2_lags\": [i for i, name in enumerate(bundle.feature_names) if name.startswith(\"VALO2_lag\")],\n",
    "        \"gas_flow\": [i for i, name in enumerate(bundle.feature_names) if \"gas\" in name.lower()],\n",
    "        \"inj\": [i for i, name in enumerate(bundle.feature_names) if \"inj\" in name.lower()],\n",
    "        \"calendar\": [i for i, name in enumerate(bundle.feature_names) if \"DATETIME\" in name],\n",
    "    }\n",
    "\n",
    "    perm_df = permutation_importance(\n",
    "        trained_model,\n",
    "        bundle,\n",
    "        spec,\n",
    "        feature_groups=prefix_groups,\n",
    "        split=\"test\",\n",
    "        n_repeats=5,\n",
    "    )\n",
    "    display(perm_df.sort_values(\"mean_delta\", ascending=False))\n",
    "else:\n",
    "    print(\"Train the target model first; EXPERIMENT_ARTIFACTS does not contain it yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6a52d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1c6a52d",
    "outputId": "d49a5b92-2fee-4a2c-e9e9-077ef7072a24"
   },
   "outputs": [],
   "source": [
    "TARGET_DATASET = \"Jena_tdegc_72ctx_36h\"\n",
    "TARGET_MODEL = \"ResPSANN_conv_spine\"\n",
    "\n",
    "if TARGET_DATASET in EXPERIMENT_ARTIFACTS and TARGET_MODEL in EXPERIMENT_ARTIFACTS[TARGET_DATASET]:\n",
    "    bundle = DATA_BUNDLES[TARGET_DATASET]\n",
    "    spec = next(spec for spec in EXPERIMENT_REGISTRY[TARGET_DATASET] if spec.name == TARGET_MODEL)\n",
    "    trained_model = EXPERIMENT_ARTIFACTS[TARGET_DATASET][TARGET_MODEL][\"model\"].to(DEVICE)\n",
    "    sample_loader = build_dataloader(\n",
    "        bundle.val[\"X\"],\n",
    "        bundle.val[\"y\"],\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        task_type=spec.task_type,\n",
    "    )\n",
    "    sample_batch = next(iter(sample_loader))[0][:64]\n",
    "    singular_values = compute_jacobian_singular_values(trained_model, sample_batch, max_samples=64)\n",
    "    pr = participation_ratio(singular_values)\n",
    "    print(f\"Participation ratio: {pr:.4f}\")\n",
    "    trained_model.to(\"cpu\")\n",
    "else:\n",
    "    print(\"Train the target model first to access EXPERIMENT_ARTIFACTS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd5b0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9afd5b0b",
    "outputId": "469ae4f3-6921-485b-f5b7-ecb50255d3cd"
   },
   "outputs": [],
   "source": [
    "TARGET_DATASET = \"Beijing_PM25_24h_ctx_6h_horizon\"\n",
    "TARGET_MODEL = \"ResPSANN_conv_spine\"\n",
    "\n",
    "if TARGET_DATASET in EXPERIMENT_ARTIFACTS and TARGET_MODEL in EXPERIMENT_ARTIFACTS[TARGET_DATASET]:\n",
    "    bundle = DATA_BUNDLES[TARGET_DATASET]\n",
    "    spec = next(spec for spec in EXPERIMENT_REGISTRY[TARGET_DATASET] if spec.name == TARGET_MODEL)\n",
    "    trained_model = EXPERIMENT_ARTIFACTS[TARGET_DATASET][TARGET_MODEL][\"model\"]\n",
    "\n",
    "    def missingness_fn(X: np.ndarray, level: float) -> np.ndarray:\n",
    "        rng = np.random.default_rng(GLOBAL_CONFIG[\"seed\"])\n",
    "        mask = rng.random(size=X.shape) < level\n",
    "        X_corrupted = X.copy()\n",
    "        X_corrupted[mask] = 0.0\n",
    "        return X_corrupted\n",
    "\n",
    "    robustness_df = evaluate_robustness(\n",
    "        trained_model,\n",
    "        bundle,\n",
    "        spec,\n",
    "        corruption_fn=missingness_fn,\n",
    "        split=\"test\",\n",
    "        levels=[0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    )\n",
    "    display(robustness_df)\n",
    "else:\n",
    "    print(\"Train the target model before running robustness experiments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58771f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a58771f",
    "outputId": "5d5a7d3b-c01e-4532-e246-8e7f74af3813"
   },
   "outputs": [],
   "source": [
    "TARGET_DATASET = \"HAR_raw_sequence\"\n",
    "if TARGET_DATASET in EXPERIMENT_ARTIFACTS:\n",
    "    results = EXPERIMENT_ARTIFACTS[TARGET_DATASET]\n",
    "    if \"ResPSANN_conv_spine\" in results and \"ResPSANN_attention_spine\" in results:\n",
    "        conv_acc = results[\"ResPSANN_conv_spine\"][\"test_metrics\"][\"accuracy\"]\n",
    "        attn_acc = results[\"ResPSANN_attention_spine\"][\"test_metrics\"][\"accuracy\"]\n",
    "        print(f\"Conv spine accuracy: {conv_acc:.4f}\")\n",
    "        print(f\"Attention spine accuracy: {attn_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"Run both PSANN spine variants on HAR_raw_sequence first.\")\n",
    "else:\n",
    "    print(\"Train HAR_raw_sequence models before evaluating H5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x6qgdD_WPmvf",
   "metadata": {
    "id": "x6qgdD_WPmvf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def zip_folder(folder_path: str | Path, output_path: str | Path | None = None, *, include_hidden: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Compresses an entire folder (recursively) into a .zip archive.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str or Path\n",
    "        Path to the folder to zip.\n",
    "    output_path : str or Path or None, optional\n",
    "        Output .zip file path. Defaults to \"<folder_name>.zip\" in the same directory.\n",
    "    include_hidden : bool, optional\n",
    "        Whether to include hidden files (those starting with '.').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the created .zip file.\n",
    "    \"\"\"\n",
    "    folder_path = Path(folder_path).resolve()\n",
    "    if not folder_path.is_dir():\n",
    "        raise ValueError(f\"{folder_path} is not a valid directory\")\n",
    "\n",
    "    if output_path is None:\n",
    "        output_path = folder_path.with_suffix(\".zip\")\n",
    "    else:\n",
    "        output_path = Path(output_path).resolve()\n",
    "\n",
    "    with zipfile.ZipFile(output_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            # skip hidden dirs/files if requested\n",
    "            if not include_hidden:\n",
    "                dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "                files = [f for f in files if not f.startswith(\".\")]\n",
    "\n",
    "            for file in files:\n",
    "                abs_path = Path(root) / file\n",
    "                # relative path inside the zip\n",
    "                rel_path = abs_path.relative_to(folder_path)\n",
    "                zf.write(abs_path, arcname=rel_path)\n",
    "\n",
    "    print(f\"Zipped {folder_path} → {output_path}\")\n",
    "    return output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeG6oGYtM1rd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeG6oGYtM1rd",
    "outputId": "7ca9648d-cf42-4505-870b-aaebc1700389"
   },
   "outputs": [],
   "source": [
    "zip_folder(folder_path = '/content/colab_results', output_path = '/content/colab_results.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kKFbc--ONBQa",
   "metadata": {
    "id": "kKFbc--ONBQa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411c15a6",
   "metadata": {
    "id": "411c15a6"
   },
   "source": [
    "---\n",
    "\n",
    "## Synthetic Probe Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0a1ab",
   "metadata": {
    "id": "fdb0a1ab"
   },
   "source": [
    "This notebook fabricates compact synthetic datasets that stand in for the larger\n",
    "benchmarks in the plan, then runs a focused suite of probes:\n",
    "\n",
    "• Parity-locked comparisons (PSANN + tiny temporal spine vs TCN/LSTM/MLP)\n",
    "• Cross-\"station\" generalization & missingness robustness\n",
    "• Grouped permutation (information-usage) ablations\n",
    "• Spectral/geometry diagnostics (Jacobian SVD & participation ratio)\n",
    "• EAF-style per-heat resets & ΔTEMP targets\n",
    "\n",
    "Results are saved to: /content/psann_synth_results/\n",
    "\n",
    "Design draws from the experiment plan and PSANN docs:\n",
    "- Plan & hypotheses (H1–H5), fairness constraints, and probes:  :contentReference[oaicite:3]{index=3}\n",
    "- Estimator surface and usage patterns (sklearn-style, HISSO-ready):  :contentReference[oaicite:4]{index=4}\n",
    "- Activation math, residual wrappers, and utilities (Jacobian/NTK):  :contentReference[oaicite:5]{index=5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc84dd",
   "metadata": {
    "id": "32fc84dd"
   },
   "outputs": [],
   "source": [
    "# @title Imports, device, tiny utils\n",
    "import os, math, time, json, random, itertools, functools\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List, Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RESULTS_DIR = \"/content/psann_synth_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def set_seed(s:int):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "\n",
    "def param_count(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-8):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps)\n",
    "    return (100.0 / len(y_true)) * np.sum(np.abs(y_pred - y_true) / denom)\n",
    "\n",
    "def mase(y_true, y_pred, m=1):\n",
    "    # naive seasonal m=1 by default\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    naive = np.mean(np.abs(y_true[m:] - y_true[:-m])) + 1e-8\n",
    "    return np.mean(np.abs(y_true - y_pred)) / naive\n",
    "\n",
    "def save_csv(df: pd.DataFrame, name: str):\n",
    "    path = os.path.join(RESULTS_DIR, name)\n",
    "    if os.path.exists(path):\n",
    "        old = pd.read_csv(path)\n",
    "        df = pd.concat([old, df], ignore_index=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved -> {path} ({len(df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1494e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae1494e1",
    "outputId": "887c082b-04dd-4448-a62a-678419068ce4"
   },
   "outputs": [],
   "source": [
    "# @title Synthetic Probes tests\n",
    "try:\n",
    "    import psann  # type: ignore\n",
    "    HAVE_PSANN = True\n",
    "    from psann import PSANNRegressor  # sklearn-style\n",
    "    try:\n",
    "        from psann.models.wave_resnet import WaveResNet as PSANNWaveResNet\n",
    "    except Exception:\n",
    "        PSANNWaveResNet = None\n",
    "    print(\"Using installed psann package.\")\n",
    "except Exception as e:\n",
    "    HAVE_PSANN = False\n",
    "    PSANNRegressor = None\n",
    "    PSANNWaveResNet = None\n",
    "    print(\"psann not found; using a compact sine-activated residual fallback.\")\n",
    "\n",
    "import math\n",
    "\n",
    "class SineParam(nn.Module):\n",
    "    \"\"\"SIREN-style learnable sine activation with optional decay as in TECHNICAL_DETAILS.md (simplified).\"\"\"\n",
    "    def __init__(self, features, w0=30.0, use_decay=True):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.zeros(features))  # amplitude pre-softplus\n",
    "        self.b = nn.Parameter(torch.zeros(features))  # frequency pre-softplus\n",
    "        self.c = nn.Parameter(torch.zeros(features))  # decay pre-softplus\n",
    "        self.w0 = w0\n",
    "        self.use_decay = use_decay\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        A = self.softplus(self.a).view(1, -1)\n",
    "        f = self.softplus(self.b).view(1, -1) + 1e-6\n",
    "        d = self.softplus(self.c).view(1, -1)\n",
    "        if z.dim() == 3:  # (B, T, F)\n",
    "            A = A.unsqueeze(1)\n",
    "            f = f.unsqueeze(1)\n",
    "            d = d.unsqueeze(1)\n",
    "        if self.use_decay:\n",
    "            return A * torch.exp(-d * torch.abs(z)) * torch.sin(f * z)\n",
    "        else:\n",
    "            return A * torch.sin(f * z)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_f, out_f, p_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_f, out_f)\n",
    "        self.act = SineParam(out_f)\n",
    "        self.alpha = nn.Parameter(torch.tensor(1.0))\n",
    "        self.do = nn.Dropout(p_drop)\n",
    "        self.short = (in_f == out_f)\n",
    "\n",
    "        # SIREN-ish init for fc\n",
    "        wstd = math.sqrt(6.0 / in_f) / 30.0\n",
    "        nn.init.uniform_(self.fc.weight, -wstd, wstd)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.do(self.act(self.fc(x)))\n",
    "        out = h\n",
    "        if self.short:\n",
    "            out = x + self.alpha * h\n",
    "        return out\n",
    "\n",
    "class ResSineMLP(nn.Module):\n",
    "    def __init__(self, in_f, hidden, layers=2, out_f=1, p_drop=0.0):\n",
    "        super().__init__()\n",
    "        net = []\n",
    "        dim = in_f\n",
    "        for _ in range(layers):\n",
    "            net.append(ResBlock(dim, hidden, p_drop))\n",
    "            dim = hidden\n",
    "        self.net = nn.Sequential(*net)\n",
    "        self.head = nn.Linear(dim, out_f)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):  # x: (B, F)\n",
    "        h = self.net(x)\n",
    "        return self.head(h)\n",
    "\n",
    "class ConvSpine1D(nn.Module):\n",
    "    \"\"\"Tiny strided Conv1d spine -> global average -> residual sine MLP head.\"\"\"\n",
    "    def __init__(self, in_ch, ch=32, k=5, stride=2, mlp_hidden=64, mlp_layers=2, out_f=1):\n",
    "        super().__init__()\n",
    "        pad = (k//2)\n",
    "        self.conv1 = nn.Conv1d(in_ch, ch, kernel_size=k, padding=pad, stride=stride)\n",
    "        self.conv2 = nn.Conv1d(ch, ch, kernel_size=k, padding=pad, stride=stride)\n",
    "        self.mlp = ResSineMLP(in_f=ch, hidden=mlp_hidden, layers=mlp_layers, out_f=out_f)\n",
    "        # SIREN-ish init\n",
    "        for c in [self.conv1, self.conv2]:\n",
    "            nn.init.kaiming_uniform_(c.weight, a=math.sqrt(5))\n",
    "            if c.bias is not None: nn.init.zeros_(c.bias)\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, F) -> (B, F, T)\n",
    "        x = x.transpose(1, 2)\n",
    "        h = F.silu(self.conv1(x))\n",
    "        h = F.silu(self.conv2(h))\n",
    "        h = h.mean(-1)  # GAP over time -> (B, ch)\n",
    "        return self.mlp(h)\n",
    "\n",
    "class AttentionSpine1D(nn.Module):\n",
    "    \"\"\"Single-head self-attention spine -> mean pool -> residual sine MLP head.\"\"\"\n",
    "    def __init__(self, in_f, d=64, mlp_hidden=64, mlp_layers=2, out_f=1):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(in_f, d); self.k = nn.Linear(in_f, d); self.v = nn.Linear(in_f, d)\n",
    "        self.mlp = ResSineMLP(in_f=d, hidden=mlp_hidden, layers=mlp_layers, out_f=out_f)\n",
    "\n",
    "    def forward(self, x):  # (B, T, F)\n",
    "        Q, K, V = self.q(x), self.k(x), self.v(x)\n",
    "        att = torch.softmax(Q @ K.transpose(1,2) / math.sqrt(Q.size(-1)), dim=-1)\n",
    "        H = att @ V\n",
    "        h = H.mean(1)  # pool over time\n",
    "        return self.mlp(h)\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    \"\"\"Simple 1D TCN block with causal padding.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=5, dilation=1):\n",
    "        super().__init__()\n",
    "        pad = (k-1)*dilation\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, k, padding=pad, dilation=dilation)\n",
    "        self.short = (in_ch == out_ch)\n",
    "        self.proj = nn.Conv1d(in_ch, out_ch, 1) if not self.short else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = h[..., :x.size(-1)]  # causal trim\n",
    "        h = F.relu(h)\n",
    "        return F.relu(self.proj(x) + h)\n",
    "\n",
    "class TinyTCN(nn.Module):\n",
    "    def __init__(self, in_ch, ch=32, layers=2, k=5, out_f=1):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        c = in_ch\n",
    "        for i in range(layers):\n",
    "            blocks.append(TCNBlock(c, ch, k=k, dilation=2**i))\n",
    "            c = ch\n",
    "        self.tcn = nn.Sequential(*blocks)\n",
    "        self.head = nn.Linear(ch, out_f)\n",
    "\n",
    "    def forward(self, x):  # (B, T, F) -> (B, F, T)\n",
    "        x = x.transpose(1,2)\n",
    "        h = self.tcn(x).transpose(1,2)  # back to (B, T, C)\n",
    "        h_last = h[:, -1, :]\n",
    "        return self.head(h_last)\n",
    "\n",
    "class TinyLSTM(nn.Module):\n",
    "    def __init__(self, in_f, hidden=64, out_f=1, layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_f, hidden, num_layers=layers, batch_first=True)\n",
    "        self.head = nn.Linear(hidden, out_f)\n",
    "\n",
    "    def forward(self, x):  # (B, T, F)\n",
    "        h, (hn, cn) = self.lstm(x)\n",
    "        return self.head(h[:, -1, :])\n",
    "\n",
    "class WaveResNetFallback(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        *,\n",
    "        hidden_dim: int,\n",
    "        depth: int,\n",
    "        output_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        first_layer_w0: float = 30.0,\n",
    "        hidden_w0: float = 1.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if input_dim <= 0 or hidden_dim <= 0 or output_dim <= 0:\n",
    "            raise ValueError(\"input_dim, hidden_dim, and output_dim must be positive.\")\n",
    "        if depth <= 0:\n",
    "            raise ValueError(\"depth must be positive.\")\n",
    "        self.stem = nn.Linear(input_dim, hidden_dim)\n",
    "        wstd = math.sqrt(6.0 / input_dim) / max(first_layer_w0, 1e-6)\n",
    "        nn.init.uniform_(self.stem.weight, -wstd, wstd)\n",
    "        nn.init.zeros_(self.stem.bias)\n",
    "        self.blocks = nn.ModuleList([ResBlock(hidden_dim, hidden_dim, p_drop=dropout) for _ in range(depth)])\n",
    "        self.head = nn.Linear(hidden_dim, output_dim)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "        self.stem_w0 = first_layer_w0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = torch.sin(self.stem_w0 * self.stem(x))\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "def make_wave_resnet(\n",
    "    input_dim: int,\n",
    "    *,\n",
    "    hidden_dim: int,\n",
    "    depth: int,\n",
    "    output_dim: int,\n",
    "    dropout: float = 0.0,\n",
    "    first_layer_w0: float = 30.0,\n",
    "    hidden_w0: float = 1.0,\n",
    "):\n",
    "    if 'PSANNWaveResNet' in globals() and PSANNWaveResNet is not None:\n",
    "        return PSANNWaveResNet(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            depth=depth,\n",
    "            output_dim=output_dim,\n",
    "            dropout=dropout,\n",
    "            first_layer_w0=first_layer_w0,\n",
    "            hidden_w0=hidden_w0,\n",
    "        )\n",
    "    return WaveResNetFallback(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        depth=depth,\n",
    "        output_dim=output_dim,\n",
    "        dropout=dropout,\n",
    "        first_layer_w0=first_layer_w0,\n",
    "        hidden_w0=hidden_w0,\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Generic training helpers (regression) ---\n",
    "def fit_regressor(model, X_train, y_train, X_val, y_val, epochs=60, lr=1e-3, bs=128, verbose=False):\n",
    "    model = model.to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    best = math.inf; best_state = None\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32); y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32); y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    train_loader = DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pred = model(xb)\n",
    "            loss = F.mse_loss(pred.view_as(yb), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val.to(DEVICE)).cpu().numpy().ravel()\n",
    "            vloss = np.mean((val_pred - y_val.numpy().ravel())**2)\n",
    "        if vloss < best:\n",
    "            best = vloss; best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "        if verbose and (ep+1)%10==0:\n",
    "            print(f\"ep {ep+1:3d} val_mse={vloss:.6f}\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def predict_regressor(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=DEVICE)\n",
    "        y = model(X).cpu().numpy().ravel()\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3d01d",
   "metadata": {
    "id": "03c3d01d"
   },
   "outputs": [],
   "source": [
    "# @title Generators: Seasonal (Jena proxy), Cross-Station Air (Beijing proxy), and EAF ΔTEMP\n",
    "def gen_seasonal_series(n=20000, noise=0.15, drift=0.0002, w=(1/24, 1/168)):\n",
    "    t = np.arange(n, dtype=np.float32)\n",
    "    daily = np.sin(2*np.pi*w[0]*t)\n",
    "    weekly = 0.5*np.sin(2*np.pi*w[1]*t + 0.3)\n",
    "    trend = drift * t\n",
    "    base = daily + weekly + trend\n",
    "    exo = np.stack([\n",
    "        np.cos(2*np.pi*w[0]*t), np.sin(2*np.pi*w[0]*t),\n",
    "        np.cos(2*np.pi*w[1]*t), np.sin(2*np.pi*w[1]*t),\n",
    "    ], axis=1)\n",
    "    y = base + noise*np.random.randn(n).astype(np.float32)\n",
    "    return y.astype(np.float32), exo.astype(np.float32)\n",
    "\n",
    "def window_xy(y, exo, ctx=72, horizon=6):\n",
    "    Xs, Ys = [], []\n",
    "    for i in range(ctx, len(y)-horizon):\n",
    "        hist = y[i-ctx:i].reshape(-1,1)\n",
    "        feats = np.concatenate([hist, exo[i-ctx:i]], axis=1)\n",
    "        Xs.append(feats)           # (ctx, 1+exo_dim)\n",
    "        Ys.append(y[i+horizon])\n",
    "    return np.array(Xs, np.float32), np.array(Ys, np.float32)\n",
    "\n",
    "def gen_cross_station_air(stations=10, n=5000, ctx=24, horizon=3, missing=0.0, seed=0):\n",
    "    rs = np.random.RandomState(seed)\n",
    "    data = []\n",
    "    for s in range(stations):\n",
    "        amp = 0.8 + 0.4*rs.rand()\n",
    "        phi = rs.rand()*2*np.pi\n",
    "        t = np.arange(n, dtype=np.float32)\n",
    "        base = amp*np.sin(2*np.pi*(1/24)*t + phi) + 0.2*np.sin(2*np.pi*(1/48)*t)\n",
    "        met = np.stack([\n",
    "            np.sin(2*np.pi*(1/24)*t + 0.1), np.cos(2*np.pi*(1/24)*t + 0.2),\n",
    "            0.5*np.sin(2*np.pi*(1/168)*t + 0.3)\n",
    "        ], axis=1).astype(np.float32)\n",
    "        y = base + 0.1*rs.randn(n).astype(np.float32)\n",
    "        X, Y = window_xy(y, met, ctx=ctx, horizon=horizon)\n",
    "        # inject missingness on met channels only\n",
    "        if missing > 0:\n",
    "            mask = rs.rand(*X.shape) < (missing * (X.shape[-1]-1)/X.shape[-1])\n",
    "            # keep history column (index 0) intact, drop some exo\n",
    "            mask[..., 0] = False\n",
    "            X[mask] = 0.0\n",
    "            # optional: append masks as features\n",
    "            X = np.concatenate([X, mask.astype(np.float32)], axis=-1)\n",
    "        data.append((s, X, Y))\n",
    "    return data  # list of (station_id, X, y)\n",
    "\n",
    "def gen_eaf_heats(heats=120, min_len=80, max_len=200, seed=0):\n",
    "    \"\"\"ΔTEMP_t ≈ a1*O2_t + a2*MW_t + a3*sqrt(O2_t)*noise; counters reset per heat.\"\"\"\n",
    "    rs = np.random.RandomState(seed)\n",
    "    X_rows, y_rows, heat_ids = [], [], []\n",
    "    for h in range(heats):\n",
    "        L = rs.randint(min_len, max_len+1)\n",
    "        O2 = np.abs(rs.randn(L).astype(np.float32))*2.0\n",
    "        MW = np.abs(rs.randn(L).astype(np.float32))*3.0\n",
    "        # cumulative counters reset at each heat:\n",
    "        c_O2 = np.cumsum(O2)\n",
    "        c_MW = np.cumsum(MW)\n",
    "        dtemp = 0.7*O2 + 0.4*MW + 0.15*np.sqrt(O2+1e-6)*rs.randn(L).astype(np.float32)\n",
    "        TEMP = np.cumsum(dtemp) + 20*rs.rand()  # integral of ΔTEMP\n",
    "        # target: next-step ΔTEMP\n",
    "        y = np.roll(dtemp, -1); y[-1] = dtemp[-1]\n",
    "        feats = np.stack([O2, MW, c_O2, c_MW, TEMP], axis=1).astype(np.float32)\n",
    "        X_rows.append(feats); y_rows.append(y.astype(np.float32)); heat_ids += [h]*L\n",
    "    X = np.concatenate(X_rows, axis=0)\n",
    "    y = np.concatenate(y_rows, axis=0)\n",
    "    heat_ids = np.array(heat_ids, dtype=np.int32)\n",
    "    # build windows\n",
    "    ctx=24\n",
    "    Xs, Ys = [], []\n",
    "    for i in range(ctx, len(y)-1):\n",
    "        if heat_ids[i-ctx] != heat_ids[i]:  # ensure window does not cross heats\n",
    "            continue\n",
    "        hist = X[i-ctx:i, :]  # includes counters already reset per-heat\n",
    "        Xs.append(hist); Ys.append(y[i])\n",
    "    return np.array(Xs, np.float32), np.array(Ys, np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0619f2",
   "metadata": {
    "id": "4d0619f2"
   },
   "outputs": [],
   "source": [
    "# @title Parity matchers: adjust hidden sizes to hit a target parameter budget\n",
    "def build_psann_conv(in_f, target_params=200_000, tol=0.15, out_f=1):\n",
    "    # search small grid over (conv_ch, mlp_hidden, layers)\n",
    "    for ch in [16, 24, 32, 40, 48]:\n",
    "        for mh in [32, 48, 64, 96]:\n",
    "            for layers in [1, 2, 3]:\n",
    "                m = ConvSpine1D(in_ch=in_f, ch=ch, k=5, stride=2, mlp_hidden=mh, mlp_layers=layers, out_f=out_f)\n",
    "                p = param_count(m)\n",
    "                if abs(p - target_params)/target_params <= tol:\n",
    "                    return m, p\n",
    "    # fallback: pick closest\n",
    "    best, bestp = None, 1e18\n",
    "    for ch in [16, 24, 32, 40, 48]:\n",
    "        for mh in [32, 48, 64, 96]:\n",
    "            for layers in [1, 2, 3]:\n",
    "                m = ConvSpine1D(in_ch=in_f, ch=ch, k=5, stride=2, mlp_hidden=mh, mlp_layers=layers, out_f=out_f)\n",
    "                p = param_count(m)\n",
    "                if abs(p - target_params) < abs(bestp - target_params):\n",
    "                    best, bestp = m, p\n",
    "    return best, bestp\n",
    "\n",
    "def build_tcn(in_f, target_params=200_000, tol=0.15, out_f=1):\n",
    "    for ch in [16, 24, 32, 40, 48, 64]:\n",
    "        for layers in [1,2,3,4]:\n",
    "            m = TinyTCN(in_ch=in_f, ch=ch, layers=layers, k=5, out_f=out_f)\n",
    "            p = param_count(m)\n",
    "            if abs(p - target_params)/target_params <= tol:\n",
    "                return m, p\n",
    "    # fallback\n",
    "    best, bestp = None, 1e18\n",
    "    for ch in [16, 24, 32, 40, 48, 64]:\n",
    "        for layers in [1,2,3,4]:\n",
    "            m = TinyTCN(in_ch=in_f, ch=ch, layers=layers, k=5, out_f=out_f)\n",
    "            p = param_count(m)\n",
    "            if abs(p - target_params) < abs(bestp - target_params):\n",
    "                best, bestp = m, p\n",
    "    return best, bestp\n",
    "\n",
    "def build_lstm(in_f, target_params=200_000, tol=0.15, out_f=1):\n",
    "    for h in [32, 48, 64, 80, 96, 128, 160]:\n",
    "        m = TinyLSTM(in_f=in_f, hidden=h, out_f=out_f, layers=1)\n",
    "        p = param_count(m)\n",
    "        if abs(p - target_params)/target_params <= tol:\n",
    "            return m, p\n",
    "    # fallback\n",
    "    best, bestp = None, 1e18\n",
    "    for h in [32, 48, 64, 80, 96, 128, 160]:\n",
    "        m = TinyLSTM(in_f=in_f, hidden=h, out_f=out_f, layers=1)\n",
    "        p = param_count(m)\n",
    "        if abs(p - target_params) < abs(bestp - target_params):\n",
    "            best, bestp = m, p\n",
    "    return best, bestp\n",
    "\n",
    "def build_psann_tabular(in_f, target_params=200_000, tol=0.15, out_f=1):\n",
    "    for mh in [32, 48, 64, 96, 128]:\n",
    "        for layers in [1, 2, 3]:\n",
    "            m = ResSineMLP(in_f, hidden=mh, layers=layers, out_f=out_f)\n",
    "            p = param_count(m)\n",
    "            if abs(p - target_params)/target_params <= tol:\n",
    "                return m, p\n",
    "    # fallback\n",
    "    best, bestp = None, 1e18\n",
    "    for mh in [32, 48, 64, 96, 128]:\n",
    "        for layers in [1, 2, 3]:\n",
    "            m = ResSineMLP(in_f, hidden=mh, layers=layers, out_f=out_f)\n",
    "            p = param_count(m)\n",
    "            if abs(p - target_params) < abs(bestp - target_params):\n",
    "                best, bestp = m, p\n",
    "    return best, bestp\n",
    "\n",
    "class SimpleScaler:\n",
    "    def __init__(self):\n",
    "        self.mean = None; self.std = None\n",
    "    def fit(self, X):\n",
    "        self.mean = X.mean(axis=0, keepdims=True)\n",
    "        self.std = X.std(axis=0, keepdims=True) + 1e-8\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean)/self.std\n",
    "\n",
    "def eval_regression(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    sm = smape(y_true, y_pred)\n",
    "    ms = mase(y_true, y_pred, m=1)\n",
    "    return dict(r2=r2, mae=mae, rmse=rmse, smape=sm, mase=ms)\n",
    "\n",
    "class WaveResNetSeqBaseline(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        *,\n",
    "        conv_channels: int = 64,\n",
    "        conv_depth: int = 2,\n",
    "        hidden_dim: int = 160,\n",
    "        depth: int = 4,\n",
    "        out_f: int = 1,\n",
    "        dropout: float = 0.05,\n",
    "        first_layer_w0: float = 30.0,\n",
    "        hidden_w0: float = 1.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        channels = in_ch\n",
    "        for _ in range(conv_depth):\n",
    "            conv = nn.Conv1d(channels, conv_channels, kernel_size=5, padding=2, stride=2)\n",
    "            nn.init.kaiming_uniform_(conv.weight, a=math.sqrt(5))\n",
    "            if conv.bias is not None:\n",
    "                nn.init.zeros_(conv.bias)\n",
    "            layers.extend([conv, nn.GELU()])\n",
    "            channels = conv_channels\n",
    "        self.conv = nn.Sequential(*layers) if layers else nn.Identity()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        final_input = conv_channels if layers else in_ch\n",
    "        self.wave = make_wave_resnet(\n",
    "            final_input,\n",
    "            hidden_dim=hidden_dim,\n",
    "            depth=depth,\n",
    "            output_dim=out_f,\n",
    "            dropout=dropout,\n",
    "            first_layer_w0=first_layer_w0,\n",
    "            hidden_w0=hidden_w0,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = x.transpose(1, 2)\n",
    "        z = self.conv(z)\n",
    "        z = self.pool(z).squeeze(-1)\n",
    "        return self.wave(z)\n",
    "\n",
    "\n",
    "def build_wave_resnet_seq(in_f, target_params=200_000, tol=0.15, out_f=1):\n",
    "    best, bestp = None, float('inf')\n",
    "    for conv_channels in [48, 64, 96, 128]:\n",
    "        for conv_depth in [1, 2, 3]:\n",
    "            for hidden_dim in [128, 160, 192, 224]:\n",
    "                for depth in [3, 4, 5]:\n",
    "                    model = WaveResNetSeqBaseline(\n",
    "                        in_ch=in_f,\n",
    "                        conv_channels=conv_channels,\n",
    "                        conv_depth=conv_depth,\n",
    "                        hidden_dim=hidden_dim,\n",
    "                        depth=depth,\n",
    "                        out_f=out_f,\n",
    "                        dropout=0.05,\n",
    "                    )\n",
    "                    p = param_count(model)\n",
    "                    if abs(p - target_params) / max(target_params, 1) <= tol:\n",
    "                        return model, p\n",
    "                    if abs(p - target_params) < abs(bestp - target_params):\n",
    "                        best, bestp = model, p\n",
    "    return best, bestp\n",
    "\n",
    "\n",
    "def build_wave_resnet_tabular(in_f, target_params=200_000, tol=0.15, out_f=1):\n",
    "    best, bestp = None, float('inf')\n",
    "    for hidden_dim in [128, 160, 192, 224, 256]:\n",
    "        for depth in [3, 4, 5, 6]:\n",
    "            model = make_wave_resnet(\n",
    "                in_f,\n",
    "                hidden_dim=hidden_dim,\n",
    "                depth=depth,\n",
    "                output_dim=out_f,\n",
    "                dropout=0.05,\n",
    "            )\n",
    "            p = param_count(model)\n",
    "            if abs(p - target_params) / max(target_params, 1) <= tol:\n",
    "                return model, p\n",
    "            if abs(p - target_params) < abs(bestp - target_params):\n",
    "                best, bestp = model, p\n",
    "    return best, bestp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fbc57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "355fbc57",
    "outputId": "19d0d47b-e962-48cf-b67b-8abb38f156b9"
   },
   "outputs": [],
   "source": [
    "# @title Seasonal probe: PSANN+Conv vs TCN vs LSTM (parity, 3 seeds)\n",
    "set_seed(SEED)\n",
    "y, exo = gen_seasonal_series(n=22000, noise=0.12, drift=0.00015)\n",
    "X, Y = window_xy(y, exo, ctx=72, horizon=6)   # X: (N, 72, 1+4) -> F=5\n",
    "# Train/val/test split\n",
    "N = len(Y); n_train = int(0.7*N); n_val = int(0.15*N)\n",
    "X_train, y_train = X[:n_train], Y[:n_train]\n",
    "X_val,   y_val   = X[n_train:n_train+n_val], Y[n_train:n_train+n_val]\n",
    "X_test,  y_test  = X[n_train+n_val:], Y[n_train+n_val:]\n",
    "sc = SimpleScaler();  # scale per-feature across flattened dims\n",
    "flat = X_train.reshape(-1, X_train.shape[-1]); sc.fit(flat)\n",
    "def scale_seq(Xseq):\n",
    "    shp = Xseq.shape\n",
    "    Xs = sc.transform(Xseq.reshape(-1, shp[-1])).reshape(shp)\n",
    "    return Xs\n",
    "\n",
    "X_train_s = scale_seq(X_train); X_val_s = scale_seq(X_val); X_test_s = scale_seq(X_test)\n",
    "\n",
    "target_params = 220_000\n",
    "in_f = X.shape[-1]\n",
    "records = []\n",
    "for seed in [1, 2, 3]:\n",
    "    set_seed(seed)\n",
    "    # Build parity-matched models\n",
    "    psann_m, p_ps = build_psann_conv(in_f, target_params, out_f=1)\n",
    "    tcn_m,   p_tc = build_tcn(in_f, target_params, out_f=1)\n",
    "    lstm_m,  p_ls = build_lstm(in_f, target_params, out_f=1)\n",
    "    wave_m,  p_wr = build_wave_resnet_seq(in_f, target_params, out_f=1)\n",
    "    for name, model, pcount in [\n",
    "        (\"ResPSANN_conv_spine\", psann_m, p_ps),\n",
    "        (\"TCN_baseline\",        tcn_m,   p_tc),\n",
    "        (\"LSTM_baseline\",       lstm_m,  p_ls),\n",
    "        (\"WaveResNet_sequence\",  wave_m,  p_wr),\n",
    "    ]:\n",
    "        t0 = time.time()\n",
    "        model = fit_regressor(model, X_train_s, y_train[:,None], X_val_s, y_val[:,None],\n",
    "                              epochs=60, lr=3e-3, bs=128, verbose=False)\n",
    "        t1 = time.time()\n",
    "        yhat = predict_regressor(model, X_test_s)\n",
    "        mets = eval_regression(y_test, yhat)\n",
    "        rec = dict(\n",
    "            dataset=\"SEASONAL_JENA_PROXY\", probe=\"PARITY\", split=\"test\", seed=seed,\n",
    "            model=name, params=param_count(model), train_wall_seconds=t1-t0, **mets\n",
    "        )\n",
    "        records.append(rec)\n",
    "df = pd.DataFrame(records)\n",
    "save_csv(df, \"synthetic_experiment_metrics.csv\")\n",
    "df.pivot_table(index=[\"dataset\",\"probe\",\"model\"], values=[\"r2\",\"mae\",\"rmse\",\"smape\",\"mase\",\"params\",\"train_wall_seconds\"]).round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561039d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "a561039d",
    "outputId": "e2b4930d-7300-40d9-cbe4-8593dc26646a"
   },
   "outputs": [],
   "source": [
    "# @title Cross-station held-out & missingness robustness; parity-matched PSANN+Conv vs TCN vs LSTM\n",
    "set_seed(SEED)\n",
    "\n",
    "def prep_dataset(miss=0.0, seed=42, ctx=24, horizon=3):\n",
    "    \"\"\"\n",
    "    Generate a fresh cross-station dataset with a given missingness rate,\n",
    "    split into train (stations != 0,1), val (station 0), test (station 1),\n",
    "    and fit a scaler on the training split only.\n",
    "    \"\"\"\n",
    "    data = gen_cross_station_air(stations=10, n=6000, ctx=ctx, horizon=horizon, missing=miss, seed=seed)\n",
    "    # Split by station id\n",
    "    X_train = np.concatenate([x for (sid, x, y) in data if sid not in (0, 1)], axis=0)\n",
    "    y_train = np.concatenate([y for (sid, x, y) in data if sid not in (0, 1)], axis=0)\n",
    "    X_val   = next(x for (sid, x, y) in data if sid == 0)\n",
    "    y_val   = next(y for (sid, x, y) in data if sid == 0)\n",
    "    X_test  = next(x for (sid, x, y) in data if sid == 1)\n",
    "    y_test  = next(y for (sid, x, y) in data if sid == 1)\n",
    "\n",
    "    # Scale using only TRAIN statistics\n",
    "    scaler = SimpleScaler()\n",
    "    scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
    "    def scale_seq(X):\n",
    "        shp = X.shape\n",
    "        return scaler.transform(X.reshape(-1, shp[-1])).reshape(shp)\n",
    "\n",
    "    return scale_seq(X_train), y_train, scale_seq(X_val), y_val, scale_seq(X_test), y_test\n",
    "\n",
    "target_params = 200_000\n",
    "records = []\n",
    "for miss in [0.0, 0.1, 0.3]:\n",
    "    # Fresh dataset (and scaler) per missingness level for clean robustness curves\n",
    "    Xtr, ytr, Xv, yv, Xte, yte = prep_dataset(miss=miss, seed=42, ctx=24, horizon=3)\n",
    "    in_f = Xtr.shape[-1]\n",
    "\n",
    "    for seed in [7, 8, 9]:\n",
    "        set_seed(seed)\n",
    "        # Build parity-matched models (≈ same param count)\n",
    "        psann_m, p_ps = build_psann_conv(in_f, target_params, out_f=1)\n",
    "        tcn_m,   p_tc = build_tcn(in_f, target_params, out_f=1)\n",
    "        lstm_m,  p_ls = build_lstm(in_f, target_params, out_f=1)\n",
    "        wave_m,  p_wr = build_wave_resnet_seq(in_f, target_params, out_f=1)\n",
    "\n",
    "        for name, model in [\n",
    "            (\"ResPSANN_conv_spine\", psann_m),\n",
    "            (\"TCN_baseline\",        tcn_m),\n",
    "            (\"LSTM_baseline\",       lstm_m),\n",
    "            (\"WaveResNet_sequence\",  wave_m),\n",
    "        ]:\n",
    "            t0 = time.time()\n",
    "            model = fit_regressor(model, Xtr, ytr[:, None], Xv, yv[:, None],\n",
    "                                  epochs=50, lr=3e-3, bs=256)\n",
    "            t1 = time.time()\n",
    "            yhat = predict_regressor(model, Xte)\n",
    "            mets = eval_regression(yte, yhat)\n",
    "            rec = dict(\n",
    "                dataset=\"AIR_BEIJING_PROXY\",\n",
    "                probe=f\"HELDOUT+MISS_{int(miss*100)}\",\n",
    "                split=\"test\",\n",
    "                seed=seed,\n",
    "                model=name,\n",
    "                params=param_count(model),\n",
    "                train_wall_seconds=t1 - t0,\n",
    "                **mets\n",
    "            )\n",
    "            records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "save_csv(df, \"synthetic_experiment_metrics.csv\")\n",
    "df[df[\"dataset\"] == \"AIR_BEIJING_PROXY\"].pivot_table(\n",
    "    index=[\"probe\", \"model\"], values=[\"r2\", \"mae\", \"rmse\"]\n",
    ").round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bb2fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "ba5bb2fe",
    "outputId": "a98653e3-5b51-4186-db5e-338264ce363c"
   },
   "outputs": [],
   "source": [
    "# @title Permute grouped features on the AIR proxy (history vs meteorology vs calendar)\n",
    "# FIX: make sure F is the PyTorch functional module, not an int\n",
    "import torch.nn.functional as F  # restore F after any accidental shadowing\n",
    "\n",
    "def grouped_permute(X, groups: Dict[str, List[int]], which: str, rs: np.random.RandomState):\n",
    "    Xp = X.copy()\n",
    "    idxs = groups[which]\n",
    "    # permute across samples independently for each feature channel\n",
    "    for j in idxs:\n",
    "        rs.shuffle(Xp[:, :, j])\n",
    "    return Xp\n",
    "\n",
    "# Rebuild a dataset with explicit groups: [0]=history, [1..m]=met; append calendar(sin/cos hour)\n",
    "def air_with_calendar(seed=2024):\n",
    "    data = gen_cross_station_air(stations=10, n=6000, ctx=24, horizon=3, missing=0.0, seed=seed)\n",
    "    def add_calendar(X):\n",
    "        N, T, Fd = X.shape\n",
    "        hours = np.arange(T, dtype=np.float32)[None, :, None].repeat(N, axis=0)\n",
    "        sin = np.sin(2*np.pi*hours/24.0).astype(np.float32)\n",
    "        cos = np.cos(2*np.pi*hours/24.0).astype(np.float32)\n",
    "        return np.concatenate([X, sin, cos], axis=2)\n",
    "    return [(s, add_calendar(X), y) for (s, X, y) in data]\n",
    "\n",
    "set_seed(101)\n",
    "data = air_with_calendar(seed=101)\n",
    "# Train on all but station 1; test on station 1\n",
    "Xtr = np.concatenate([x for (s, x, y) in data if s != 1], axis=0)\n",
    "ytr = np.concatenate([y for (s, x, y) in data if s != 1], axis=0)\n",
    "Xte = [x for (s, x, y) in data if s == 1][0]\n",
    "yte = [y for (s, x, y) in data if s == 1][0]\n",
    "\n",
    "# Scale on training only\n",
    "sc = SimpleScaler(); sc.fit(Xtr.reshape(-1, Xtr.shape[-1]))\n",
    "def scale_seq(X):\n",
    "    shp = X.shape\n",
    "    return sc.transform(X.reshape(-1, shp[-1])).reshape(shp)\n",
    "\n",
    "Xtr_s, Xte_s = scale_seq(Xtr), scale_seq(Xte)\n",
    "\n",
    "# Group indices:\n",
    "feat_dim = Xtr_s.shape[-1]   # FIX: don't shadow F\n",
    "# history at index 0; meteorology next 3; calendar at the end (2 dims)\n",
    "groups = {\"history\": [0], \"meteorology\": [1, 2, 3], \"calendar\": [feat_dim - 2, feat_dim - 1]}\n",
    "\n",
    "# pick a parity-matched winner config (PSANN+Conv) and compare\n",
    "psann_m, _ = build_psann_conv(feat_dim, target_params=180_000, out_f=1)\n",
    "psann_m = fit_regressor(psann_m, Xtr_s, ytr[:, None], Xtr_s[-5000:], ytr[-5000:, None],\n",
    "                        epochs=40, lr=3e-3, bs=256)\n",
    "base = eval_regression(yte, predict_regressor(psann_m, Xte_s))[\"r2\"]\n",
    "\n",
    "abl_records = []\n",
    "rs = np.random.RandomState(7)\n",
    "for gname in [\"history\", \"meteorology\", \"calendar\"]:\n",
    "    Xp = grouped_permute(Xte_s, groups, gname, rs)\n",
    "    r2 = eval_regression(yte, predict_regressor(psann_m, Xp))[\"r2\"]\n",
    "    abl_records.append(dict(dataset=\"AIR_BEIJING_PROXY\", probe=\"ABLATE_GROUPS\",\n",
    "                            model=\"ResPSANN_conv_spine\",\n",
    "                            group=gname, base_r2=base, ablated_r2=r2, delta=r2 - base))\n",
    "\n",
    "abl_df = pd.DataFrame(abl_records)\n",
    "save_csv(abl_df, \"synthetic_ablation_results.csv\")\n",
    "abl_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cc412",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a7cc412",
    "outputId": "0df1e230-0c94-4fd6-bf2d-1307eece6ece"
   },
   "outputs": [],
   "source": [
    "# @title Jacobian spectrum & participation ratio (PSANN vs MLP) on a small batch — FIXED\n",
    "def jacobian_matrix(model: nn.Module, Xb: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute d y / d x for a batch: for each sample i (out_i is scalar y[i,0]),\n",
    "    we compute grad(out_i, Xb)[i, :, :] and vectorize into one row.\n",
    "    Returns J of shape (B, T*F).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    xb = torch.tensor(Xb, dtype=torch.float32, device=DEVICE, requires_grad=True)\n",
    "    y = model(xb)  # (B, 1)\n",
    "    assert y.ndim == 2 and y.shape[1] == 1, f\"Expected (B,1) output, got {tuple(y.shape)}\"\n",
    "\n",
    "    B, T, Fdim = xb.shape\n",
    "    rows = []\n",
    "    for i in range(B):\n",
    "        # grad_outputs must be same shape as y: (B,1)\n",
    "        go = torch.zeros_like(y)\n",
    "        go[i, 0] = 1.0\n",
    "        g = torch.autograd.grad(outputs=y, inputs=xb,\n",
    "                                grad_outputs=go,\n",
    "                                retain_graph=True, create_graph=False,\n",
    "                                allow_unused=False)[0]  # (B,T,F)\n",
    "        gi = g[i].reshape(-1).detach().cpu().numpy()       # (T*F,)\n",
    "        rows.append(gi)\n",
    "    J = np.stack(rows, axis=0)  # (B, T*F)\n",
    "    return J\n",
    "\n",
    "def participation_ratio(M: np.ndarray):\n",
    "    # effective dimensionality of rows of M\n",
    "    C = (M - M.mean(0, keepdims=True))\n",
    "    C = C.T @ C / max(C.shape[0]-1, 1)\n",
    "    evals = np.maximum(np.linalg.eigvalsh(C), 1e-12)\n",
    "    s1 = evals.sum(); s2 = (evals**2).sum()\n",
    "    return float((s1**2) / s2)\n",
    "\n",
    "# build small dataset\n",
    "y, exo = gen_seasonal_series(n=6000)\n",
    "X, Y = window_xy(y, exo, ctx=72, horizon=6)\n",
    "sc = SimpleScaler(); sc.fit(X.reshape(-1, X.shape[-1]))\n",
    "Xs = sc.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "# models: PSANN+Conv (parity-targeted) vs MLP baseline on pooled features\n",
    "psann_m, _ = build_psann_conv(Xs.shape[-1], target_params=160_000, out_f=1)\n",
    "psann_m = fit_regressor(psann_m, Xs[:3000], Y[:3000,None], Xs[3000:4000], Y[3000:4000,None], epochs=30)\n",
    "\n",
    "# Simple MLP baseline on last-step features only\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_f, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_f, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n",
    "    def forward(self, x):  # x: (B,T,F)\n",
    "        x_last = x[:, -1, :]\n",
    "        return self.net(x_last)\n",
    "\n",
    "mlp = TinyMLP(Xs.shape[-1])\n",
    "mlp = fit_regressor(mlp, Xs[:3000], Y[:3000,None], Xs[3000:4000], Y[3000:4000,None], epochs=30)\n",
    "\n",
    "# Jacobian on a tiny batch for each\n",
    "batch = Xs[4000:4024]\n",
    "J_ps = jacobian_matrix(psann_m, batch)\n",
    "J_ml = jacobian_matrix(mlp, batch)\n",
    "\n",
    "# SVD & metrics\n",
    "_, s_ps, _ = np.linalg.svd(J_ps, full_matrices=False)\n",
    "_, s_ml, _ = np.linalg.svd(J_ml, full_matrices=False)\n",
    "\n",
    "spec_records = [{\n",
    "    \"dataset\":\"SEASONAL_JENA_PROXY\",\"probe\":\"SPECTRAL\",\"model\":\"ResPSANN_conv_spine\",\n",
    "    \"top_sv\": float(s_ps[0]), \"sum_sv\": float(s_ps.sum()), \"pr\": participation_ratio(J_ps)\n",
    "},{\n",
    "    \"dataset\":\"SEASONAL_JENA_PROXY\",\"probe\":\"SPECTRAL\",\"model\":\"MLP_laststep\",\n",
    "    \"top_sv\": float(s_ml[0]), \"sum_sv\": float(s_ml.sum()), \"pr\": participation_ratio(J_ml)\n",
    "}]\n",
    "with open(os.path.join(RESULTS_DIR,\"synthetic_spectral_results.json\"),\"w\") as f:\n",
    "    json.dump(spec_records, f, indent=2)\n",
    "spec_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839de4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "a839de4f",
    "outputId": "c2f36eb5-17e0-44a7-fc3a-97c46cf41241"
   },
   "outputs": [],
   "source": [
    "# @title Fairness check: single-head attention spine vs conv spine under matched params\n",
    "set_seed(2025)\n",
    "data = gen_cross_station_air(stations=8, n=5000, ctx=24, horizon=3, missing=0.0, seed=77)\n",
    "X = np.concatenate([x for (s,x,y) in data], axis=0)\n",
    "y = np.concatenate([y for (s,x,y) in data], axis=0)\n",
    "sc = SimpleScaler(); sc.fit(X.reshape(-1, X.shape[-1]))\n",
    "Xs = sc.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "N = len(y); ntr=int(0.7*N); nv=int(0.15*N)\n",
    "Xtr, ytr = Xs[:ntr], y[:ntr]; Xv, yv = Xs[ntr:ntr+nv], y[ntr:ntr+nv]; Xte, yte = Xs[ntr+nv:], y[ntr+nv:]\n",
    "\n",
    "target_params = 180_000\n",
    "# match conv\n",
    "conv_m, p_conv = build_psann_conv(Xs.shape[-1], target_params, out_f=1)\n",
    "# for attention, sweep d & mlp until within tol\n",
    "best_att = None; bestp = 1e18\n",
    "for d in [48, 64, 80]:\n",
    "    for mh in [48, 64, 80]:\n",
    "        att = AttentionSpine1D(in_f=Xs.shape[-1], d=d, mlp_hidden=mh, mlp_layers=2, out_f=1)\n",
    "        p = param_count(att)\n",
    "        if abs(p - target_params) < abs(bestp - target_params):\n",
    "            bestp = p; best_att = att\n",
    "att_m = best_att\n",
    "\n",
    "conv_m = fit_regressor(conv_m, Xtr, ytr[:,None], Xv, yv[:,None], epochs=40, lr=3e-3)\n",
    "att_m  = fit_regressor(att_m,  Xtr, ytr[:,None], Xv, yv[:,None], epochs=40, lr=3e-3)\n",
    "\n",
    "yhat_c = predict_regressor(conv_m, Xte)\n",
    "yhat_a = predict_regressor(att_m,  Xte)\n",
    "rec = pd.DataFrame([\n",
    "    dict(dataset=\"AIR_BEIJING_PROXY\", probe=\"SPINE_FAIRNESS\", split=\"test\", model=\"ResPSANN_conv_spine\",\n",
    "         params=param_count(conv_m), **eval_regression(yte, yhat_c)),\n",
    "    dict(dataset=\"AIR_BEIJING_PROXY\", probe=\"SPINE_FAIRNESS\", split=\"test\", model=\"ResPSANN_attention_spine\",\n",
    "         params=param_count(att_m), **eval_regression(yte, yhat_a)),\n",
    "])\n",
    "save_csv(rec, \"synthetic_experiment_metrics.csv\")\n",
    "rec[[\"model\",\"r2\",\"mae\",\"rmse\",\"params\"]].round(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24434be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "d24434be",
    "outputId": "c2789596-b08b-4606-f917-c66bd26a58d4"
   },
   "outputs": [],
   "source": [
    "# @title EAF synthetic: ΔTEMP regression with per-heat resets; PSANN-tabular vs MLP baseline\n",
    "set_seed(606)\n",
    "X, y = gen_eaf_heats(heats=140, min_len=80, max_len=180, seed=606)  # X: (N, ctx, F_tabular_per_step)\n",
    "# Collapse per-step features to summary stats over last k steps (tabular proxy)\n",
    "def collapse_tail(Xseq, k=6):\n",
    "    # concat mean and last values for each channel to form a fixed-length tabular vector\n",
    "    tail = Xseq[:, -k:, :]  # (N,k,F)\n",
    "    mean = tail.mean(axis=1)\n",
    "    last = tail[:, -1, :]\n",
    "    return np.concatenate([mean, last], axis=1)\n",
    "\n",
    "X_tab = collapse_tail(X, k=8)\n",
    "# scale\n",
    "mu = X_tab.mean(0, keepdims=True); sd = X_tab.std(0, keepdims=True) + 1e-8\n",
    "Xn = (X_tab - mu)/sd\n",
    "\n",
    "N = len(y); ntr=int(0.7*N); nv=int(0.15*N)\n",
    "Xtr, ytr = Xn[:ntr], y[:ntr]; Xv,yv = Xn[ntr:ntr+nv], y[ntr:ntr+nv]; Xte,yte = Xn[ntr+nv:], y[ntr+nv:]\n",
    "\n",
    "# Baseline MLP\n",
    "class MLP_Reg(nn.Module):\n",
    "    def __init__(self, in_f, hidden=64, layers=2):\n",
    "        super().__init__()\n",
    "        net=[nn.Linear(in_f, hidden), nn.ReLU()]\n",
    "        for _ in range(layers-1):\n",
    "            net += [nn.Linear(hidden, hidden), nn.ReLU()]\n",
    "        self.net = nn.Sequential(*net)\n",
    "        self.head = nn.Linear(hidden, 1)\n",
    "    def forward(self, x): return self.head(self.net(x))\n",
    "\n",
    "def fit_reg_tab(model, Xtr, ytr, Xv, yv, epochs=80, lr=2e-3, bs=128):\n",
    "    model = model.to(DEVICE); opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    Xtr = torch.tensor(Xtr, dtype=torch.float32); ytr = torch.tensor(ytr, dtype=torch.float32)[:,None]\n",
    "    Xv = torch.tensor(Xv, dtype=torch.float32); yv = torch.tensor(yv, dtype=torch.float32)[:,None]\n",
    "    dl = DataLoader(torch.utils.data.TensorDataset(Xtr, ytr), batch_size=bs, shuffle=True)\n",
    "    best=1e9; best_state=None\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in dl:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pred = model(xb)\n",
    "            loss = F.mse_loss(pred, yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vpred = model(Xv.to(DEVICE)).cpu().numpy().ravel()\n",
    "        v = np.mean((vpred - yv.numpy().ravel())**2)\n",
    "        if v < best: best=v; best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    if best_state: model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "in_f = Xn.shape[1]\n",
    "target_params = 160_000\n",
    "psann_tab, p_ps = build_psann_tabular(in_f, target_params, out_f=1)\n",
    "mlp = MLP_Reg(in_f, hidden=128, layers=3)\n",
    "wave_tab, p_wr = build_wave_resnet_tabular(in_f, target_params, out_f=1)\n",
    "\n",
    "t0 = time.time(); psann_tab = fit_reg_tab(psann_tab, Xtr, ytr, Xv, yv, epochs=80, lr=2e-3); t1 = time.time()\n",
    "tw0 = time.time(); wave_tab = fit_reg_tab(wave_tab, Xtr, ytr, Xv, yv, epochs=80, lr=2e-3); tw1 = time.time()\n",
    "tp0 = time.time(); mlp       = fit_reg_tab(mlp,       Xtr, ytr, Xv, yv, epochs=80, lr=2e-3); tp1 = time.time()\n",
    "yhat_ps = predict_regressor(psann_tab, Xte)\n",
    "yhat_wr = predict_regressor(wave_tab, Xte)\n",
    "yhat_ml = predict_regressor(mlp, Xte)\n",
    "\n",
    "rec = pd.DataFrame([\n",
    "    dict(dataset=\"EAF_PROXY\", probe=\"DELTA_TEMP\", split=\"test\", model=\"ResPSANN_tabular\",\n",
    "         params=param_count(psann_tab), train_wall_seconds=t1-t0, **eval_regression(yte, yhat_ps)),\n",
    "    dict(dataset=\"EAF_PROXY\", probe=\"DELTA_TEMP\", split=\"test\", model=\"WaveResNet_tabular\",\n",
    "         params=param_count(wave_tab), train_wall_seconds=tw1-tw0, **eval_regression(yte, yhat_wr)),\n",
    "    dict(dataset=\"EAF_PROXY\", probe=\"DELTA_TEMP\", split=\"test\", model=\"MLP_baseline\",\n",
    "         params=param_count(mlp), train_wall_seconds=tp1-tp0, **eval_regression(yte, yhat_ml)),\n",
    "])\n",
    "save_csv(rec, \"synthetic_experiment_metrics.csv\")\n",
    "rec[[\"model\",\"r2\",\"mae\",\"rmse\",\"params\",\"train_wall_seconds\"]].round(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84feb95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a84feb95",
    "outputId": "a18bf7cd-fae7-4046-8266-8a16989044c5"
   },
   "outputs": [],
   "source": [
    "# @title Load and preview all CSV outputs\n",
    "em_path = os.path.join(RESULTS_DIR, \"synthetic_experiment_metrics.csv\")\n",
    "ab_path = os.path.join(RESULTS_DIR, \"synthetic_ablation_results.csv\")\n",
    "\n",
    "em = pd.read_csv(em_path) if os.path.exists(em_path) else pd.DataFrame()\n",
    "ab = pd.read_csv(ab_path) if os.path.exists(ab_path) else pd.DataFrame()\n",
    "\n",
    "print(\"Experiment metrics rows:\", len(em))\n",
    "display(em.sort_values([\"dataset\",\"probe\",\"model\"]).head(20))\n",
    "print(\"\\nAblation results rows:\", len(ab))\n",
    "display(ab)\n",
    "print(\"\\nSpectral results:\", os.path.join(RESULTS_DIR,\"synthetic_spectral_results.json\"))\n",
    "if os.path.exists(os.path.join(RESULTS_DIR,\"synthetic_spectral_results.json\")):\n",
    "    print(open(os.path.join(RESULTS_DIR,\"synthetic_spectral_results.json\")).read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805dacf4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "805dacf4",
    "outputId": "cb5eea24-2e1c-4e87-ec58-2183532ea56a"
   },
   "outputs": [],
   "source": [
    "zip_folder(folder_path = '/content/psann_synth_results', output_path = '/content/psann_synth_results.zip')"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "cBnMNMM7pJF7"
   },
   "id": "cBnMNMM7pJF7",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}